{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abumaude/AI-Foolosophy/blob/main/FTE_HARM_Full_Validation_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FTE-HARM FULL: Complete Validation Pipeline\n",
        "\n",
        "This notebook implements the **complete FTE-HARM validation pipeline** with:\n",
        "\n",
        "- **Multiple hypotheses** (8+ based on discovered labels)\n",
        "- **Two P_Score methods** (Option A: Binary, Option B3: Confidence-Weighted)\n",
        "- **Three validation approaches** (Binary, Two-Stage, Confidence Calibration)\n",
        "- **Multi-dataset processing** with results saved to Google Drive\n",
        "\n",
        "## Dataset Structure\n",
        "\n",
        "```\n",
        "Base Path: /content/drive/My Drive/thesis/dataset/\n",
        "\n",
        "Supported naming patterns:\n",
        "- log.log / label.log\n",
        "- log_auth.log / label_auth.log  \n",
        "- openvpn.log / openvpn_labels.log\n",
        "- <name>.log / <name>_labels.log\n",
        "```\n",
        "\n",
        "## Workflow\n",
        "\n",
        "1. **Label Discovery** - Find all unique labels across datasets\n",
        "2. **Hypothesis Generation** - Create hypotheses from discovered labels\n",
        "3. **Entity Extraction** - Use transformer NER model\n",
        "4. **P_Score Calculation** - Score with Option A (Binary) and B3 (Confidence)\n",
        "5. **Validation** - Binary, Two-Stage, and Calibration approaches\n",
        "6. **Save Results** - Export to Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cell 1: Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# FTE-HARM FULL: IMPORTS AND CONFIGURATION\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "# Install transformers if needed\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "except ImportError:\n",
        "    !pip install -q transformers\n",
        "    from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FTE-HARM FULL VALIDATION PIPELINE\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# PATH CONFIGURATION\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "DATASET_BASE_PATH = '/content/drive/My Drive/thesis/dataset'\n",
        "OUTPUT_PATH = '/content/drive/My Drive/thesis/hypotheses_validation'\n",
        "\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# Transformer models - update paths to your models\n",
        "MODELS = {\n",
        "    'distilbert': '/content/drive/My Drive/thesis/transformer/distilberta_base_uncased/results/checkpoint-5245',\n",
        "    'distilroberta': '/content/drive/My Drive/thesis/transformer/distilroberta_base/results/checkpoint-5275',\n",
        "    'roberta_large': '/content/drive/My Drive/thesis/transformer/roberta_large/results/checkpoint-2772',\n",
        "    'xlm_roberta_base': '/content/drive/My Drive/thesis/transformer/xlm_roberta_base/results/checkpoint-12216',\n",
        "    'xlm_roberta_large': '/content/drive/My Drive/thesis/transformer/xlm_roberta_large/results/checkpoint-12240',\n",
        "}\n",
        "\n",
        "# Select which model to use\n",
        "SELECTED_MODEL = 'xlm_roberta_large'\n",
        "\n",
        "# Confidence thresholds for triage\n",
        "THRESHOLDS = {\n",
        "    'HIGH': 0.65,\n",
        "    'MEDIUM': 0.50,\n",
        "    'LOW': 0.35\n",
        "}\n",
        "\n",
        "# Triage priority descriptions\n",
        "TRIAGE_PRIORITIES = {\n",
        "    'HIGH': 'Priority 1: Investigate immediately',\n",
        "    'MEDIUM': 'Priority 2: Queue for investigation',\n",
        "    'LOW': 'Priority 3: Investigate later',\n",
        "    'INSUFFICIENT': 'Priority 4: Archive for future relevance'\n",
        "}\n",
        "\n",
        "print(\"✓ Configuration loaded\")\n",
        "print(f\"  Dataset path: {DATASET_BASE_PATH}\")\n",
        "print(f\"  Output path: {OUTPUT_PATH}\")\n",
        "print(f\"  Selected model: {SELECTED_MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cell 2: Dataset Discovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# DATASET DISCOVERY - FLEXIBLE FILE NAMING\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def find_log_label_pair(folder_path):\n",
        "    \"\"\"Find log and label files with variable naming conventions.\"\"\"\n",
        "    if not os.path.isdir(folder_path):\n",
        "        return None, None\n",
        "    \n",
        "    files = os.listdir(folder_path)\n",
        "    log_files = [f for f in files if f.endswith('.log')]\n",
        "    \n",
        "    log_file = None\n",
        "    label_file = None\n",
        "    \n",
        "    # Strategy 1: log.log / label.log\n",
        "    if 'log.log' in log_files and 'label.log' in log_files:\n",
        "        log_file, label_file = 'log.log', 'label.log'\n",
        "    \n",
        "    # Strategy 2: log_*.log / label_*.log\n",
        "    elif any(f.startswith('log_') for f in log_files):\n",
        "        for f in log_files:\n",
        "            if f.startswith('log_'):\n",
        "                suffix = f[4:]\n",
        "                potential_label = f'label_{suffix}'\n",
        "                if potential_label in log_files:\n",
        "                    log_file, label_file = f, potential_label\n",
        "                    break\n",
        "    \n",
        "    # Strategy 3: <name>.log / <name>_labels.log\n",
        "    else:\n",
        "        for f in log_files:\n",
        "            if not f.endswith('_labels.log'):\n",
        "                base_name = f[:-4]\n",
        "                potential_label = f'{base_name}_labels.log'\n",
        "                if potential_label in log_files:\n",
        "                    log_file, label_file = f, potential_label\n",
        "                    break\n",
        "    \n",
        "    if log_file and label_file:\n",
        "        return os.path.join(folder_path, log_file), os.path.join(folder_path, label_file)\n",
        "    return None, None\n",
        "\n",
        "\n",
        "def scan_all_datasets(base_path):\n",
        "    \"\"\"Recursively scan for all valid dataset pairs.\"\"\"\n",
        "    datasets = []\n",
        "    \n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        log_path, label_path = find_log_label_pair(root)\n",
        "        \n",
        "        if log_path and label_path:\n",
        "            rel_path = os.path.relpath(root, base_path)\n",
        "            datasets.append({\n",
        "                'name': rel_path,\n",
        "                'folder': root,\n",
        "                'log_path': log_path,\n",
        "                'label_path': label_path,\n",
        "                'log_file': os.path.basename(log_path),\n",
        "                'label_file': os.path.basename(label_path)\n",
        "            })\n",
        "    \n",
        "    return datasets\n",
        "\n",
        "\n",
        "# Scan datasets\n",
        "print(\"Scanning for datasets...\")\n",
        "all_datasets = scan_all_datasets(DATASET_BASE_PATH)\n",
        "print(f\"\\n✓ Found {len(all_datasets)} dataset pairs\")\n",
        "\n",
        "for i, ds in enumerate(all_datasets, 1):\n",
        "    print(f\"  {i}. {ds['name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cell 3: Label Discovery (Comprehensive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# LABEL DISCOVERY - COMPREHENSIVE\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def discover_labels(label_path):\n",
        "    \"\"\"Discover all unique labels in a label file.\"\"\"\n",
        "    all_labels = set()\n",
        "    label_counts = defaultdict(int)\n",
        "    first_label = None\n",
        "    total_entries = 0\n",
        "    \n",
        "    if not os.path.exists(label_path):\n",
        "        return None\n",
        "    \n",
        "    with open(label_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            \n",
        "            try:\n",
        "                entry = json.loads(line)\n",
        "                labels = entry.get('labels', [])\n",
        "                \n",
        "                if labels:\n",
        "                    total_entries += 1\n",
        "                    for label in labels:\n",
        "                        all_labels.add(label)\n",
        "                        label_counts[label] += 1\n",
        "                        if first_label is None:\n",
        "                            first_label = label\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "    \n",
        "    sorted_labels = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    return {\n",
        "        'all_labels': all_labels,\n",
        "        'label_counts': dict(label_counts),\n",
        "        'sorted_labels': sorted_labels,\n",
        "        'first_label': first_label,\n",
        "        'most_common': sorted_labels[0][0] if sorted_labels else None,\n",
        "        'total_entries': total_entries\n",
        "    }\n",
        "\n",
        "\n",
        "def discover_all_labels(datasets):\n",
        "    \"\"\"Discover labels across all datasets.\"\"\"\n",
        "    combined_labels = set()\n",
        "    combined_counts = defaultdict(int)\n",
        "    labels_per_dataset = {}\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"LABEL DISCOVERY\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for ds in datasets:\n",
        "        result = discover_labels(ds['label_path'])\n",
        "        \n",
        "        if result:\n",
        "            labels_per_dataset[ds['name']] = list(result['all_labels'])\n",
        "            combined_labels.update(result['all_labels'])\n",
        "            \n",
        "            for label, count in result['label_counts'].items():\n",
        "                combined_counts[label] += count\n",
        "            \n",
        "            print(f\"\\n{ds['name']}:\")\n",
        "            print(f\"  Entries: {result['total_entries']}, Labels: {list(result['all_labels'])}\")\n",
        "    \n",
        "    sorted_combined = sorted(combined_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    print(f\"\\n{'─'*40}\")\n",
        "    print(f\"COMBINED: {len(combined_labels)} unique labels\")\n",
        "    for label, count in sorted_combined:\n",
        "        print(f\"  {label}: {count}\")\n",
        "    \n",
        "    return {\n",
        "        'all_labels': combined_labels,\n",
        "        'label_counts': dict(combined_counts),\n",
        "        'sorted_labels': sorted_combined,\n",
        "        'labels_per_dataset': labels_per_dataset\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run label discovery\n",
        "label_discovery = discover_all_labels(all_datasets)\n",
        "\n",
        "# Save discovery results\n",
        "discovery_path = os.path.join(OUTPUT_PATH, f'label_discovery_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt')\n",
        "with open(discovery_path, 'w') as f:\n",
        "    f.write(\"LABEL DISCOVERY RESULTS\\n\")\n",
        "    f.write(f\"Timestamp: {datetime.now().isoformat()}\\n\")\n",
        "    f.write(f\"Datasets scanned: {len(all_datasets)}\\n\\n\")\n",
        "    f.write(f\"Unique labels: {len(label_discovery['all_labels'])}\\n\\n\")\n",
        "    for label, count in label_discovery['sorted_labels']:\n",
        "        f.write(f\"  {label}: {count}\\n\")\n",
        "\n",
        "print(f\"\\n✓ Discovery saved: {discovery_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cell 4: Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# DATA LOADERS\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def load_ground_truth(label_path):\n",
        "    \"\"\"Load ground truth (NO tokenization).\"\"\"\n",
        "    ground_truth = {}\n",
        "    \n",
        "    if not os.path.exists(label_path):\n",
        "        return ground_truth\n",
        "    \n",
        "    with open(label_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for json_line in f:\n",
        "            json_line = json_line.strip()\n",
        "            if not json_line:\n",
        "                continue\n",
        "            \n",
        "            try:\n",
        "                entry = json.loads(json_line)\n",
        "                line_num = entry.get('line')\n",
        "                \n",
        "                if line_num is not None:\n",
        "                    ground_truth[line_num] = {\n",
        "                        'labels': entry.get('labels', []),\n",
        "                        'rules': entry.get('rules', {})\n",
        "                    }\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "    \n",
        "    return ground_truth\n",
        "\n",
        "\n",
        "def get_label_for_line(line_number, ground_truth):\n",
        "    \"\"\"Get ground truth for a line.\"\"\"\n",
        "    if line_number in ground_truth:\n",
        "        return {\n",
        "            \"is_malicious\": True,\n",
        "            \"labels\": ground_truth[line_number]['labels'],\n",
        "            \"rules\": ground_truth[line_number]['rules']\n",
        "        }\n",
        "    return {\"is_malicious\": False, \"labels\": [], \"rules\": {}}\n",
        "\n",
        "\n",
        "def load_raw_logs(log_path):\n",
        "    \"\"\"Load logs with 1-indexed line tracking.\"\"\"\n",
        "    logs = []\n",
        "    \n",
        "    if not os.path.exists(log_path):\n",
        "        return logs\n",
        "    \n",
        "    with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line_number, log_text in enumerate(f, 1):\n",
        "            log_text = log_text.strip()\n",
        "            if log_text:\n",
        "                logs.append((line_number, log_text))\n",
        "    \n",
        "    return logs\n",
        "\n",
        "\n",
        "def load_dataset(dataset_info):\n",
        "    \"\"\"Load complete dataset.\"\"\"\n",
        "    logs = load_raw_logs(dataset_info['log_path'])\n",
        "    ground_truth = load_ground_truth(dataset_info['label_path'])\n",
        "    \n",
        "    stats = {\n",
        "        'name': dataset_info['name'],\n",
        "        'total_lines': len(logs),\n",
        "        'malicious_lines': len(ground_truth),\n",
        "        'benign_lines': len(logs) - len(ground_truth),\n",
        "        'malicious_pct': (len(ground_truth) / len(logs) * 100) if logs else 0\n",
        "    }\n",
        "    \n",
        "    return logs, ground_truth, stats\n",
        "\n",
        "\n",
        "print(\"✓ Data loaders defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cell 5: Model Loading and Entity Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# MODEL LOADING AND ENTITY EXTRACTION\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def get_model_pipeline(model_path):\n",
        "    \"\"\"Load NER model.\"\"\"\n",
        "    print(f\"Loading model: {model_path}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "    nlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "    print(\"✓ Model loaded\")\n",
        "    return nlp\n",
        "\n",
        "\n",
        "def process_text(text, nlp_pipeline):\n",
        "    \"\"\"Physical Token Quantization for entity extraction.\"\"\"\n",
        "    raw_results = nlp_pipeline(text)\n",
        "    physical_tokens = [m for m in re.finditer(r'[^\\[\\]\\s]+', text)]\n",
        "    \n",
        "    atomic_entities = []\n",
        "    \n",
        "    for pt in physical_tokens:\n",
        "        t_start, t_end = pt.span()\n",
        "        matches = [r for r in raw_results if r['start'] < t_end and r['end'] > t_start]\n",
        "        \n",
        "        if not matches:\n",
        "            continue\n",
        "        \n",
        "        labels = set(m['entity_group'] for m in matches)\n",
        "        chosen_label = 'IPAddress' if 'IPAddress' in labels and 'DNSName' in labels else matches[0]['entity_group']\n",
        "        avg_score = sum(float(m['score']) for m in matches) / len(matches)\n",
        "        \n",
        "        atomic_entities.append({\n",
        "            \"label\": chosen_label,\n",
        "            \"text\": text[t_start:t_end],\n",
        "            \"start\": t_start,\n",
        "            \"end\": t_end,\n",
        "            \"confidence\": avg_score\n",
        "        })\n",
        "    \n",
        "    # Merge adjacent same-label entities\n",
        "    if not atomic_entities:\n",
        "        return []\n",
        "    \n",
        "    final = [atomic_entities[0]]\n",
        "    for curr in atomic_entities[1:]:\n",
        "        prev = final[-1]\n",
        "        between = text[prev['end']:curr['start']]\n",
        "        merge = between.strip() == '' and '[' not in between and ']' not in between\n",
        "        \n",
        "        if merge and prev['label'] == curr['label']:\n",
        "            prev['end'] = curr['end']\n",
        "            prev['text'] = text[prev['start']:prev['end']]\n",
        "            prev['confidence'] = (prev['confidence'] + curr['confidence']) / 2\n",
        "        else:\n",
        "            final.append(curr)\n",
        "    \n",
        "    for e in final:\n",
        "        e['confidence'] = round(e['confidence'], 4)\n",
        "    \n",
        "    return final\n",
        "\n",
        "\n",
        "def extract_entities(line_number, log_text, nlp):\n",
        "    \"\"\"Extract entities with line tracking.\"\"\"\n",
        "    entities = process_text(log_text, nlp)\n",
        "    \n",
        "    entity_types = defaultdict(list)\n",
        "    for e in entities:\n",
        "        entity_types[e['label']].append({'value': e['text'], 'confidence': e['confidence']})\n",
        "    \n",
        "    return {\n",
        "        'line_number': line_number,\n",
        "        'log_text': log_text,\n",
        "        'entities': entities,\n",
        "        'entity_types': dict(entity_types)\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"✓ Entity extraction functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model\n",
        "nlp = get_model_pipeline(MODELS[SELECTED_MODEL])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cell 6: Multiple Hypotheses (Dynamic Generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# MULTIPLE HYPOTHESES - DYNAMIC GENERATION\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def generate_hypotheses_from_labels(discovered_labels):\n",
        "    \"\"\"\n",
        "    Generate hypotheses based on discovered labels.\n",
        "    \n",
        "    Creates a hypothesis for each unique label found in the dataset.\n",
        "    \"\"\"\n",
        "    hypotheses = {}\n",
        "    \n",
        "    # Default weight templates by category\n",
        "    weight_templates = {\n",
        "        'escalate': {\n",
        "            'Process': 0.30, 'Username': 0.25, 'Action': 0.20,\n",
        "            'DateTime': 0.15, 'Status': 0.10\n",
        "        },\n",
        "        'attacker': {\n",
        "            'Username': 0.30, 'IPAddress': 0.25, 'Action': 0.20,\n",
        "            'Process': 0.15, 'DateTime': 0.10\n",
        "        },\n",
        "        'lateral': {\n",
        "            'IPAddress': 0.30, 'Username': 0.25, 'Process': 0.20,\n",
        "            'Action': 0.15, 'DateTime': 0.10\n",
        "        },\n",
        "        'dns': {\n",
        "            'DNSName': 0.35, 'IPAddress': 0.25, 'Action': 0.15,\n",
        "            'Process': 0.15, 'DateTime': 0.10\n",
        "        },\n",
        "        'default': {\n",
        "            'Process': 0.25, 'Action': 0.20, 'Username': 0.20,\n",
        "            'IPAddress': 0.15, 'DateTime': 0.10, 'Status': 0.10\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    for i, (label, count) in enumerate(discovered_labels, 1):\n",
        "        # Select weight template based on label keywords\n",
        "        label_lower = label.lower()\n",
        "        \n",
        "        if 'escalat' in label_lower or 'su' in label_lower or 'privilege' in label_lower:\n",
        "            weights = weight_templates['escalate'].copy()\n",
        "            critical = 'Process'\n",
        "        elif 'attacker' in label_lower or 'change_user' in label_lower:\n",
        "            weights = weight_templates['attacker'].copy()\n",
        "            critical = 'Username'\n",
        "        elif 'lateral' in label_lower or 'movement' in label_lower or 'ssh' in label_lower:\n",
        "            weights = weight_templates['lateral'].copy()\n",
        "            critical = 'IPAddress'\n",
        "        elif 'dns' in label_lower or 'query' in label_lower:\n",
        "            weights = weight_templates['dns'].copy()\n",
        "            critical = 'DNSName'\n",
        "        else:\n",
        "            weights = weight_templates['default'].copy()\n",
        "            critical = 'Process'\n",
        "        \n",
        "        hyp_name = f'H{i}_{label}'\n",
        "        hypotheses[hyp_name] = {\n",
        "            'name': hyp_name,\n",
        "            'description': f'Hypothesis for {label}',\n",
        "            'target_labels': [label],  # Labels this hypothesis maps to\n",
        "            'weights': weights,\n",
        "            'critical_entity': critical,\n",
        "            'penalty_factor': 0.20\n",
        "        }\n",
        "    \n",
        "    return hypotheses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate hypotheses from discovered labels\n",
        "HYPOTHESES = generate_hypotheses_from_labels(label_discovery['sorted_labels'])\n",
        "\n",
        "print(f\"\\n✓ Generated {len(HYPOTHESES)} hypotheses:\")\n",
        "for name, hyp in HYPOTHESES.items():\n",
        "    print(f\"  {name}: critical={hyp['critical_entity']}, targets={hyp['target_labels']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cell 7: P_Score Option A (Binary Presence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# P_SCORE OPTION A: BINARY PRESENCE\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def calculate_pscore_option_a(entity_types, hypothesis):\n",
        "    \"\"\"\n",
        "    P_Score with BINARY presence.\n",
        "    Formula: P_Score = (Σ(W_i × E_i)) × (1 - P_F)\n",
        "    E_i = 1 if present, 0 if absent\n",
        "    \"\"\"\n",
        "    weights = hypothesis['weights']\n",
        "    critical = hypothesis['critical_entity']\n",
        "    penalty = hypothesis['penalty_factor']\n",
        "    \n",
        "    weighted_sum = 0.0\n",
        "    breakdown = {}\n",
        "    \n",
        "    for etype, weight in weights.items():\n",
        "        present = etype in entity_types and len(entity_types[etype]) > 0\n",
        "        contrib = weight * (1 if present else 0)\n",
        "        weighted_sum += contrib\n",
        "        breakdown[etype] = {'weight': weight, 'present': present, 'contribution': contrib}\n",
        "    \n",
        "    critical_present = critical in entity_types and len(entity_types[critical]) > 0\n",
        "    p_score = weighted_sum if critical_present else weighted_sum * (1 - penalty)\n",
        "    \n",
        "    if p_score >= THRESHOLDS['HIGH']:\n",
        "        level = 'HIGH'\n",
        "    elif p_score >= THRESHOLDS['MEDIUM']:\n",
        "        level = 'MEDIUM'\n",
        "    elif p_score >= THRESHOLDS['LOW']:\n",
        "        level = 'LOW'\n",
        "    else:\n",
        "        level = 'INSUFFICIENT'\n",
        "    \n",
        "    return {\n",
        "        'p_score': round(p_score, 4),\n",
        "        'confidence_level': level,\n",
        "        'is_malicious': p_score >= THRESHOLDS['LOW'],\n",
        "        'triage_priority': TRIAGE_PRIORITIES[level],\n",
        "        'critical_present': critical_present,\n",
        "        'breakdown': breakdown,\n",
        "        'method': 'Option_A'\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"✓ P_Score Option A (Binary) defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cell 8: P_Score Option B3 (Confidence Weighted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# P_SCORE OPTION B3: CONFIDENCE WEIGHTED\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def calculate_pscore_option_b3(entity_types, hypothesis):\n",
        "    \"\"\"\n",
        "    P_Score with CONFIDENCE weighting.\n",
        "    Formula: P_Score = (Σ(W_i × C_i)) × (1 - P_F)\n",
        "    C_i = average confidence of entities (0.0-1.0)\n",
        "    \"\"\"\n",
        "    weights = hypothesis['weights']\n",
        "    critical = hypothesis['critical_entity']\n",
        "    penalty = hypothesis['penalty_factor']\n",
        "    \n",
        "    weighted_sum = 0.0\n",
        "    breakdown = {}\n",
        "    \n",
        "    for etype, weight in weights.items():\n",
        "        if etype in entity_types and entity_types[etype]:\n",
        "            confidences = [e['confidence'] for e in entity_types[etype]]\n",
        "            avg_conf = sum(confidences) / len(confidences)\n",
        "            contrib = weight * avg_conf\n",
        "            breakdown[etype] = {\n",
        "                'weight': weight, 'present': True,\n",
        "                'avg_confidence': round(avg_conf, 4), 'contribution': round(contrib, 4)\n",
        "            }\n",
        "        else:\n",
        "            contrib = 0.0\n",
        "            breakdown[etype] = {'weight': weight, 'present': False, 'contribution': 0.0}\n",
        "        \n",
        "        weighted_sum += contrib\n",
        "    \n",
        "    critical_present = critical in entity_types and len(entity_types[critical]) > 0\n",
        "    p_score = weighted_sum if critical_present else weighted_sum * (1 - penalty)\n",
        "    \n",
        "    if p_score >= THRESHOLDS['HIGH']:\n",
        "        level = 'HIGH'\n",
        "    elif p_score >= THRESHOLDS['MEDIUM']:\n",
        "        level = 'MEDIUM'\n",
        "    elif p_score >= THRESHOLDS['LOW']:\n",
        "        level = 'LOW'\n",
        "    else:\n",
        "        level = 'INSUFFICIENT'\n",
        "    \n",
        "    return {\n",
        "        'p_score': round(p_score, 4),\n",
        "        'confidence_level': level,\n",
        "        'is_malicious': p_score >= THRESHOLDS['LOW'],\n",
        "        'triage_priority': TRIAGE_PRIORITIES[level],\n",
        "        'critical_present': critical_present,\n",
        "        'breakdown': breakdown,\n",
        "        'method': 'Option_B3'\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"✓ P_Score Option B3 (Confidence) defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cell 9: Multi-Hypothesis Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# MULTI-HYPOTHESIS SCORING\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def score_all_hypotheses(entity_types, hypotheses, method='option_a'):\n",
        "    \"\"\"Score against all hypotheses and rank.\"\"\"\n",
        "    score_func = calculate_pscore_option_a if method == 'option_a' else calculate_pscore_option_b3\n",
        "    \n",
        "    scores = {name: score_func(entity_types, hyp) for name, hyp in hypotheses.items()}\n",
        "    ranking = sorted([(n, s['p_score']) for n, s in scores.items()], key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    top = ranking[0] if ranking else (None, 0.0)\n",
        "    \n",
        "    return {\n",
        "        'scores': scores,\n",
        "        'ranking': ranking,\n",
        "        'top_hypothesis': top[0],\n",
        "        'top_score': top[1],\n",
        "        'top_level': scores[top[0]]['confidence_level'] if top[0] else 'INSUFFICIENT',\n",
        "        'is_malicious': top[1] >= THRESHOLDS['LOW'],\n",
        "        'triage': scores[top[0]]['triage_priority'] if top[0] else TRIAGE_PRIORITIES['INSUFFICIENT']\n",
        "    }\n",
        "\n",
        "\n",
        "def process_log_full(line_number, log_text, nlp, hypotheses, method='option_a'):\n",
        "    \"\"\"Complete processing for one log line.\"\"\"\n",
        "    extraction = extract_entities(line_number, log_text, nlp)\n",
        "    scoring = score_all_hypotheses(extraction['entity_types'], hypotheses, method)\n",
        "    \n",
        "    return {\n",
        "        'line_number': line_number,\n",
        "        'log_text': log_text,\n",
        "        'entities': extraction['entities'],\n",
        "        'entity_types': extraction['entity_types'],\n",
        "        'scoring': scoring,\n",
        "        'prediction': {\n",
        "            'hypothesis': scoring['top_hypothesis'],\n",
        "            'p_score': scoring['top_score'],\n",
        "            'confidence': scoring['top_level'],\n",
        "            'is_malicious': scoring['is_malicious'],\n",
        "            'triage': scoring['triage']\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"✓ Multi-hypothesis scoring defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cell 10: Validation Approach 1 (Binary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# VALIDATION APPROACH 1: BINARY\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def validate_binary(predictions, ground_truth):\n",
        "    \"\"\"Binary validation: malicious vs benign.\"\"\"\n",
        "    tp = fp = tn = fn = 0\n",
        "    details = {'tp': [], 'fp': [], 'tn': [], 'fn': []}\n",
        "    \n",
        "    for pred in predictions:\n",
        "        line = pred['line_number']\n",
        "        pred_mal = pred['prediction']['is_malicious']\n",
        "        gt = get_label_for_line(line, ground_truth)\n",
        "        actual_mal = gt['is_malicious']\n",
        "        \n",
        "        if pred_mal and actual_mal:\n",
        "            tp += 1\n",
        "            details['tp'].append({'line': line, 'score': pred['prediction']['p_score'], 'labels': gt['labels']})\n",
        "        elif pred_mal and not actual_mal:\n",
        "            fp += 1\n",
        "            details['fp'].append({'line': line, 'score': pred['prediction']['p_score']})\n",
        "        elif not pred_mal and not actual_mal:\n",
        "            tn += 1\n",
        "        else:\n",
        "            fn += 1\n",
        "            details['fn'].append({'line': line, 'labels': gt['labels']})\n",
        "    \n",
        "    total = tp + fp + tn + fn\n",
        "    prec = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "    rec = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "    f1 = 2 * prec * rec / (prec + rec) if prec + rec > 0 else 0\n",
        "    acc = (tp + tn) / total if total > 0 else 0\n",
        "    \n",
        "    return {\n",
        "        'approach': 'Approach 1: Binary',\n",
        "        'cm': {'TP': tp, 'FP': fp, 'TN': tn, 'FN': fn},\n",
        "        'metrics': {'precision': round(prec, 4), 'recall': round(rec, 4), 'f1': round(f1, 4), 'accuracy': round(acc, 4)},\n",
        "        'details': details\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"✓ Validation Approach 1 (Binary) defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cell 11: Validation Approach 3 (Two-Stage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# VALIDATION APPROACH 3: TWO-STAGE\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def validate_two_stage(predictions, ground_truth, hypotheses):\n",
        "    \"\"\"Two-stage: detection + hypothesis accuracy.\"\"\"\n",
        "    # Stage A: Binary\n",
        "    binary = validate_binary(predictions, ground_truth)\n",
        "    \n",
        "    # Stage B: Hypothesis accuracy for true positives\n",
        "    correct = incorrect = 0\n",
        "    \n",
        "    for tp in binary['details']['tp']:\n",
        "        line = tp['line']\n",
        "        gt_labels = tp['labels']\n",
        "        \n",
        "        # Find predicted hypothesis\n",
        "        pred = next((p for p in predictions if p['line_number'] == line), None)\n",
        "        if not pred:\n",
        "            continue\n",
        "        \n",
        "        pred_hyp = pred['prediction']['hypothesis']\n",
        "        if pred_hyp and pred_hyp in hypotheses:\n",
        "            target_labels = hypotheses[pred_hyp].get('target_labels', [])\n",
        "            \n",
        "            # Check if any gt label matches target labels\n",
        "            matched = any(\n",
        "                any(tl.lower() in gl.lower() or gl.lower() in tl.lower() \n",
        "                    for tl in target_labels)\n",
        "                for gl in gt_labels\n",
        "            )\n",
        "            \n",
        "            if matched:\n",
        "                correct += 1\n",
        "            else:\n",
        "                incorrect += 1\n",
        "    \n",
        "    total_tp = correct + incorrect\n",
        "    hyp_acc = correct / total_tp if total_tp > 0 else 0\n",
        "    \n",
        "    return {\n",
        "        'approach': 'Approach 3: Two-Stage',\n",
        "        'stage_a': binary['metrics'],\n",
        "        'stage_b': {\n",
        "            'total_tp': total_tp,\n",
        "            'correct': correct,\n",
        "            'incorrect': incorrect,\n",
        "            'hypothesis_accuracy': round(hyp_acc, 4)\n",
        "        },\n",
        "        'combined': {\n",
        "            'detection_f1': binary['metrics']['f1'],\n",
        "            'hypothesis_acc': round(hyp_acc, 4),\n",
        "            'overall': round(binary['metrics']['f1'] * hyp_acc, 4)\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"✓ Validation Approach 3 (Two-Stage) defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cell 12: Validation Approach 5 (Confidence Calibration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# VALIDATION APPROACH 5: CONFIDENCE CALIBRATION\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def validate_calibration(predictions, ground_truth):\n",
        "    \"\"\"Confidence calibration: does confidence correlate with accuracy?\"\"\"\n",
        "    buckets = {level: {'correct': 0, 'incorrect': 0} for level in ['HIGH', 'MEDIUM', 'LOW', 'INSUFFICIENT']}\n",
        "    \n",
        "    for pred in predictions:\n",
        "        line = pred['line_number']\n",
        "        level = pred['prediction']['confidence']\n",
        "        pred_mal = pred['prediction']['is_malicious']\n",
        "        \n",
        "        gt = get_label_for_line(line, ground_truth)\n",
        "        correct = (pred_mal == gt['is_malicious'])\n",
        "        \n",
        "        if correct:\n",
        "            buckets[level]['correct'] += 1\n",
        "        else:\n",
        "            buckets[level]['incorrect'] += 1\n",
        "    \n",
        "    # Calculate accuracy per level\n",
        "    calibration = {}\n",
        "    expected = {'HIGH': 0.90, 'MEDIUM': 0.70, 'LOW': 0.50, 'INSUFFICIENT': 0.30}\n",
        "    \n",
        "    for level, bucket in buckets.items():\n",
        "        total = bucket['correct'] + bucket['incorrect']\n",
        "        acc = bucket['correct'] / total if total > 0 else 0\n",
        "        calibration[level] = {'total': total, 'correct': bucket['correct'], 'accuracy': round(acc, 4)}\n",
        "    \n",
        "    # Expected Calibration Error (ECE)\n",
        "    total_preds = sum(c['total'] for c in calibration.values())\n",
        "    ece = sum(\n",
        "        (calibration[l]['total'] / total_preds * abs(calibration[l]['accuracy'] - expected[l]))\n",
        "        for l in calibration if calibration[l]['total'] > 0\n",
        "    ) if total_preds > 0 else 0\n",
        "    \n",
        "    quality = 'GOOD' if ece < 0.15 else 'MODERATE' if ece < 0.25 else 'POOR'\n",
        "    \n",
        "    return {\n",
        "        'approach': 'Approach 5: Calibration',\n",
        "        'by_level': calibration,\n",
        "        'ece': round(ece, 4),\n",
        "        'quality': quality\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"✓ Validation Approach 5 (Calibration) defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cell 13: Full Validation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# FULL VALIDATION PIPELINE\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def run_full_validation(dataset_info, nlp, hypotheses):\n",
        "    \"\"\"Run complete validation with both P_Score methods and all approaches.\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"FULL VALIDATION: {dataset_info['name']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    logs, ground_truth, stats = load_dataset(dataset_info)\n",
        "    print(f\"Loaded: {stats['total_lines']} logs, {stats['malicious_lines']} malicious\")\n",
        "    \n",
        "    results = {\n",
        "        'dataset': dataset_info['name'],\n",
        "        'stats': stats,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'model': SELECTED_MODEL,\n",
        "        'option_a': {},\n",
        "        'option_b3': {}\n",
        "    }\n",
        "    \n",
        "    for method in ['option_a', 'option_b3']:\n",
        "        print(f\"\\n─── Processing with {method.upper()} ───\")\n",
        "        \n",
        "        # Process all logs\n",
        "        predictions = []\n",
        "        for i, (line_num, log_text) in enumerate(logs):\n",
        "            pred = process_log_full(line_num, log_text, nlp, hypotheses, method)\n",
        "            predictions.append(pred)\n",
        "            \n",
        "            if (i + 1) % 500 == 0:\n",
        "                print(f\"  {i+1}/{len(logs)} processed\")\n",
        "        \n",
        "        print(f\"  ✓ {len(predictions)} predictions\")\n",
        "        \n",
        "        # Run validations\n",
        "        v1 = validate_binary(predictions, ground_truth)\n",
        "        v3 = validate_two_stage(predictions, ground_truth, hypotheses)\n",
        "        v5 = validate_calibration(predictions, ground_truth)\n",
        "        \n",
        "        results[method] = {\n",
        "            'predictions': predictions,\n",
        "            'approach_1': v1,\n",
        "            'approach_3': v3,\n",
        "            'approach_5': v5\n",
        "        }\n",
        "        \n",
        "        print(f\"  Binary F1: {v1['metrics']['f1']:.4f}\")\n",
        "        print(f\"  Hypothesis Acc: {v3['stage_b']['hypothesis_accuracy']:.4f}\")\n",
        "        print(f\"  Calibration: {v5['quality']} (ECE={v5['ece']:.4f})\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"✓ Full validation pipeline defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cell 14: Save Results to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SAVE RESULTS TO GOOGLE DRIVE\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def save_results(results, output_path=OUTPUT_PATH):\n",
        "    \"\"\"Save all validation results.\"\"\"\n",
        "    dataset_name = results['dataset'].replace('/', '_')\n",
        "    ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    folder = os.path.join(output_path, dataset_name)\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    \n",
        "    saved = []\n",
        "    \n",
        "    for method in ['option_a', 'option_b3']:\n",
        "        mr = results[method]\n",
        "        \n",
        "        # Approach 1 - Binary\n",
        "        path = os.path.join(folder, f'{method}_binary_{ts}.txt')\n",
        "        with open(path, 'w') as f:\n",
        "            f.write(f\"FTE-HARM Validation - Approach 1 (Binary)\\n\")\n",
        "            f.write(f\"Dataset: {results['dataset']}\\n\")\n",
        "            f.write(f\"Method: {method}\\n\")\n",
        "            f.write(f\"Timestamp: {results['timestamp']}\\n\\n\")\n",
        "            f.write(f\"Confusion Matrix:\\n\")\n",
        "            for k, v in mr['approach_1']['cm'].items():\n",
        "                f.write(f\"  {k}: {v}\\n\")\n",
        "            f.write(f\"\\nMetrics:\\n\")\n",
        "            for k, v in mr['approach_1']['metrics'].items():\n",
        "                f.write(f\"  {k}: {v}\\n\")\n",
        "        saved.append(path)\n",
        "        \n",
        "        # Approach 3 - Two-Stage\n",
        "        path = os.path.join(folder, f'{method}_twostage_{ts}.txt')\n",
        "        with open(path, 'w') as f:\n",
        "            f.write(f\"FTE-HARM Validation - Approach 3 (Two-Stage)\\n\")\n",
        "            f.write(f\"Dataset: {results['dataset']}\\n\")\n",
        "            f.write(f\"Method: {method}\\n\\n\")\n",
        "            f.write(f\"Stage A (Detection):\\n\")\n",
        "            for k, v in mr['approach_3']['stage_a'].items():\n",
        "                f.write(f\"  {k}: {v}\\n\")\n",
        "            f.write(f\"\\nStage B (Hypothesis):\\n\")\n",
        "            for k, v in mr['approach_3']['stage_b'].items():\n",
        "                f.write(f\"  {k}: {v}\\n\")\n",
        "            f.write(f\"\\nCombined:\\n\")\n",
        "            for k, v in mr['approach_3']['combined'].items():\n",
        "                f.write(f\"  {k}: {v}\\n\")\n",
        "        saved.append(path)\n",
        "        \n",
        "        # Approach 5 - Calibration\n",
        "        path = os.path.join(folder, f'{method}_calibration_{ts}.txt')\n",
        "        with open(path, 'w') as f:\n",
        "            f.write(f\"FTE-HARM Validation - Approach 5 (Calibration)\\n\")\n",
        "            f.write(f\"Dataset: {results['dataset']}\\n\")\n",
        "            f.write(f\"Method: {method}\\n\\n\")\n",
        "            f.write(f\"By Confidence Level:\\n\")\n",
        "            for level, data in mr['approach_5']['by_level'].items():\n",
        "                f.write(f\"  {level}: {data}\\n\")\n",
        "            f.write(f\"\\nECE: {mr['approach_5']['ece']}\\n\")\n",
        "            f.write(f\"Quality: {mr['approach_5']['quality']}\\n\")\n",
        "        saved.append(path)\n",
        "    \n",
        "    print(f\"\\n✓ Saved {len(saved)} files to: {folder}\")\n",
        "    return saved\n",
        "\n",
        "\n",
        "print(\"✓ Save functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cell 15: Run All Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# RUN VALIDATION ON ALL DATASETS\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def run_all_datasets(datasets, nlp, hypotheses):\n",
        "    \"\"\"Run validation on all datasets.\"\"\"\n",
        "    all_results = {}\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"RUNNING FULL VALIDATION ON {len(datasets)} DATASETS\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    for i, ds in enumerate(datasets, 1):\n",
        "        print(f\"\\n[{i}/{len(datasets)}] {ds['name']}\")\n",
        "        \n",
        "        try:\n",
        "            results = run_full_validation(ds, nlp, hypotheses)\n",
        "            save_results(results)\n",
        "            all_results[ds['name']] = results\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: {e}\")\n",
        "            all_results[ds['name']] = {'error': str(e)}\n",
        "    \n",
        "    # Summary\n",
        "    summary_path = os.path.join(OUTPUT_PATH, f'summary_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt')\n",
        "    with open(summary_path, 'w') as f:\n",
        "        f.write(\"FTE-HARM VALIDATION SUMMARY\\n\")\n",
        "        f.write(f\"Datasets: {len(datasets)}\\n\\n\")\n",
        "        \n",
        "        for name, res in all_results.items():\n",
        "            f.write(f\"\\n{name}:\\n\")\n",
        "            if 'error' in res:\n",
        "                f.write(f\"  ERROR: {res['error']}\\n\")\n",
        "            else:\n",
        "                for m in ['option_a', 'option_b3']:\n",
        "                    f.write(f\"  {m}: F1={res[m]['approach_1']['metrics']['f1']}, HypAcc={res[m]['approach_3']['stage_b']['hypothesis_accuracy']}\\n\")\n",
        "    \n",
        "    print(f\"\\n✓ Summary saved: {summary_path}\")\n",
        "    return all_results\n",
        "\n",
        "\n",
        "print(\"✓ Batch processing functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cell 16: Execute Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# EXECUTE VALIDATION\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "# Option 1: Run on single dataset for testing\n",
        "if all_datasets:\n",
        "    print(\"Running on first dataset for testing...\")\n",
        "    test_results = run_full_validation(all_datasets[0], nlp, HYPOTHESES)\n",
        "    save_results(test_results)\n",
        "else:\n",
        "    print(\"No datasets found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 2: Run on ALL datasets (uncomment to execute)\n",
        "# all_results = run_all_datasets(all_datasets, nlp, HYPOTHESES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Output Structure\n",
        "\n",
        "```\n",
        "/content/drive/My Drive/thesis/hypotheses_validation/\n",
        "├── label_discovery_<ts>.txt\n",
        "├── summary_<ts>.txt\n",
        "└── <dataset_name>/\n",
        "    ├── option_a_binary_<ts>.txt\n",
        "    ├── option_a_twostage_<ts>.txt\n",
        "    ├── option_a_calibration_<ts>.txt\n",
        "    ├── option_b3_binary_<ts>.txt\n",
        "    ├── option_b3_twostage_<ts>.txt\n",
        "    └── option_b3_calibration_<ts>.txt\n",
        "```\n",
        "\n",
        "## Checklist\n",
        "\n",
        "- [ ] Label discovery completed\n",
        "- [ ] Hypotheses generated from labels\n",
        "- [ ] Both P_Score methods tested (Option A + B3)\n",
        "- [ ] All 3 validation approaches run (Binary, Two-Stage, Calibration)\n",
        "- [ ] Results saved to Google Drive\n",
        "- [ ] Summary generated"
      ]
    }
  ]
}
