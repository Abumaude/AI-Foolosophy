{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abumaude/AI-Foolosophy/blob/main/dataset_loader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Dataset Loader and Ground Truth Pairing for FTE-HARM Validation\n",
        "\n",
        "This notebook implements a comprehensive dataset loading and ground truth pairing system for forensic log analysis validation. The system enables rigorous validation of **FTE-HARM** (Forensic Triage Entity - Hypothesis Assessment Risk Model) against known attack patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "**Why Ground Truth Pairing is Critical:**\n",
        "- **Validation Requirement:** FTE-HARM's hypothesis scoring must be validated against known attack patterns\n",
        "- **Ground Truth Necessity:** Without ground truth labels, we cannot measure precision, recall, or accuracy\n",
        "- **Dataset Pairing:** Log files and ground truth must be matched correctly to ensure evaluation validity\n",
        "- **Forensic Accountability:** Every triage decision must be traceable to verified evidence\n",
        "\n",
        "---\n",
        "\n",
        "## Supported Ground Truth Formats\n",
        "\n",
        "| Format | Extension | Example |\n",
        "|--------|-----------|--------|\n",
        "| JSON Labels (AIT) | `.log` | `{\"labels\": [\"attacker_vpn\"]}` or `{\"labels\": []}` |\n",
        "| Line-by-Line | `.log`, `.txt` | `benign,0,none` or `malicious,1,privilege_escalation` |\n",
        "| CSV with Line Numbers | `.csv` | `line_number,label,attack_type,confidence` |\n",
        "| JSON Temporal | `.json` | Attack windows with start/end times |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_header"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 1: Setup - Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mount_drive"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "module_header"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 2: Dataset Loader Module (Embedded)\n",
        "\n",
        "Run this cell to define all the dataset loading classes and functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dataset_loader_module"
      },
      "source": [
        "\"\"\"\n",
        "Dataset Loader and Ground Truth Pairing Module for FTE-HARM Validation\n",
        "\n",
        "This module provides functionality to:\n",
        "1. Scan forensic log dataset directories\n",
        "2. Pair log files with their corresponding ground truth annotation files\n",
        "3. Load and parse multiple ground truth formats (including AIT JSON labels)\n",
        "4. Validate dataset integrity\n",
        "5. Generate dataset statistics\n",
        "6. Iterate through matched log-ground truth pairs for FTE-HARM validation\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Any, Callable, Union\n",
        "from dataclasses import dataclass, field\n",
        "from enum import Enum\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "class GroundTruthFormat(Enum):\n",
        "    \"\"\"Supported ground truth file formats\"\"\"\n",
        "    LINE_BY_LINE = \"line_by_line\"  # Each line corresponds to same line in log\n",
        "    CSV = \"csv\"                     # CSV with explicit line numbers\n",
        "    JSON_TEMPORAL = \"json_temporal\"  # JSON with temporal attack windows\n",
        "    UNKNOWN = \"unknown\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DatasetConfig:\n",
        "    \"\"\"Configuration for dataset paths and pairing rules\"\"\"\n",
        "    DATASET_PATHS: Dict[str, str] = field(default_factory=lambda: {\n",
        "        'grp1': '/content/drive/My Drive/thesis/dataset/grp1',\n",
        "        'grp2': '/content/drive/My Drive/thesis/dataset/grp2'\n",
        "    })\n",
        "    LOG_EXTENSIONS: List[str] = field(default_factory=lambda: ['.log', '.txt'])\n",
        "    LABEL_PATTERNS: List[str] = field(default_factory=lambda: [\n",
        "        'label', 'labels', 'gt', 'ground_truth', 'annotation', 'truth'\n",
        "    ])\n",
        "    LABEL_EXTENSIONS: List[str] = field(default_factory=lambda: [\n",
        "        '.log', '.csv', '.json', '.txt'\n",
        "    ])\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATA CLASSES\n",
        "# =============================================================================\n",
        "\n",
        "@dataclass\n",
        "class GroundTruthEntry:\n",
        "    \"\"\"Single ground truth entry for a log line\"\"\"\n",
        "    label: str  # 'benign' or 'malicious'\n",
        "    binary: int  # 0 or 1\n",
        "    attack_type: str  # Classification (e.g., 'privilege_escalation')\n",
        "    confidence: float = 1.0\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "    @property\n",
        "    def is_malicious(self) -> bool:\n",
        "        \"\"\"Check if entry is malicious\"\"\"\n",
        "        return self.binary == 1 or self.label.lower() == 'malicious'\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class AttackWindow:\n",
        "    \"\"\"Temporal attack window annotation\"\"\"\n",
        "    start_time: str\n",
        "    end_time: str\n",
        "    attack_type: str\n",
        "    description: str = \"\"\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DatasetPair:\n",
        "    \"\"\"Paired log file and ground truth file\"\"\"\n",
        "    dataset_name: str\n",
        "    log_file: str\n",
        "    label_file: Optional[str]\n",
        "    paired: bool\n",
        "    base_path: str\n",
        "    log_line_count: int = 0\n",
        "    label_count: int = 0\n",
        "    ground_truth_format: GroundTruthFormat = GroundTruthFormat.UNKNOWN\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        status = \"PAIRED\" if self.paired else \"UNPAIRED\"\n",
        "        return f\"[{status}] {self.dataset_name}: {os.path.basename(self.log_file)}\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ValidationResult:\n",
        "    \"\"\"Result of dataset validation\"\"\"\n",
        "    valid: bool\n",
        "    errors: List[str] = field(default_factory=list)\n",
        "    warnings: List[str] = field(default_factory=list)\n",
        "\n",
        "    def add_error(self, error: str):\n",
        "        self.valid = False\n",
        "        self.errors.append(error)\n",
        "\n",
        "    def add_warning(self, warning: str):\n",
        "        self.warnings.append(warning)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DatasetStatistics:\n",
        "    \"\"\"Statistics for a dataset or collection of datasets\"\"\"\n",
        "    total_datasets: int = 0\n",
        "    paired_datasets: int = 0\n",
        "    unpaired_datasets: int = 0\n",
        "    total_log_lines: int = 0\n",
        "    total_malicious: int = 0\n",
        "    total_benign: int = 0\n",
        "    by_group: Dict[str, Dict[str, int]] = field(default_factory=dict)\n",
        "    by_attack_type: Dict[str, int] = field(default_factory=dict)\n",
        "\n",
        "    @property\n",
        "    def malicious_ratio(self) -> float:\n",
        "        if self.total_log_lines == 0:\n",
        "            return 0.0\n",
        "        return self.total_malicious / self.total_log_lines\n",
        "\n",
        "    @property\n",
        "    def benign_ratio(self) -> float:\n",
        "        if self.total_log_lines == 0:\n",
        "            return 0.0\n",
        "        return self.total_benign / self.total_log_lines\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATASET SCANNER\n",
        "# =============================================================================\n",
        "\n",
        "class DatasetScanner:\n",
        "    \"\"\"Scans dataset directories to identify log files and ground truth files\"\"\"\n",
        "\n",
        "    def __init__(self, config: Optional[DatasetConfig] = None):\n",
        "        self.config = config or DatasetConfig()\n",
        "\n",
        "    def scan_directory(self, base_path: str) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"Scan directory for log files and identify structure\"\"\"\n",
        "        datasets = {}\n",
        "\n",
        "        if not os.path.exists(base_path):\n",
        "            print(f\"Warning: Path does not exist: {base_path}\")\n",
        "            return datasets\n",
        "\n",
        "        for root, dirs, files in os.walk(base_path):\n",
        "            if files:\n",
        "                subdir = os.path.relpath(root, base_path)\n",
        "                if subdir == '.':\n",
        "                    subdir = os.path.basename(base_path)\n",
        "\n",
        "                log_files = self._identify_log_files(files)\n",
        "                label_files = self._identify_label_files(files)\n",
        "\n",
        "                if log_files or label_files:\n",
        "                    datasets[subdir] = {\n",
        "                        'path': root,\n",
        "                        'log_files': log_files,\n",
        "                        'label_files': label_files,\n",
        "                        'all_files': files\n",
        "                    }\n",
        "\n",
        "        return datasets\n",
        "\n",
        "    def scan_all_datasets(self, paths: Optional[Dict[str, str]] = None) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"Scan all configured dataset directories\"\"\"\n",
        "        paths = paths or self.config.DATASET_PATHS\n",
        "        all_datasets = {}\n",
        "\n",
        "        for group_name, group_path in paths.items():\n",
        "            print(f\"Scanning {group_name}: {group_path}\")\n",
        "            group_datasets = self.scan_directory(group_path)\n",
        "\n",
        "            for subdir, info in group_datasets.items():\n",
        "                key = f\"{group_name}/{subdir}\"\n",
        "                all_datasets[key] = info\n",
        "\n",
        "        return all_datasets\n",
        "\n",
        "    def _identify_log_files(self, files: List[str]) -> List[str]:\n",
        "        \"\"\"Identify log files from a list of files\"\"\"\n",
        "        log_files = []\n",
        "        for f in files:\n",
        "            if self._is_label_file(f):\n",
        "                continue\n",
        "            _, ext = os.path.splitext(f)\n",
        "            if ext.lower() in self.config.LOG_EXTENSIONS:\n",
        "                if f.startswith('log_') or not self._is_label_file(f):\n",
        "                    log_files.append(f)\n",
        "        return log_files\n",
        "\n",
        "    def _identify_label_files(self, files: List[str]) -> List[str]:\n",
        "        \"\"\"Identify ground truth/label files from a list of files\"\"\"\n",
        "        return [f for f in files if self._is_label_file(f)]\n",
        "\n",
        "    def _is_label_file(self, filename: str) -> bool:\n",
        "        \"\"\"Check if a file is likely a ground truth/label file\"\"\"\n",
        "        lower_name = filename.lower()\n",
        "        for pattern in self.config.LABEL_PATTERNS:\n",
        "            if pattern in lower_name:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# GROUND TRUTH PAIRING\n",
        "# =============================================================================\n",
        "\n",
        "class DatasetPairer:\n",
        "    \"\"\"Pairs log files with their corresponding ground truth files\"\"\"\n",
        "\n",
        "    def __init__(self, config: Optional[DatasetConfig] = None):\n",
        "        self.config = config or DatasetConfig()\n",
        "\n",
        "    def pair_log_with_groundtruth(self, log_file: str, label_files: List[str]) -> Optional[str]:\n",
        "        \"\"\"Match a log file with its ground truth file\"\"\"\n",
        "        if not label_files:\n",
        "            return None\n",
        "\n",
        "        log_basename = os.path.basename(log_file)\n",
        "        log_root = log_basename.replace('log_', '').replace('.log', '').replace('.txt', '')\n",
        "\n",
        "        # Rule 1: Direct prefix match (log_X -> label_X)\n",
        "        expected_label = log_basename.replace('log_', 'label_')\n",
        "        if expected_label in label_files:\n",
        "            return expected_label\n",
        "\n",
        "        # Rule 2: Root name with _labels suffix\n",
        "        for ext in ['.csv', '.log', '.txt', '.json']:\n",
        "            expected = f\"{log_root}_labels{ext}\"\n",
        "            if expected in label_files:\n",
        "                return expected\n",
        "\n",
        "        # Rule 3: Root name with _gt suffix\n",
        "        for ext in ['.csv', '.log', '.txt', '.json']:\n",
        "            expected = f\"{log_root}_gt{ext}\"\n",
        "            if expected in label_files:\n",
        "                return expected\n",
        "\n",
        "        # Rule 4: Root name contained in label file\n",
        "        for label_file in label_files:\n",
        "            if log_root in label_file:\n",
        "                return label_file\n",
        "\n",
        "        # Rule 5: Try matching without underscores\n",
        "        log_root_simple = log_root.replace('_', '')\n",
        "        for label_file in label_files:\n",
        "            label_simple = label_file.lower().replace('_', '')\n",
        "            if log_root_simple in label_simple:\n",
        "                return label_file\n",
        "\n",
        "        return None\n",
        "\n",
        "    def create_dataset_pairs(self, datasets: Dict[str, Dict[str, Any]]) -> List[DatasetPair]:\n",
        "        \"\"\"Create complete pairing of all log files with ground truth\"\"\"\n",
        "        pairs = []\n",
        "\n",
        "        for subdir, info in datasets.items():\n",
        "            for log_file in info['log_files']:\n",
        "                label_file = self.pair_log_with_groundtruth(log_file, info['label_files'])\n",
        "\n",
        "                log_path = os.path.join(info['path'], log_file)\n",
        "                label_path = os.path.join(info['path'], label_file) if label_file else None\n",
        "\n",
        "                pair = DatasetPair(\n",
        "                    dataset_name=subdir,\n",
        "                    log_file=log_path,\n",
        "                    label_file=label_path,\n",
        "                    paired=label_file is not None,\n",
        "                    base_path=info['path']\n",
        "                )\n",
        "\n",
        "                if label_file:\n",
        "                    pair.ground_truth_format = self._detect_format(label_file)\n",
        "\n",
        "                pairs.append(pair)\n",
        "\n",
        "        return pairs\n",
        "\n",
        "    def _detect_format(self, label_file: str) -> GroundTruthFormat:\n",
        "        \"\"\"Detect the format of a ground truth file\"\"\"\n",
        "        ext = os.path.splitext(label_file)[1].lower()\n",
        "        if ext == '.csv':\n",
        "            return GroundTruthFormat.CSV\n",
        "        elif ext == '.json':\n",
        "            return GroundTruthFormat.JSON_TEMPORAL\n",
        "        elif ext in ['.log', '.txt']:\n",
        "            return GroundTruthFormat.LINE_BY_LINE\n",
        "        return GroundTruthFormat.UNKNOWN\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# GROUND TRUTH LOADER\n",
        "# =============================================================================\n",
        "\n",
        "class GroundTruthLoader:\n",
        "    \"\"\"Loads and parses ground truth files in various formats\"\"\"\n",
        "\n",
        "    def load(self, label_file: str, format_hint: Optional[GroundTruthFormat] = None\n",
        "    ) -> Union[List[GroundTruthEntry], Dict[int, GroundTruthEntry], Dict[str, Any]]:\n",
        "        \"\"\"Auto-detect format and load ground truth\"\"\"\n",
        "        if not os.path.exists(label_file):\n",
        "            raise FileNotFoundError(f\"Ground truth file not found: {label_file}\")\n",
        "\n",
        "        if format_hint is None:\n",
        "            ext = os.path.splitext(label_file)[1].lower()\n",
        "            if ext == '.csv':\n",
        "                format_hint = GroundTruthFormat.CSV\n",
        "            elif ext == '.json':\n",
        "                format_hint = GroundTruthFormat.JSON_TEMPORAL\n",
        "            else:\n",
        "                format_hint = GroundTruthFormat.LINE_BY_LINE\n",
        "\n",
        "        if format_hint == GroundTruthFormat.CSV:\n",
        "            return self._load_csv(label_file)\n",
        "        elif format_hint == GroundTruthFormat.JSON_TEMPORAL:\n",
        "            return self._load_json(label_file)\n",
        "        else:\n",
        "            return self._load_line_by_line(label_file)\n",
        "\n",
        "    def _load_line_by_line(self, label_file: str) -> List[GroundTruthEntry]:\n",
        "        \"\"\"Load line-by-line ground truth\"\"\"\n",
        "        ground_truth = []\n",
        "\n",
        "        with open(label_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            for line_num, line in enumerate(f, 1):\n",
        "                line = line.strip()\n",
        "                if not line or line.startswith('#'):\n",
        "                    continue\n",
        "                entry = self._parse_line_entry(line, line_num)\n",
        "                ground_truth.append(entry)\n",
        "\n",
        "        return ground_truth\n",
        "\n",
        "    def _parse_line_entry(self, line: str, line_num: int) -> GroundTruthEntry:\n",
        "        \"\"\"Parse a single line-by-line entry\n",
        "\n",
        "        Supports multiple formats:\n",
        "        1. JSON format: {\"labels\": [\"attacker_vpn\", \"dnsteal\"]} or {\"labels\": []}\n",
        "        2. Simple CSV: benign,0,none or malicious,1,privilege_escalation\n",
        "        3. Binary only: 0 or 1\n",
        "        \"\"\"\n",
        "        line = line.strip()\n",
        "\n",
        "        # Try JSON format first (AIT dataset format)\n",
        "        if line.startswith('{'):\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                labels = data.get('labels', [])\n",
        "\n",
        "                # If labels list is not empty, it's malicious\n",
        "                if labels:\n",
        "                    attack_types = ','.join(labels)  # Join multiple labels\n",
        "                    return GroundTruthEntry(\n",
        "                        label='malicious',\n",
        "                        binary=1,\n",
        "                        attack_type=attack_types,\n",
        "                        metadata={'line_number': line_num, 'raw_labels': labels}\n",
        "                    )\n",
        "                else:\n",
        "                    return GroundTruthEntry(\n",
        "                        label='benign',\n",
        "                        binary=0,\n",
        "                        attack_type='none',\n",
        "                        metadata={'line_number': line_num, 'raw_labels': []}\n",
        "                    )\n",
        "            except json.JSONDecodeError:\n",
        "                pass  # Fall through to CSV parsing\n",
        "\n",
        "        # Try CSV format: label,binary,attack_type\n",
        "        parts = line.split(',')\n",
        "\n",
        "        if len(parts) >= 3:\n",
        "            try:\n",
        "                return GroundTruthEntry(\n",
        "                    label=parts[0].strip(),\n",
        "                    binary=int(parts[1].strip()),\n",
        "                    attack_type=parts[2].strip(),\n",
        "                    metadata={'line_number': line_num}\n",
        "                )\n",
        "            except ValueError:\n",
        "                pass  # Fall through\n",
        "\n",
        "        if len(parts) == 2:\n",
        "            label = parts[0].strip()\n",
        "            try:\n",
        "                binary = int(parts[1].strip())\n",
        "            except ValueError:\n",
        "                binary = 1 if label.lower() == 'malicious' else 0\n",
        "            return GroundTruthEntry(\n",
        "                label=label,\n",
        "                binary=binary,\n",
        "                attack_type='unknown',\n",
        "                metadata={'line_number': line_num}\n",
        "            )\n",
        "\n",
        "        # Single value - try to interpret\n",
        "        label = parts[0].strip().lower()\n",
        "        if label in ['0', '1']:\n",
        "            binary = int(label)\n",
        "            label = 'malicious' if binary == 1 else 'benign'\n",
        "        else:\n",
        "            binary = 1 if label == 'malicious' else 0\n",
        "\n",
        "        return GroundTruthEntry(\n",
        "            label=label,\n",
        "            binary=binary,\n",
        "            attack_type='unknown',\n",
        "            metadata={'line_number': line_num}\n",
        "        )\n",
        "\n",
        "    def _load_csv(self, label_file: str) -> Dict[int, GroundTruthEntry]:\n",
        "        \"\"\"Load CSV ground truth with line numbers\"\"\"\n",
        "        ground_truth = {}\n",
        "\n",
        "        with open(label_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            sample = f.read(1024)\n",
        "            f.seek(0)\n",
        "\n",
        "            delimiter = ','\n",
        "            if '\\t' in sample and ',' not in sample:\n",
        "                delimiter = '\\t'\n",
        "\n",
        "            reader = csv.DictReader(f, delimiter=delimiter)\n",
        "\n",
        "            for row in reader:\n",
        "                line_num = None\n",
        "                for key in ['line_number', 'line', 'line_num', 'lineno', 'idx', 'index']:\n",
        "                    if key in row:\n",
        "                        try:\n",
        "                            line_num = int(row[key])\n",
        "                            break\n",
        "                        except (ValueError, TypeError):\n",
        "                            continue\n",
        "\n",
        "                if line_num is None:\n",
        "                    continue\n",
        "\n",
        "                label = row.get('label', row.get('class', row.get('type', 'unknown')))\n",
        "                binary_str = row.get('binary', row.get('malicious', row.get('is_attack', '0')))\n",
        "                try:\n",
        "                    binary = int(binary_str)\n",
        "                except (ValueError, TypeError):\n",
        "                    binary = 1 if label.lower() == 'malicious' else 0\n",
        "\n",
        "                attack_type = row.get('attack_type', row.get('attack', row.get('category', 'unknown')))\n",
        "\n",
        "                try:\n",
        "                    confidence = float(row.get('confidence', row.get('score', 1.0)))\n",
        "                except (ValueError, TypeError):\n",
        "                    confidence = 1.0\n",
        "\n",
        "                ground_truth[line_num] = GroundTruthEntry(\n",
        "                    label=label,\n",
        "                    binary=binary,\n",
        "                    attack_type=attack_type,\n",
        "                    confidence=confidence,\n",
        "                    metadata={k: v for k, v in row.items()}\n",
        "                )\n",
        "\n",
        "        return ground_truth\n",
        "\n",
        "    def _load_json(self, label_file: str) -> Dict[str, Any]:\n",
        "        \"\"\"Load JSON temporal ground truth\"\"\"\n",
        "        with open(label_file, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if 'attack_windows' in data:\n",
        "            windows = []\n",
        "            for window in data['attack_windows']:\n",
        "                windows.append(AttackWindow(\n",
        "                    start_time=window.get('start_time', ''),\n",
        "                    end_time=window.get('end_time', ''),\n",
        "                    attack_type=window.get('attack_type', 'unknown'),\n",
        "                    description=window.get('description', ''),\n",
        "                    metadata={k: v for k, v in window.items()\n",
        "                             if k not in ['start_time', 'end_time', 'attack_type', 'description']}\n",
        "                ))\n",
        "            data['attack_windows_parsed'] = windows\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATASET VALIDATOR\n",
        "# =============================================================================\n",
        "\n",
        "class DatasetValidator:\n",
        "    \"\"\"Validates dataset integrity and pairing correctness\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.loader = GroundTruthLoader()\n",
        "\n",
        "    def validate_pair(self, pair: DatasetPair) -> ValidationResult:\n",
        "        \"\"\"Validate that log and ground truth files match correctly\"\"\"\n",
        "        result = ValidationResult(valid=True)\n",
        "\n",
        "        if not os.path.exists(pair.log_file):\n",
        "            result.add_error(f\"Log file not found: {pair.log_file}\")\n",
        "            return result\n",
        "\n",
        "        if not pair.paired or pair.label_file is None:\n",
        "            result.add_warning(\"No ground truth file paired with this log file\")\n",
        "            return result\n",
        "\n",
        "        if not os.path.exists(pair.label_file):\n",
        "            result.add_error(f\"Label file not found: {pair.label_file}\")\n",
        "            return result\n",
        "\n",
        "        log_lines = self._count_lines(pair.log_file)\n",
        "        pair.log_line_count = log_lines\n",
        "\n",
        "        try:\n",
        "            ground_truth = self.loader.load(pair.label_file, pair.ground_truth_format)\n",
        "        except Exception as e:\n",
        "            result.add_error(f\"Failed to load ground truth: {str(e)}\")\n",
        "            return result\n",
        "\n",
        "        if isinstance(ground_truth, list):\n",
        "            gt_lines = len(ground_truth)\n",
        "            pair.label_count = gt_lines\n",
        "\n",
        "            if gt_lines != log_lines:\n",
        "                result.add_error(\n",
        "                    f\"Line count mismatch: {log_lines} log lines, {gt_lines} labels\"\n",
        "                )\n",
        "\n",
        "            self._validate_entries(ground_truth, result)\n",
        "\n",
        "        elif isinstance(ground_truth, dict) and 'attack_windows' not in ground_truth:\n",
        "            pair.label_count = len(ground_truth)\n",
        "\n",
        "            if ground_truth:\n",
        "                max_line = max(ground_truth.keys())\n",
        "\n",
        "                if max_line > log_lines:\n",
        "                    result.add_error(\n",
        "                        f\"Ground truth references line {max_line}, \"\n",
        "                        f\"but log only has {log_lines} lines\"\n",
        "                    )\n",
        "\n",
        "                expected_lines = set(range(1, log_lines + 1))\n",
        "                labeled_lines = set(ground_truth.keys())\n",
        "                unlabeled = expected_lines - labeled_lines\n",
        "\n",
        "                if unlabeled:\n",
        "                    result.add_warning(\n",
        "                        f\"{len(unlabeled)} log lines have no ground truth label\"\n",
        "                    )\n",
        "\n",
        "                self._validate_entries(list(ground_truth.values()), result)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def validate_all(self, pairs: List[DatasetPair]) -> Dict[str, ValidationResult]:\n",
        "        \"\"\"Validate all dataset pairs\"\"\"\n",
        "        results = {}\n",
        "        for pair in pairs:\n",
        "            key = f\"{pair.dataset_name}/{os.path.basename(pair.log_file)}\"\n",
        "            results[key] = self.validate_pair(pair)\n",
        "        return results\n",
        "\n",
        "    def _count_lines(self, filepath: str) -> int:\n",
        "        \"\"\"Count non-empty lines in a file\"\"\"\n",
        "        count = 0\n",
        "        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    count += 1\n",
        "        return count\n",
        "\n",
        "    def _validate_entries(self, entries: List[GroundTruthEntry], result: ValidationResult):\n",
        "        \"\"\"Validate ground truth entries\"\"\"\n",
        "        valid_labels = {'benign', 'malicious', '0', '1', 'normal', 'attack', 'anomaly'}\n",
        "\n",
        "        for i, entry in enumerate(entries):\n",
        "            if entry.label.lower() not in valid_labels:\n",
        "                result.add_warning(f\"Entry {i}: Unusual label '{entry.label}'\")\n",
        "\n",
        "            if entry.binary not in [0, 1]:\n",
        "                result.add_error(f\"Entry {i}: Invalid binary value {entry.binary}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATASET STATISTICS\n",
        "# =============================================================================\n",
        "\n",
        "class DatasetStatsGenerator:\n",
        "    \"\"\"Generates comprehensive dataset statistics\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.loader = GroundTruthLoader()\n",
        "\n",
        "    def generate_stats(self, pairs: List[DatasetPair]) -> DatasetStatistics:\n",
        "        \"\"\"Generate comprehensive dataset statistics\"\"\"\n",
        "        stats = DatasetStatistics()\n",
        "        stats.total_datasets = len(pairs)\n",
        "        stats.paired_datasets = sum(1 for p in pairs if p.paired)\n",
        "        stats.unpaired_datasets = sum(1 for p in pairs if not p.paired)\n",
        "\n",
        "        for pair in pairs:\n",
        "            if not pair.paired or pair.label_file is None:\n",
        "                continue\n",
        "\n",
        "            group = pair.dataset_name\n",
        "            if group not in stats.by_group:\n",
        "                stats.by_group[group] = {\n",
        "                    'total_logs': 0,\n",
        "                    'total_malicious': 0,\n",
        "                    'total_benign': 0,\n",
        "                    'attack_types': {}\n",
        "                }\n",
        "\n",
        "            try:\n",
        "                ground_truth = self.loader.load(pair.label_file, pair.ground_truth_format)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not load {pair.label_file}: {e}\")\n",
        "                continue\n",
        "\n",
        "            entries = self._get_entries(ground_truth)\n",
        "\n",
        "            for entry in entries:\n",
        "                stats.total_log_lines += 1\n",
        "                stats.by_group[group]['total_logs'] += 1\n",
        "\n",
        "                if entry.is_malicious:\n",
        "                    stats.total_malicious += 1\n",
        "                    stats.by_group[group]['total_malicious'] += 1\n",
        "\n",
        "                    attack = entry.attack_type\n",
        "                    stats.by_attack_type[attack] = stats.by_attack_type.get(attack, 0) + 1\n",
        "                    stats.by_group[group]['attack_types'][attack] = \\\n",
        "                        stats.by_group[group]['attack_types'].get(attack, 0) + 1\n",
        "                else:\n",
        "                    stats.total_benign += 1\n",
        "                    stats.by_group[group]['total_benign'] += 1\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def _get_entries(self, ground_truth: Union[List, Dict]) -> List[GroundTruthEntry]:\n",
        "        \"\"\"Extract entries from ground truth in any format\"\"\"\n",
        "        if isinstance(ground_truth, list):\n",
        "            return ground_truth\n",
        "        elif isinstance(ground_truth, dict):\n",
        "            if 'attack_windows' in ground_truth:\n",
        "                return []\n",
        "            else:\n",
        "                return list(ground_truth.values())\n",
        "        return []\n",
        "\n",
        "    def print_report(self, stats: DatasetStatistics):\n",
        "        \"\"\"Print formatted dataset report\"\"\"\n",
        "        print(\"=\" * 80)\n",
        "        print(\"DATASET REPORT\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Total datasets: {stats.total_datasets}\")\n",
        "        print(f\"  Paired: {stats.paired_datasets}\")\n",
        "        print(f\"  Unpaired: {stats.unpaired_datasets}\")\n",
        "        print()\n",
        "        print(f\"Total log lines: {stats.total_log_lines}\")\n",
        "        print(f\"  Malicious: {stats.total_malicious} ({stats.malicious_ratio*100:.1f}%)\")\n",
        "        print(f\"  Benign: {stats.total_benign} ({stats.benign_ratio*100:.1f}%)\")\n",
        "        print()\n",
        "\n",
        "        if stats.by_group:\n",
        "            print(\"By Group:\")\n",
        "            for group, info in stats.by_group.items():\n",
        "                print(f\"  {group}:\")\n",
        "                print(f\"    Total logs: {info['total_logs']}\")\n",
        "                if info['total_logs'] > 0:\n",
        "                    mal_ratio = info['total_malicious'] / info['total_logs'] * 100\n",
        "                    ben_ratio = info['total_benign'] / info['total_logs'] * 100\n",
        "                    print(f\"    Malicious: {info['total_malicious']} ({mal_ratio:.1f}%)\")\n",
        "                    print(f\"    Benign: {info['total_benign']} ({ben_ratio:.1f}%)\")\n",
        "            print()\n",
        "\n",
        "        if stats.by_attack_type:\n",
        "            print(\"By Attack Type:\")\n",
        "            sorted_attacks = sorted(\n",
        "                stats.by_attack_type.items(),\n",
        "                key=lambda x: x[1],\n",
        "                reverse=True\n",
        "            )\n",
        "            for attack, count in sorted_attacks:\n",
        "                print(f\"  {attack}: {count}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATASET ITERATOR\n",
        "# =============================================================================\n",
        "\n",
        "class DatasetIterator:\n",
        "    \"\"\"Iterates through matched log-ground truth pairs for processing\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.loader = GroundTruthLoader()\n",
        "\n",
        "    def iterate_pairs(self, pairs: List[DatasetPair], process_fn: Optional[Callable] = None,\n",
        "                      verbose: bool = True) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Iterate through all paired datasets and process each log entry\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for pair in pairs:\n",
        "            if not pair.paired or pair.label_file is None:\n",
        "                continue\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Processing: {pair.dataset_name}\")\n",
        "\n",
        "            with open(pair.log_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                log_lines = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "            try:\n",
        "                ground_truth = self.loader.load(pair.label_file, pair.ground_truth_format)\n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    print(f\"  Warning: Could not load ground truth: {e}\")\n",
        "                continue\n",
        "\n",
        "            pair_results = self._process_entries(pair, log_lines, ground_truth, process_fn)\n",
        "            results.extend(pair_results)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _process_entries(self, pair: DatasetPair, log_lines: List[str],\n",
        "                         ground_truth: Union[List, Dict], process_fn: Optional[Callable]\n",
        "    ) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Process entries based on ground truth format\"\"\"\n",
        "        results = []\n",
        "\n",
        "        if isinstance(ground_truth, list):\n",
        "            for idx, (log_line, gt) in enumerate(zip(log_lines, ground_truth)):\n",
        "                result = None\n",
        "                if process_fn:\n",
        "                    result = process_fn(log_line, gt, idx + 1)\n",
        "\n",
        "                results.append({\n",
        "                    'dataset': pair.dataset_name,\n",
        "                    'line_number': idx + 1,\n",
        "                    'log_line': log_line,\n",
        "                    'ground_truth': gt,\n",
        "                    'result': result\n",
        "                })\n",
        "\n",
        "        elif isinstance(ground_truth, dict) and 'attack_windows' not in ground_truth:\n",
        "            for idx, log_line in enumerate(log_lines):\n",
        "                line_num = idx + 1\n",
        "                gt = ground_truth.get(\n",
        "                    line_num,\n",
        "                    GroundTruthEntry(label='unknown', binary=0, attack_type='unknown')\n",
        "                )\n",
        "\n",
        "                result = None\n",
        "                if process_fn:\n",
        "                    result = process_fn(log_line, gt, line_num)\n",
        "\n",
        "                results.append({\n",
        "                    'dataset': pair.dataset_name,\n",
        "                    'line_number': line_num,\n",
        "                    'log_line': log_line,\n",
        "                    'ground_truth': gt,\n",
        "                    'result': result\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# FTE-HARM VALIDATION WORKFLOW\n",
        "# =============================================================================\n",
        "\n",
        "class FTEHARMValidator:\n",
        "    \"\"\"Validation workflow for FTE-HARM hypothesis testing\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.iterator = DatasetIterator()\n",
        "\n",
        "    def validate(self, pairs: List[DatasetPair], entity_extractor: Callable,\n",
        "                 hypothesis_scorer: Callable, hypothesis_configs: Dict[str, Any],\n",
        "                 triage_threshold: float = 0.45) -> Dict[str, Any]:\n",
        "        \"\"\"Complete validation workflow for FTE-HARM\"\"\"\n",
        "        validation_results = {\n",
        "            'total': 0,\n",
        "            'true_positives': 0,\n",
        "            'false_positives': 0,\n",
        "            'true_negatives': 0,\n",
        "            'false_negatives': 0,\n",
        "            'by_hypothesis': {},\n",
        "            'by_attack_type': {},\n",
        "            'predictions': []\n",
        "        }\n",
        "\n",
        "        def process_entry(log_line, gt, line_num):\n",
        "            entities = entity_extractor(log_line)\n",
        "\n",
        "            hypothesis_scores = {}\n",
        "            for hyp_name, hyp_config in hypothesis_configs.items():\n",
        "                score_result = hypothesis_scorer(entities, hyp_config)\n",
        "                hypothesis_scores[hyp_name] = score_result.get('p_score', 0.0)\n",
        "\n",
        "            if hypothesis_scores:\n",
        "                best_hypothesis = max(hypothesis_scores, key=hypothesis_scores.get)\n",
        "                best_score = hypothesis_scores[best_hypothesis]\n",
        "            else:\n",
        "                best_hypothesis = None\n",
        "                best_score = 0.0\n",
        "\n",
        "            predicted_malicious = best_score >= triage_threshold\n",
        "            actual_malicious = gt.is_malicious if isinstance(gt, GroundTruthEntry) else \\\n",
        "                               gt.get('binary', 0) == 1 or gt.get('label', '').lower() == 'malicious'\n",
        "\n",
        "            return {\n",
        "                'predicted': predicted_malicious,\n",
        "                'actual': actual_malicious,\n",
        "                'best_hypothesis': best_hypothesis,\n",
        "                'best_score': best_score,\n",
        "                'all_scores': hypothesis_scores,\n",
        "                'attack_type': gt.attack_type if isinstance(gt, GroundTruthEntry) else gt.get('attack_type', 'unknown')\n",
        "            }\n",
        "\n",
        "        results = self.iterator.iterate_pairs(pairs, process_entry, verbose=True)\n",
        "\n",
        "        for entry in results:\n",
        "            result = entry['result']\n",
        "            if result is None:\n",
        "                continue\n",
        "\n",
        "            validation_results['total'] += 1\n",
        "            validation_results['predictions'].append(result)\n",
        "\n",
        "            predicted = result['predicted']\n",
        "            actual = result['actual']\n",
        "\n",
        "            if predicted and actual:\n",
        "                validation_results['true_positives'] += 1\n",
        "            elif predicted and not actual:\n",
        "                validation_results['false_positives'] += 1\n",
        "            elif not predicted and not actual:\n",
        "                validation_results['true_negatives'] += 1\n",
        "            else:\n",
        "                validation_results['false_negatives'] += 1\n",
        "\n",
        "            attack_type = result['attack_type']\n",
        "            if attack_type not in validation_results['by_attack_type']:\n",
        "                validation_results['by_attack_type'][attack_type] = {\n",
        "                    'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0\n",
        "                }\n",
        "\n",
        "            if predicted and actual:\n",
        "                validation_results['by_attack_type'][attack_type]['tp'] += 1\n",
        "            elif predicted and not actual:\n",
        "                validation_results['by_attack_type'][attack_type]['fp'] += 1\n",
        "            elif not predicted and not actual:\n",
        "                validation_results['by_attack_type'][attack_type]['tn'] += 1\n",
        "            else:\n",
        "                validation_results['by_attack_type'][attack_type]['fn'] += 1\n",
        "\n",
        "        self._calculate_metrics(validation_results)\n",
        "\n",
        "        return validation_results\n",
        "\n",
        "    def _calculate_metrics(self, results: Dict[str, Any]):\n",
        "        \"\"\"Calculate precision, recall, F1, accuracy\"\"\"\n",
        "        tp = results['true_positives']\n",
        "        fp = results['false_positives']\n",
        "        tn = results['true_negatives']\n",
        "        fn = results['false_negatives']\n",
        "\n",
        "        results['precision'] = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "        results['recall'] = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "\n",
        "        if results['precision'] + results['recall'] > 0:\n",
        "            results['f1_score'] = 2 * (results['precision'] * results['recall']) / \\\n",
        "                                  (results['precision'] + results['recall'])\n",
        "        else:\n",
        "            results['f1_score'] = 0.0\n",
        "\n",
        "        results['accuracy'] = (tp + tn) / results['total'] if results['total'] > 0 else 0.0\n",
        "\n",
        "        for attack_type, counts in results['by_attack_type'].items():\n",
        "            tp = counts['tp']\n",
        "            fp = counts['fp']\n",
        "            tn = counts['tn']\n",
        "            fn = counts['fn']\n",
        "\n",
        "            counts['precision'] = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "            counts['recall'] = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "\n",
        "            if counts['precision'] + counts['recall'] > 0:\n",
        "                counts['f1_score'] = 2 * (counts['precision'] * counts['recall']) / \\\n",
        "                                     (counts['precision'] + counts['recall'])\n",
        "            else:\n",
        "                counts['f1_score'] = 0.0\n",
        "\n",
        "    def print_validation_report(self, results: Dict[str, Any]):\n",
        "        \"\"\"Print formatted validation report\"\"\"\n",
        "        print(\"=\" * 80)\n",
        "        print(\"FTE-HARM VALIDATION REPORT\")\n",
        "        print(\"=\" * 80)\n",
        "        print()\n",
        "        print(\"Overall Metrics:\")\n",
        "        print(f\"  Total samples: {results['total']}\")\n",
        "        print(f\"  True Positives: {results['true_positives']}\")\n",
        "        print(f\"  False Positives: {results['false_positives']}\")\n",
        "        print(f\"  True Negatives: {results['true_negatives']}\")\n",
        "        print(f\"  False Negatives: {results['false_negatives']}\")\n",
        "        print()\n",
        "        print(f\"  Precision: {results['precision']:.4f}\")\n",
        "        print(f\"  Recall: {results['recall']:.4f}\")\n",
        "        print(f\"  F1 Score: {results['f1_score']:.4f}\")\n",
        "        print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
        "        print()\n",
        "\n",
        "        if results['by_attack_type']:\n",
        "            print(\"By Attack Type:\")\n",
        "            for attack_type, metrics in results['by_attack_type'].items():\n",
        "                total = metrics['tp'] + metrics['fp'] + metrics['tn'] + metrics['fn']\n",
        "                print(f\"  {attack_type} (n={total}):\")\n",
        "                print(f\"    Precision: {metrics['precision']:.4f}\")\n",
        "                print(f\"    Recall: {metrics['recall']:.4f}\")\n",
        "                print(f\"    F1 Score: {metrics['f1_score']:.4f}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CONVENIENCE FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def load_and_pair_datasets(paths: Optional[Dict[str, str]] = None,\n",
        "                           config: Optional[DatasetConfig] = None\n",
        ") -> Tuple[List[DatasetPair], DatasetStatistics]:\n",
        "    \"\"\"Convenience function to load and pair all datasets\"\"\"\n",
        "    config = config or DatasetConfig()\n",
        "    if paths:\n",
        "        config.DATASET_PATHS = paths\n",
        "\n",
        "    scanner = DatasetScanner(config)\n",
        "    datasets = scanner.scan_all_datasets()\n",
        "\n",
        "    pairer = DatasetPairer(config)\n",
        "    pairs = pairer.create_dataset_pairs(datasets)\n",
        "\n",
        "    stats_gen = DatasetStatsGenerator()\n",
        "    stats = stats_gen.generate_stats(pairs)\n",
        "\n",
        "    return pairs, stats\n",
        "\n",
        "\n",
        "def validate_datasets(pairs: List[DatasetPair]) -> Dict[str, ValidationResult]:\n",
        "    \"\"\"Convenience function to validate all dataset pairs\"\"\"\n",
        "    validator = DatasetValidator()\n",
        "    return validator.validate_all(pairs)\n",
        "\n",
        "\n",
        "def iterate_with_groundtruth(pairs: List[DatasetPair], process_fn: Callable\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"Convenience function to iterate through datasets\"\"\"\n",
        "    iterator = DatasetIterator()\n",
        "    return iterator.iterate_pairs(pairs, process_fn)\n",
        "\n",
        "\n",
        "print(\"Dataset Loader Module loaded successfully!\")\n",
        "print(\"Classes available: DatasetScanner, DatasetPairer, GroundTruthLoader,\")\n",
        "print(\"                   DatasetValidator, DatasetStatsGenerator, DatasetIterator,\")\n",
        "print(\"                   FTEHARMValidator\")\n",
        "print(\"Functions available: load_and_pair_datasets, validate_datasets, iterate_with_groundtruth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3_header"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 3: Configure Dataset Paths\n",
        "\n",
        "Configure the paths to your forensic log datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "configure_paths"
      },
      "source": [
        "# Configure dataset paths\n",
        "DATASET_PATHS = {\n",
        "    'grp1': '/content/drive/My Drive/thesis/dataset/grp1',\n",
        "    'grp2': '/content/drive/My Drive/thesis/dataset/grp2'\n",
        "}\n",
        "\n",
        "# Create custom configuration\n",
        "config = DatasetConfig()\n",
        "config.DATASET_PATHS = DATASET_PATHS\n",
        "\n",
        "# Display configuration\n",
        "print(\"Dataset Configuration:\")\n",
        "print(\"=\" * 60)\n",
        "for group, path in DATASET_PATHS.items():\n",
        "    exists = os.path.exists(path)\n",
        "    status = \"EXISTS\" if exists else \"NOT FOUND\"\n",
        "    print(f\"  {group}: {path} [{status}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_header"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 4: Scan Dataset Directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scan_directories"
      },
      "source": [
        "# Scan all dataset directories\n",
        "scanner = DatasetScanner(config)\n",
        "\n",
        "print(\"Scanning dataset directories...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "all_datasets = {}\n",
        "for group_name, group_path in DATASET_PATHS.items():\n",
        "    if os.path.exists(group_path):\n",
        "        group_datasets = scanner.scan_directory(group_path)\n",
        "        print(f\"\\n{group_name.upper()} ({group_path}):\")\n",
        "        \n",
        "        if not group_datasets:\n",
        "            print(\"  No datasets found\")\n",
        "        else:\n",
        "            for subdir, info in group_datasets.items():\n",
        "                print(f\"  {subdir}/\")\n",
        "                print(f\"    Log files: {len(info['log_files'])}\")\n",
        "                print(f\"    Label files: {len(info['label_files'])}\")\n",
        "                \n",
        "                all_datasets[f\"{group_name}/{subdir}\"] = info\n",
        "    else:\n",
        "        print(f\"\\n{group_name.upper()}: Directory not found\")\n",
        "\n",
        "print(f\"\\nTotal subdirectories scanned: {len(all_datasets)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5_header"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 5: Pair Log Files with Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "create_pairs"
      },
      "source": [
        "# Create dataset pairs\n",
        "pairer = DatasetPairer(config)\n",
        "\n",
        "if all_datasets:\n",
        "    dataset_pairs = pairer.create_dataset_pairs(all_datasets)\n",
        "    \n",
        "    print(\"Dataset Pairing Results:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    paired_count = sum(1 for p in dataset_pairs if p.paired)\n",
        "    unpaired_count = len(dataset_pairs) - paired_count\n",
        "    \n",
        "    print(f\"\\nTotal log files: {len(dataset_pairs)}\")\n",
        "    print(f\"Successfully paired: {paired_count}\")\n",
        "    print(f\"Unpaired: {unpaired_count}\")\n",
        "    \n",
        "    # Show only paired datasets\n",
        "    print(\"\\nPaired Datasets:\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for pair in dataset_pairs:\n",
        "        if pair.paired:\n",
        "            log_name = os.path.basename(pair.log_file)\n",
        "            label_name = os.path.basename(pair.label_file) if pair.label_file else \"N/A\"\n",
        "            format_name = pair.ground_truth_format.value\n",
        "            \n",
        "            print(f\"[PAIRED] {pair.dataset_name}\")\n",
        "            print(f\"  Log: {log_name}\")\n",
        "            print(f\"  Label: {label_name}\")\n",
        "            print(f\"  Format: {format_name}\")\n",
        "            print()\n",
        "else:\n",
        "    dataset_pairs = []\n",
        "    print(\"No datasets found to pair.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step6_header"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 6: Validate Dataset Integrity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "validate_datasets"
      },
      "source": [
        "# Validate all dataset pairs\n",
        "validator = DatasetValidator()\n",
        "\n",
        "if dataset_pairs:\n",
        "    print(\"Validating Dataset Pairs...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    validation_results = validator.validate_all(dataset_pairs)\n",
        "    \n",
        "    valid_count = sum(1 for r in validation_results.values() if r.valid)\n",
        "    invalid_count = sum(1 for r in validation_results.values() if not r.valid)\n",
        "    \n",
        "    print(f\"\\nValidation Summary:\")\n",
        "    print(f\"  Valid: {valid_count}\")\n",
        "    print(f\"  Invalid: {invalid_count}\")\n",
        "    \n",
        "    # Show only errors (not warnings)\n",
        "    print(\"\\nValidation Errors:\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    error_found = False\n",
        "    for dataset_name, result in validation_results.items():\n",
        "        if not result.valid:\n",
        "            error_found = True\n",
        "            print(f\"\\n[INVALID] {dataset_name}\")\n",
        "            for error in result.errors:\n",
        "                print(f\"  ERROR: {error}\")\n",
        "    \n",
        "    if not error_found:\n",
        "        print(\"No validation errors found!\")\n",
        "else:\n",
        "    print(\"No dataset pairs to validate.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step7_header"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 7: Generate Dataset Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "generate_stats"
      },
      "source": [
        "# Generate dataset statistics\n",
        "stats_generator = DatasetStatsGenerator()\n",
        "\n",
        "if dataset_pairs:\n",
        "    paired_datasets = [p for p in dataset_pairs if p.paired]\n",
        "    \n",
        "    if paired_datasets:\n",
        "        print(\"Generating Dataset Statistics...\")\n",
        "        stats = stats_generator.generate_stats(dataset_pairs)\n",
        "        \n",
        "        # Print detailed report\n",
        "        stats_generator.print_report(stats)\n",
        "    else:\n",
        "        print(\"No paired datasets available for statistics generation.\")\n",
        "else:\n",
        "    print(\"No datasets available for statistics.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step8_header"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 8: Iterate Through Dataset Pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iterate_pairs"
      },
      "source": [
        "# Example: Iterate through dataset pairs with a simple processor\n",
        "iterator = DatasetIterator()\n",
        "\n",
        "def example_processor(log_line, ground_truth, line_number):\n",
        "    \"\"\"Example processing function for each log entry.\"\"\"\n",
        "    if isinstance(ground_truth, GroundTruthEntry):\n",
        "        is_malicious = ground_truth.is_malicious\n",
        "        attack_type = ground_truth.attack_type\n",
        "    else:\n",
        "        is_malicious = ground_truth.get('binary', 0) == 1\n",
        "        attack_type = ground_truth.get('attack_type', 'unknown')\n",
        "    \n",
        "    return {\n",
        "        'line': line_number,\n",
        "        'log_preview': log_line[:80] + '...' if len(log_line) > 80 else log_line,\n",
        "        'is_malicious': is_malicious,\n",
        "        'attack_type': attack_type\n",
        "    }\n",
        "\n",
        "# Process only valid paired datasets\n",
        "if dataset_pairs:\n",
        "    valid_pairs = [p for p in dataset_pairs if p.paired]\n",
        "    \n",
        "    if valid_pairs:\n",
        "        print(\"Iterating through dataset pairs...\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        # Process first valid dataset as example\n",
        "        results = iterator.iterate_pairs(valid_pairs[:1], example_processor, verbose=True)\n",
        "        \n",
        "        print(f\"\\nProcessed {len(results)} log entries\")\n",
        "        \n",
        "        # Count malicious vs benign\n",
        "        malicious_count = sum(1 for r in results if r['result']['is_malicious'])\n",
        "        benign_count = len(results) - malicious_count\n",
        "        print(f\"Malicious: {malicious_count}\")\n",
        "        print(f\"Benign: {benign_count}\")\n",
        "        \n",
        "        print(\"\\nSample results (first 5 malicious):\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        shown = 0\n",
        "        for r in results:\n",
        "            if r['result']['is_malicious'] and shown < 5:\n",
        "                gt = r['result']\n",
        "                print(f\"Line {gt['line']} [{gt['attack_type']}]\")\n",
        "                print(f\"  {gt['log_preview']}\")\n",
        "                print()\n",
        "                shown += 1\n",
        "    else:\n",
        "        print(\"No valid paired datasets available for iteration.\")\n",
        "else:\n",
        "    print(\"No datasets available for iteration.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "checklist"
      },
      "source": [
        "---\n",
        "\n",
        "## Validation Checklist\n",
        "\n",
        "Before proceeding to FTE-HARM hypothesis testing, ensure:\n",
        "\n",
        "- [ ] Dataset directories scanned successfully\n",
        "- [ ] All log files identified\n",
        "- [ ] Ground truth files located\n",
        "- [ ] Log-ground truth pairing completed\n",
        "- [ ] Dataset integrity validated\n",
        "- [ ] No critical validation errors\n",
        "- [ ] Ground truth format understood (JSON labels with `{\"labels\": [...]}`)\n",
        "- [ ] Dataset statistics generated\n",
        "- [ ] Iteration workflow tested\n",
        "- [ ] Ready for FTE-HARM integration"
      ]
    }
  ]
}
