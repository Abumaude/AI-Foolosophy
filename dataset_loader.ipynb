{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abumaude/AI-Foolosophy/blob/main/dataset_loader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Dataset Loader and Ground Truth Pairing for FTE-HARM Validation\n",
        "\n",
        "This notebook implements a comprehensive dataset loading and ground truth pairing system for forensic log analysis validation. The system enables rigorous validation of **FTE-HARM** (Forensic Triage Entity - Hypothesis Assessment Risk Model) against known attack patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "**Why Ground Truth Pairing is Critical:**\n",
        "- **Validation Requirement:** FTE-HARM's hypothesis scoring must be validated against known attack patterns\n",
        "- **Ground Truth Necessity:** Without ground truth labels, we cannot measure precision, recall, or accuracy\n",
        "- **Dataset Pairing:** Log files and ground truth must be matched correctly to ensure evaluation validity\n",
        "- **Forensic Accountability:** Every triage decision must be traceable to verified evidence\n",
        "\n",
        "---\n",
        "\n",
        "## Supported Ground Truth Formats\n",
        "\n",
        "| Format | Extension | Example |\n",
        "|--------|-----------|--------|\n",
        "| Line-by-Line | `.log`, `.txt` | `benign,0,none` or `malicious,1,privilege_escalation` |\n",
        "| CSV with Line Numbers | `.csv` | `line_number,label,attack_type,confidence` |\n",
        "| JSON Temporal | `.json` | Attack windows with start/end times |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_header"
      },
      "source": [
        "---\n",
        "\n",
        "## Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mount_drive"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "import_module"
      },
      "source": [
        "# Import the dataset loader module\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"Not running in Colab - using local paths\")\n",
        "\n",
        "# Add module path - adjust based on where dataset_loader.py is located\n",
        "if IN_COLAB:\n",
        "    # Option 1: If module is in Google Drive\n",
        "    module_path = '/content/drive/My Drive/thesis'\n",
        "    if os.path.exists(module_path):\n",
        "        sys.path.insert(0, module_path)\n",
        "    \n",
        "    # Option 2: If module is in the cloned repo\n",
        "    repo_path = '/content/AI-Foolosophy'\n",
        "    if os.path.exists(repo_path):\n",
        "        sys.path.insert(0, repo_path)\n",
        "else:\n",
        "    # Local development\n",
        "    module_path = os.path.dirname(os.path.abspath('__file__'))\n",
        "    sys.path.insert(0, module_path)\n",
        "\n",
        "# Import the dataset loader\n",
        "from dataset_loader import (\n",
        "    DatasetConfig,\n",
        "    DatasetScanner,\n",
        "    DatasetPairer,\n",
        "    GroundTruthLoader,\n",
        "    DatasetValidator,\n",
        "    DatasetStatsGenerator,\n",
        "    DatasetIterator,\n",
        "    FTEHARMValidator,\n",
        "    load_and_pair_datasets,\n",
        "    validate_datasets,\n",
        "    iterate_with_groundtruth,\n",
        "    GroundTruthEntry,\n",
        "    DatasetPair,\n",
        "    ValidationResult,\n",
        "    DatasetStatistics,\n",
        "    GroundTruthFormat\n",
        ")\n",
        "\n",
        "print(\"Dataset loader module imported successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1_header"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 1: Configure Dataset Paths\n",
        "\n",
        "Configure the paths to your forensic log datasets. The default configuration expects the following structure:\n",
        "\n",
        "```\n",
        "/content/drive/My Drive/thesis/dataset/\n",
        "├── grp1/                    # Group 1: Primary datasets\n",
        "│   ├── rm/                  # RussellMitchell AITv2 dataset\n",
        "│   │   ├── log_auth.log     # SSH authentication logs (raw)\n",
        "│   │   ├── label_auth.log   # Ground truth for log_auth.log\n",
        "│   │   └── ...\n",
        "│   └── santos/              # Santos DNS exfiltration dataset\n",
        "│       ├── dns_queries.log  # DNS query logs (raw)\n",
        "│       ├── dns_labels.log   # Ground truth for dns_queries.log\n",
        "│       └── ...\n",
        "└── grp2/                    # Group 2: Secondary/validation datasets\n",
        "    └── ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "configure_paths"
      },
      "source": [
        "# Configure dataset paths\n",
        "# Modify these paths according to your directory structure\n",
        "\n",
        "DATASET_PATHS = {\n",
        "    'grp1': '/content/drive/My Drive/thesis/dataset/grp1',\n",
        "    'grp2': '/content/drive/My Drive/thesis/dataset/grp2'\n",
        "}\n",
        "\n",
        "# Create custom configuration\n",
        "config = DatasetConfig()\n",
        "config.DATASET_PATHS = DATASET_PATHS\n",
        "\n",
        "# Display configuration\n",
        "print(\"Dataset Configuration:\")\n",
        "print(\"=\" * 60)\n",
        "for group, path in DATASET_PATHS.items():\n",
        "    exists = os.path.exists(path)\n",
        "    status = \"EXISTS\" if exists else \"NOT FOUND\"\n",
        "    print(f\"  {group}: {path} [{status}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_header"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 2: Scan Dataset Directories\n",
        "\n",
        "Scan the configured directories to discover log files and potential ground truth files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scan_directories"
      },
      "source": [
        "# Scan all dataset directories\n",
        "scanner = DatasetScanner(config)\n",
        "\n",
        "print(\"Scanning dataset directories...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "all_datasets = {}\n",
        "for group_name, group_path in DATASET_PATHS.items():\n",
        "    if os.path.exists(group_path):\n",
        "        group_datasets = scanner.scan_directory(group_path)\n",
        "        print(f\"\\n{group_name.upper()} ({group_path}):\")\n",
        "        \n",
        "        if not group_datasets:\n",
        "            print(\"  No datasets found\")\n",
        "        else:\n",
        "            for subdir, info in group_datasets.items():\n",
        "                print(f\"  {subdir}/\")\n",
        "                print(f\"    Log files: {info['log_files']}\")\n",
        "                print(f\"    Label files: {info['label_files']}\")\n",
        "                \n",
        "                # Store with group prefix\n",
        "                all_datasets[f\"{group_name}/{subdir}\"] = info\n",
        "    else:\n",
        "        print(f\"\\n{group_name.upper()}: Directory not found\")\n",
        "\n",
        "print(f\"\\nTotal subdirectories scanned: {len(all_datasets)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3_header"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 3: Pair Log Files with Ground Truth\n",
        "\n",
        "Match each log file with its corresponding ground truth annotation file using multiple pairing rules:\n",
        "\n",
        "1. **Prefix match:** `log_X.log` → `label_X.log`\n",
        "2. **Suffix match:** `X.log` → `X_labels.csv`\n",
        "3. **Root name match:** `X.log` → `X_gt.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "create_pairs"
      },
      "source": [
        "# Create dataset pairs\n",
        "pairer = DatasetPairer(config)\n",
        "\n",
        "if all_datasets:\n",
        "    dataset_pairs = pairer.create_dataset_pairs(all_datasets)\n",
        "    \n",
        "    print(\"Dataset Pairing Results:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    paired_count = sum(1 for p in dataset_pairs if p.paired)\n",
        "    unpaired_count = len(dataset_pairs) - paired_count\n",
        "    \n",
        "    print(f\"\\nTotal log files: {len(dataset_pairs)}\")\n",
        "    print(f\"Successfully paired: {paired_count}\")\n",
        "    print(f\"Unpaired: {unpaired_count}\")\n",
        "    \n",
        "    print(\"\\nDetailed Pairing:\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for pair in dataset_pairs:\n",
        "        status = \"PAIRED\" if pair.paired else \"UNPAIRED\"\n",
        "        log_name = os.path.basename(pair.log_file)\n",
        "        label_name = os.path.basename(pair.label_file) if pair.label_file else \"N/A\"\n",
        "        format_name = pair.ground_truth_format.value if pair.paired else \"N/A\"\n",
        "        \n",
        "        print(f\"[{status}] {pair.dataset_name}\")\n",
        "        print(f\"  Log: {log_name}\")\n",
        "        print(f\"  Label: {label_name}\")\n",
        "        print(f\"  Format: {format_name}\")\n",
        "        print()\n",
        "else:\n",
        "    dataset_pairs = []\n",
        "    print(\"No datasets found to pair. Please check your dataset paths.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_header"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 4: Validate Dataset Integrity\n",
        "\n",
        "Validate that paired datasets are correctly matched:\n",
        "- Check that files exist\n",
        "- Verify line counts match (for line-by-line format)\n",
        "- Ensure all referenced lines exist (for CSV format)\n",
        "- Validate ground truth label values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "validate_datasets"
      },
      "source": [
        "# Validate all dataset pairs\n",
        "validator = DatasetValidator()\n",
        "\n",
        "if dataset_pairs:\n",
        "    print(\"Validating Dataset Pairs...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    validation_results = validator.validate_all(dataset_pairs)\n",
        "    \n",
        "    valid_count = sum(1 for r in validation_results.values() if r.valid)\n",
        "    invalid_count = len(validation_results) - valid_count\n",
        "    \n",
        "    print(f\"\\nValidation Summary:\")\n",
        "    print(f\"  Valid: {valid_count}\")\n",
        "    print(f\"  Invalid: {invalid_count}\")\n",
        "    \n",
        "    # Show details for any invalid or warning cases\n",
        "    print(\"\\nValidation Details:\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for dataset_name, result in validation_results.items():\n",
        "        if not result.valid:\n",
        "            print(f\"\\n[INVALID] {dataset_name}\")\n",
        "            for error in result.errors:\n",
        "                print(f\"  ERROR: {error}\")\n",
        "            for warning in result.warnings:\n",
        "                print(f\"  WARNING: {warning}\")\n",
        "        elif result.warnings:\n",
        "            print(f\"\\n[VALID with warnings] {dataset_name}\")\n",
        "            for warning in result.warnings:\n",
        "                print(f\"  WARNING: {warning}\")\n",
        "        else:\n",
        "            print(f\"[VALID] {dataset_name}\")\n",
        "else:\n",
        "    print(\"No dataset pairs to validate.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5_header"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 5: Generate Dataset Statistics\n",
        "\n",
        "Generate comprehensive statistics about the paired datasets, including:\n",
        "- Total log entries\n",
        "- Malicious vs benign distribution\n",
        "- Attack type breakdown\n",
        "- Per-group statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "generate_stats"
      },
      "source": [
        "# Generate dataset statistics\n",
        "stats_generator = DatasetStatsGenerator()\n",
        "\n",
        "if dataset_pairs:\n",
        "    paired_datasets = [p for p in dataset_pairs if p.paired]\n",
        "    \n",
        "    if paired_datasets:\n",
        "        print(\"Generating Dataset Statistics...\")\n",
        "        stats = stats_generator.generate_stats(dataset_pairs)\n",
        "        \n",
        "        # Print detailed report\n",
        "        stats_generator.print_report(stats)\n",
        "    else:\n",
        "        print(\"No paired datasets available for statistics generation.\")\n",
        "else:\n",
        "    print(\"No datasets available for statistics.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step6_header"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 6: Iterate Through Dataset Pairs\n",
        "\n",
        "Iterate through matched log-ground truth pairs for FTE-HARM processing. This demonstrates how to access each log entry along with its corresponding ground truth label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iterate_pairs"
      },
      "source": [
        "# Example: Iterate through dataset pairs with a simple processor\n",
        "iterator = DatasetIterator()\n",
        "\n",
        "def example_processor(log_line, ground_truth, line_number):\n",
        "    \"\"\"\n",
        "    Example processing function for each log entry.\n",
        "    \n",
        "    In real FTE-HARM usage, this would:\n",
        "    1. Extract entities from log_line using NER\n",
        "    2. Score all hypotheses\n",
        "    3. Make triage decision\n",
        "    \n",
        "    Returns processing result for collection.\n",
        "    \"\"\"\n",
        "    # Get ground truth values\n",
        "    if isinstance(ground_truth, GroundTruthEntry):\n",
        "        is_malicious = ground_truth.is_malicious\n",
        "        attack_type = ground_truth.attack_type\n",
        "    else:\n",
        "        is_malicious = ground_truth.get('binary', 0) == 1\n",
        "        attack_type = ground_truth.get('attack_type', 'unknown')\n",
        "    \n",
        "    return {\n",
        "        'line': line_number,\n",
        "        'log_preview': log_line[:50] + '...' if len(log_line) > 50 else log_line,\n",
        "        'is_malicious': is_malicious,\n",
        "        'attack_type': attack_type\n",
        "    }\n",
        "\n",
        "# Process a sample of entries (limit for demonstration)\n",
        "if dataset_pairs:\n",
        "    paired_datasets = [p for p in dataset_pairs if p.paired]\n",
        "    \n",
        "    if paired_datasets:\n",
        "        print(\"Iterating through dataset pairs...\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        # Process first dataset as example\n",
        "        results = iterator.iterate_pairs(paired_datasets[:1], example_processor, verbose=True)\n",
        "        \n",
        "        print(f\"\\nProcessed {len(results)} log entries\")\n",
        "        print(\"\\nSample results (first 5):\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        for r in results[:5]:\n",
        "            gt = r['result']\n",
        "            status = \"MALICIOUS\" if gt['is_malicious'] else \"BENIGN\"\n",
        "            print(f\"Line {gt['line']} [{status}] {gt['attack_type']}\")\n",
        "            print(f\"  {gt['log_preview']}\")\n",
        "            print()\n",
        "    else:\n",
        "        print(\"No paired datasets available for iteration.\")\n",
        "else:\n",
        "    print(\"No datasets available for iteration.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step7_header"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 7: FTE-HARM Validation Integration\n",
        "\n",
        "This section demonstrates how to integrate the dataset loader with FTE-HARM hypothesis validation. The `FTEHARMValidator` class provides a complete workflow for:\n",
        "\n",
        "1. Loading paired datasets\n",
        "2. Extracting entities from logs\n",
        "3. Scoring hypotheses\n",
        "4. Comparing predictions with ground truth\n",
        "5. Calculating validation metrics (precision, recall, F1, accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fte_harm_validation"
      },
      "source": [
        "# Example FTE-HARM Validation Integration\n",
        "# Note: This requires actual FTE-HARM entity extraction and hypothesis scoring functions\n",
        "\n",
        "# Placeholder functions for demonstration (replace with actual implementations)\n",
        "def placeholder_entity_extractor(log_line):\n",
        "    \"\"\"\n",
        "    Placeholder entity extractor.\n",
        "    \n",
        "    In real implementation, this would use the NER transformer model\n",
        "    to extract entities like UserName, IPAddress, ProcessName, etc.\n",
        "    \"\"\"\n",
        "    # Return mock entities for demonstration\n",
        "    return {\n",
        "        'UserName': 'admin' if 'admin' in log_line.lower() else 'user',\n",
        "        'ProcessName': 'sshd' if 'ssh' in log_line.lower() else 'unknown',\n",
        "        'IPAddress': '192.168.1.1'\n",
        "    }\n",
        "\n",
        "def placeholder_hypothesis_scorer(entities, hypothesis_config):\n",
        "    \"\"\"\n",
        "    Placeholder hypothesis scorer.\n",
        "    \n",
        "    In real implementation, this would calculate P(H|E) using\n",
        "    Bayesian inference with the hypothesis configuration.\n",
        "    \"\"\"\n",
        "    # Return mock score based on entities\n",
        "    if entities.get('UserName') == 'admin':\n",
        "        return {'p_score': 0.65}\n",
        "    return {'p_score': 0.25}\n",
        "\n",
        "# Example hypothesis configurations\n",
        "example_hypothesis_configs = {\n",
        "    'privilege_escalation': {\n",
        "        'name': 'Privilege Escalation',\n",
        "        'prior': 0.15,\n",
        "        'required_entities': ['UserName', 'ProcessName'],\n",
        "        'evidence_weights': {'UserName': 0.3, 'ProcessName': 0.4}\n",
        "    },\n",
        "    'lateral_movement': {\n",
        "        'name': 'Lateral Movement', \n",
        "        'prior': 0.10,\n",
        "        'required_entities': ['IPAddress', 'UserName'],\n",
        "        'evidence_weights': {'IPAddress': 0.5, 'UserName': 0.3}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Run validation (if datasets are available)\n",
        "if dataset_pairs:\n",
        "    paired_datasets = [p for p in dataset_pairs if p.paired]\n",
        "    \n",
        "    if paired_datasets:\n",
        "        print(\"Running FTE-HARM Validation...\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"(Using placeholder functions - replace with actual implementations)\")\n",
        "        print()\n",
        "        \n",
        "        fte_validator = FTEHARMValidator()\n",
        "        \n",
        "        # Run validation on a subset for demonstration\n",
        "        validation_results = fte_validator.validate(\n",
        "            pairs=paired_datasets[:1],  # Limit for demo\n",
        "            entity_extractor=placeholder_entity_extractor,\n",
        "            hypothesis_scorer=placeholder_hypothesis_scorer,\n",
        "            hypothesis_configs=example_hypothesis_configs,\n",
        "            triage_threshold=0.45\n",
        "        )\n",
        "        \n",
        "        # Print validation report\n",
        "        fte_validator.print_validation_report(validation_results)\n",
        "    else:\n",
        "        print(\"No paired datasets available for FTE-HARM validation.\")\n",
        "else:\n",
        "    print(\"No datasets available. Showing example validation report structure:\")\n",
        "    print()\n",
        "    print(\"=\" * 60)\n",
        "    print(\"FTE-HARM VALIDATION REPORT (EXAMPLE)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Metrics that would be calculated:\")\n",
        "    print(\"  - Precision: TP / (TP + FP)\")\n",
        "    print(\"  - Recall: TP / (TP + FN)\")  \n",
        "    print(\"  - F1 Score: 2 * (P * R) / (P + R)\")\n",
        "    print(\"  - Accuracy: (TP + TN) / Total\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quick_reference"
      },
      "source": [
        "---\n",
        "\n",
        "## Quick Reference: Convenience Functions\n",
        "\n",
        "The dataset loader provides convenient one-liner functions for common operations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "convenience_functions"
      },
      "source": [
        "# Quick Reference: One-liner convenience functions\n",
        "\n",
        "# 1. Load and pair all datasets in one call\n",
        "# pairs, stats = load_and_pair_datasets(DATASET_PATHS)\n",
        "\n",
        "# 2. Validate all pairs\n",
        "# validation_results = validate_datasets(pairs)\n",
        "\n",
        "# 3. Iterate with custom processor\n",
        "# results = iterate_with_groundtruth(pairs, my_processor_fn)\n",
        "\n",
        "print(\"Convenience Function Examples:\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "print(\"# Load and pair all datasets\")\n",
        "print(\"pairs, stats = load_and_pair_datasets(DATASET_PATHS)\")\n",
        "print()\n",
        "print(\"# Validate all pairs\")\n",
        "print(\"validation_results = validate_datasets(pairs)\")\n",
        "print()\n",
        "print(\"# Iterate with custom processor\")\n",
        "print(\"results = iterate_with_groundtruth(pairs, my_processor_fn)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "checklist"
      },
      "source": [
        "---\n",
        "\n",
        "## Validation Checklist\n",
        "\n",
        "Before proceeding to FTE-HARM hypothesis testing, ensure:\n",
        "\n",
        "- [ ] Dataset directories scanned successfully\n",
        "- [ ] All log files identified\n",
        "- [ ] Ground truth files located\n",
        "- [ ] Log-ground truth pairing completed\n",
        "- [ ] Dataset integrity validated\n",
        "- [ ] No line count mismatches\n",
        "- [ ] Ground truth format understood\n",
        "- [ ] Dataset statistics generated\n",
        "- [ ] Iteration workflow tested\n",
        "- [ ] Ready for FTE-HARM integration\n",
        "\n",
        "---\n",
        "\n",
        "## Expected Dataset Statistics\n",
        "\n",
        "| Dataset | Total Logs | Malicious % | Primary Attack Types |\n",
        "|---------|-----------|-------------|---------------------|\n",
        "| RussellMitchell AITv2 | ~50,000 | ~15% | privilege_escalation, lateral_movement |\n",
        "| Santos DNS | ~100,000 | ~5% | exfiltration, command_and_control |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "notes"
      },
      "source": [
        "---\n",
        "\n",
        "## Notes for Thesis\n",
        "\n",
        "**Methodological Contribution:**\n",
        "The dataset pairing framework ensures validation integrity by establishing explicit correspondence between raw forensic data and verified ground truth annotations. This addresses the methodological challenge of validating probabilistic triage systems against deterministic forensic requirements.\n",
        "\n",
        "**Technical Finding:**\n",
        "Line-by-line ground truth format (RussellMitchell) provides stronger validation guarantees than sparse annotation formats, ensuring every triage decision can be evaluated against known labels. However, CSV formats with confidence scores enable more nuanced evaluation of threshold calibration.\n",
        "\n",
        "**Practical Impact:**\n",
        "Dataset pairing automation reduces manual validation effort from hours to seconds, enabling comprehensive evaluation across multiple datasets and attack types. This is critical for demonstrating FTE-HARM's generalization capability beyond single-domain testing."
      ]
    }
  ]
}
