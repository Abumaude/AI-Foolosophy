{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abumaude/AI-Foolosophy/blob/main/MITRE_ATT%26CK_Hypothesis_Generator_FTE_HARM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MITRE ATT&CK Hypothesis Generator for FTE-HARM Framework\n",
        "\n",
        "This notebook bridges the **MITRE ATT&CK knowledge base** with the **FTE-HARM** (Forensic Triage Entity - Hypothesis-driven Automated Risk Measurement) hypothesis scoring system.\n",
        "\n",
        "## Purpose\n",
        "\n",
        "- **Manual Configuration Problem:** Creating 9+ hypotheses manually is time-consuming and error-prone\n",
        "- **Standardization Need:** MITRE ATT&CK provides industry-standard attack taxonomy\n",
        "- **Scalability:** Automate hypothesis generation for 100+ ATT&CK techniques\n",
        "- **Traceability:** Every hypothesis maps to verified ATT&CK technique IDs\n",
        "- **Evidence Mapping:** ATT&CK data sources map directly to forensic log entities\n",
        "\n",
        "## Workflow Overview\n",
        "\n",
        "```\n",
        "STAGE 1: Select Attack Scenarios → Choose relevant ATT&CK techniques\n",
        "STAGE 2: Extract ATT&CK Fields → Technique → Tactic → Data Sources\n",
        "STAGE 3: Map to FTE-HARM Entities → ATT&CK Data Sources → 22 Entity Types\n",
        "STAGE 4: Generate Hypothesis Weights → Automated weight distribution\n",
        "STAGE 5: Apply FTE-HARM Scoring → P_Score = (Σ(W_i × E_i)) × (1 - P_F)\n",
        "STAGE 6: Validate Against Ground Truth → Compare predictions\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Stage 0: Environment Setup\n",
        "\n",
        "Install required dependencies for Google Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (run this cell first in Colab)\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_packages():\n",
        "    \"\"\"Install required packages for Colab environment.\"\"\"\n",
        "    packages = [\n",
        "        'mitreattack-python',  # MITRE ATT&CK Python library\n",
        "        'stix2',               # STIX 2.0 support\n",
        "        'taxii2-client'        # TAXII 2.0 client\n",
        "    ]\n",
        "    \n",
        "    for package in packages:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
        "    \n",
        "    print(\"\\n✓ All packages installed successfully!\")\n",
        "\n",
        "# Uncomment to install (required on first run in Colab)\n",
        "# install_packages()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import json\n",
        "import os\n",
        "import urllib.request\n",
        "import warnings\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "\n",
        "print(\"✓ Core imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Stage 1: Data Classes and Enums\n",
        "\n",
        "Define the core data structures for hypothesis configuration and scoring results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConfidenceLevel(Enum):\n",
        "    \"\"\"P_Score confidence levels for triage decisions.\"\"\"\n",
        "    HIGH = \"HIGH\"\n",
        "    MEDIUM = \"MEDIUM\"\n",
        "    LOW = \"LOW\"\n",
        "    INSUFFICIENT = \"INSUFFICIENT\"\n",
        "\n",
        "\n",
        "class TriageDecision(Enum):\n",
        "    \"\"\"Triage action based on confidence level.\"\"\"\n",
        "    INVESTIGATE_IMMEDIATE = \"INVESTIGATE_IMMEDIATE\"\n",
        "    INVESTIGATE_PRIORITY = \"INVESTIGATE_PRIORITY\"\n",
        "    INVESTIGATE_STANDARD = \"INVESTIGATE_STANDARD\"\n",
        "    MONITOR = \"MONITOR\"\n",
        "    ARCHIVE = \"ARCHIVE\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TechniqueDetails:\n",
        "    \"\"\"Container for extracted MITRE ATT&CK technique details.\"\"\"\n",
        "    id: str\n",
        "    name: str\n",
        "    description: str\n",
        "    tactics: List[str]\n",
        "    data_sources: List[str]\n",
        "    data_components: List[str]\n",
        "    platforms: List[str]\n",
        "    detection: str\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"{self.id}: {self.name}\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class HypothesisConfig:\n",
        "    \"\"\"FTE-HARM hypothesis configuration derived from ATT&CK technique.\"\"\"\n",
        "    name: str\n",
        "    mitre_technique: str\n",
        "    mitre_tactic: str\n",
        "    description: str\n",
        "    weights: Dict[str, float]\n",
        "    critical_entity: str\n",
        "    penalty_factor: float\n",
        "    threshold: float\n",
        "    data_sources: List[str]\n",
        "    platforms: List[str]\n",
        "\n",
        "    def validate(self) -> bool:\n",
        "        \"\"\"Validate that weights sum approximately to 1.0.\"\"\"\n",
        "        total = sum(self.weights.values())\n",
        "        return 0.95 <= total <= 1.05\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PScoreResult:\n",
        "    \"\"\"Result of P_Score calculation.\"\"\"\n",
        "    p_score: float\n",
        "    confidence: ConfidenceLevel\n",
        "    triage_decision: TriageDecision\n",
        "    entities_present: List[str]\n",
        "    entities_missing: List[str]\n",
        "    critical_entity_present: bool\n",
        "    mitre_technique: str\n",
        "    mitre_tactic: str\n",
        "    weighted_contributions: Dict[str, float]\n",
        "\n",
        "\n",
        "print(\"✓ Data classes defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Stage 2: Entity Mapping Configuration\n",
        "\n",
        "Define the comprehensive mapping between ATT&CK data sources and FTE-HARM's 22 entity types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive mapping between ATT&CK data sources and FTE-HARM's 22 entity types\n",
        "ATTACK_TO_ENTITY_MAPPING: Dict[str, List[str]] = {\n",
        "    # Process-related data sources\n",
        "    'Process': ['Process', 'ProcessID'],\n",
        "    'Process Creation': ['Process', 'ProcessID', 'DateTime'],\n",
        "    'Process Termination': ['Process', 'ProcessID', 'DateTime'],\n",
        "    'Command': ['Process', 'Action'],\n",
        "    'Command Execution': ['Process', 'Action', 'Username'],\n",
        "    'OS API Execution': ['Process', 'Action'],\n",
        "    'Script Execution': ['Process', 'Action', 'Object'],\n",
        "\n",
        "    # Authentication and User data sources\n",
        "    'User Account': ['Username'],\n",
        "    'User Account Authentication': ['Username', 'Action', 'Status', 'AuthenticationType'],\n",
        "    'User Account Creation': ['Username', 'DateTime', 'Action'],\n",
        "    'User Account Deletion': ['Username', 'DateTime', 'Action'],\n",
        "    'User Account Modification': ['Username', 'DateTime', 'Action'],\n",
        "    'Logon Session': ['Username', 'DateTime', 'SessionID'],\n",
        "    'Logon Session Creation': ['Username', 'DateTime', 'SessionID', 'Action'],\n",
        "    'Logon Session Metadata': ['Username', 'SessionID'],\n",
        "    'Active Directory': ['Username', 'Object'],\n",
        "\n",
        "    # Network data sources\n",
        "    'Network Traffic': ['IPAddress', 'Port', 'Protocol'],\n",
        "    'Network Traffic Content': ['IPAddress', 'Port', 'Protocol', 'DNSName'],\n",
        "    'Network Traffic Flow': ['IPAddress', 'Port', 'Protocol', 'ByteCount'],\n",
        "    'Network Connection Creation': ['IPAddress', 'Port', 'Protocol', 'DateTime'],\n",
        "    'Network Connection': ['IPAddress', 'Port', 'Protocol'],\n",
        "    'Network Share': ['IPAddress', 'Object', 'Username'],\n",
        "\n",
        "    # DNS data sources\n",
        "    'DNS': ['DNSName', 'IPAddress', 'Action'],\n",
        "    'Domain Name': ['DNSName'],\n",
        "\n",
        "    # File and Object data sources\n",
        "    'File': ['Object', 'DateTime'],\n",
        "    'File Access': ['Object', 'DateTime', 'Username', 'Action'],\n",
        "    'File Creation': ['Object', 'DateTime', 'Action'],\n",
        "    'File Deletion': ['Object', 'DateTime', 'Action'],\n",
        "    'File Metadata': ['Object'],\n",
        "    'File Modification': ['Object', 'DateTime', 'Action'],\n",
        "    'Windows Registry': ['Object', 'Action'],\n",
        "    'Windows Registry Key Access': ['Object', 'Action', 'DateTime'],\n",
        "    'Windows Registry Key Creation': ['Object', 'Action', 'DateTime'],\n",
        "    'Windows Registry Key Deletion': ['Object', 'Action', 'DateTime'],\n",
        "    'Windows Registry Key Modification': ['Object', 'Action', 'DateTime'],\n",
        "\n",
        "    # Service and Application data sources\n",
        "    'Application Log': ['DateTime', 'Severity', 'Service', 'Action'],\n",
        "    'Application Log Content': ['DateTime', 'Severity', 'Service'],\n",
        "    'Service': ['Service', 'Process'],\n",
        "    'Service Creation': ['Service', 'Process', 'DateTime'],\n",
        "    'Service Metadata': ['Service'],\n",
        "    'Service Modification': ['Service', 'DateTime', 'Action'],\n",
        "\n",
        "    # Driver and Kernel data sources\n",
        "    'Driver': ['Process', 'Object'],\n",
        "    'Driver Load': ['Process', 'Object', 'DateTime'],\n",
        "    'Kernel': ['Process'],\n",
        "    'Module': ['Process', 'Object'],\n",
        "    'Module Load': ['Process', 'Object', 'DateTime'],\n",
        "\n",
        "    # Scheduled Task and WMI\n",
        "    'Scheduled Job': ['Process', 'DateTime', 'Action'],\n",
        "    'Scheduled Job Creation': ['Process', 'DateTime', 'Action'],\n",
        "    'WMI': ['Process', 'Action'],\n",
        "    'WMI Creation': ['Process', 'DateTime', 'Action'],\n",
        "\n",
        "    # Firewall\n",
        "    'Firewall': ['IPAddress', 'Port', 'Protocol', 'Action'],\n",
        "    'Firewall Disable': ['Action', 'DateTime'],\n",
        "    'Firewall Rule Modification': ['Action', 'DateTime', 'Object'],\n",
        "\n",
        "    # Error and Status\n",
        "    'Error': ['Error', 'Status'],\n",
        "    'Status': ['Status', 'Severity'],\n",
        "    'Sensor Health': ['Status', 'DateTime', 'Service'],\n",
        "    'Host Status': ['Status', 'DateTime']\n",
        "}\n",
        "\n",
        "# The complete set of 22 FTE-HARM entity types\n",
        "FTE_HARM_ENTITIES = [\n",
        "    'DateTime', 'System', 'Service', 'Process', 'ProcessID',\n",
        "    'Username', 'Message', 'IPAddress', 'Port', 'Protocol',\n",
        "    'DNSName', 'URL', 'Object', 'Action', 'Status',\n",
        "    'Error', 'Severity', 'SessionID', 'AuthenticationType',\n",
        "    'ByteCount', 'EventID', 'LogLevel'\n",
        "]\n",
        "\n",
        "print(f\"✓ Entity mapping configured: {len(ATTACK_TO_ENTITY_MAPPING)} ATT&CK data sources\")\n",
        "print(f\"✓ FTE-HARM entity types: {len(FTE_HARM_ENTITIES)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tactic-specific threshold mappings\n",
        "TACTIC_THRESHOLDS: Dict[str, float] = {\n",
        "    'Privilege Escalation': 0.50,\n",
        "    'Lateral Movement': 0.50,\n",
        "    'Exfiltration': 0.55,\n",
        "    'Command And Control': 0.55,\n",
        "    'Initial Access': 0.50,\n",
        "    'Discovery': 0.45,\n",
        "    'Persistence': 0.50,\n",
        "    'Defense Evasion': 0.50,\n",
        "    'Credential Access': 0.50,\n",
        "    'Execution': 0.45,\n",
        "    'Collection': 0.50,\n",
        "    'Impact': 0.55,\n",
        "    'Resource Development': 0.45,\n",
        "    'Reconnaissance': 0.40\n",
        "}\n",
        "\n",
        "# Critical entity indicators based on technique characteristics\n",
        "CRITICAL_ENTITY_INDICATORS: Dict[str, List[str]] = {\n",
        "    'Process': ['sudo', 'su', 'process', 'command', 'execution', 'spawn', 'shell', 'script'],\n",
        "    'Username': ['user', 'account', 'privilege', 'escalation', 'credential', 'authentication', 'login'],\n",
        "    'DNSName': ['dns', 'domain', 'query', 'resolution', 'dga', 'beacon'],\n",
        "    'IPAddress': ['network', 'connection', 'traffic', 'address', 'remote', 'c2', 'exfil'],\n",
        "    'Action': ['authentication', 'login', 'access', 'failed', 'successful', 'create', 'modify', 'delete'],\n",
        "    'Object': ['file', 'registry', 'key', 'path', 'artifact'],\n",
        "    'Service': ['service', 'daemon', 'application']\n",
        "}\n",
        "\n",
        "print(f\"✓ Tactic thresholds configured: {len(TACTIC_THRESHOLDS)} tactics\")\n",
        "print(f\"✓ Critical entity indicators: {len(CRITICAL_ENTITY_INDICATORS)} entity types\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Stage 3: Attack Scenario Definitions\n",
        "\n",
        "Define predefined attack scenarios with relevant MITRE ATT&CK technique IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ATTACK_SCENARIOS: Dict[str, List[str]] = {\n",
        "    'privilege_escalation': [\n",
        "        'T1548.003',  # Sudo and Su\n",
        "        'T1548.002',  # Bypass User Account Control\n",
        "        'T1068',      # Exploitation for Privilege Escalation\n",
        "        'T1548.001',  # Setuid and Setgid\n",
        "        'T1134.001',  # Token Impersonation/Theft\n",
        "        'T1055',      # Process Injection\n",
        "    ],\n",
        "\n",
        "    'lateral_movement': [\n",
        "        'T1021.004',  # SSH\n",
        "        'T1021.002',  # SMB/Windows Admin Shares\n",
        "        'T1021.001',  # Remote Desktop Protocol\n",
        "        'T1563.001',  # SSH Hijacking\n",
        "        'T1563.002',  # RDP Hijacking\n",
        "        'T1210',      # Exploitation of Remote Services\n",
        "    ],\n",
        "\n",
        "    'exfiltration': [\n",
        "        'T1048.003',  # Exfiltration Over Unencrypted Non-C2 Protocol\n",
        "        'T1041',      # Exfiltration Over C2 Channel\n",
        "        'T1567.002',  # Exfiltration to Cloud Storage\n",
        "        'T1048.001',  # Exfiltration Over Symmetric Encrypted Non-C2 Protocol\n",
        "        'T1020',      # Automated Exfiltration\n",
        "        'T1030',      # Data Transfer Size Limits\n",
        "    ],\n",
        "\n",
        "    'dns_abuse': [\n",
        "        'T1071.004',  # DNS Application Layer Protocol\n",
        "        'T1568.002',  # Domain Generation Algorithms\n",
        "        'T1584.001',  # Compromise Infrastructure: Domains\n",
        "        'T1583.001',  # Acquire Infrastructure: Domains\n",
        "        'T1568.001',  # Fast Flux DNS\n",
        "        'T1071.001',  # Web Protocols\n",
        "    ],\n",
        "\n",
        "    'credential_access': [\n",
        "        'T1110.001',  # Brute Force: Password Guessing\n",
        "        'T1110.003',  # Brute Force: Password Spraying\n",
        "        'T1552.001',  # Unsecured Credentials: Credentials In Files\n",
        "        'T1003.001',  # OS Credential Dumping: LSASS Memory\n",
        "        'T1558.003',  # Kerberoasting\n",
        "        'T1555.003',  # Credentials from Web Browsers\n",
        "    ],\n",
        "\n",
        "    'command_and_control': [\n",
        "        'T1071.001',  # Web Protocols\n",
        "        'T1071.004',  # DNS\n",
        "        'T1573.001',  # Encrypted Channel: Symmetric Cryptography\n",
        "        'T1105',      # Ingress Tool Transfer\n",
        "        'T1571',      # Non-Standard Port\n",
        "        'T1572',      # Protocol Tunneling\n",
        "    ],\n",
        "\n",
        "    'persistence': [\n",
        "        'T1053.005',  # Scheduled Task/Job: Scheduled Task\n",
        "        'T1547.001',  # Boot or Logon Autostart Execution: Registry Run Keys\n",
        "        'T1543.003',  # Create or Modify System Process: Windows Service\n",
        "        'T1136.001',  # Create Account: Local Account\n",
        "        'T1078.003',  # Valid Accounts: Local Accounts\n",
        "        'T1505.003',  # Server Software Component: Web Shell\n",
        "    ],\n",
        "\n",
        "    'defense_evasion': [\n",
        "        'T1070.001',  # Indicator Removal: Clear Windows Event Logs\n",
        "        'T1562.001',  # Impair Defenses: Disable or Modify Tools\n",
        "        'T1036.005',  # Masquerading: Match Legitimate Name or Location\n",
        "        'T1027',      # Obfuscated Files or Information\n",
        "        'T1218.011',  # System Binary Proxy Execution: Rundll32\n",
        "        'T1140',      # Deobfuscate/Decode Files or Information\n",
        "    ],\n",
        "\n",
        "    'discovery': [\n",
        "        'T1087.001',  # Account Discovery: Local Account\n",
        "        'T1083',      # File and Directory Discovery\n",
        "        'T1057',      # Process Discovery\n",
        "        'T1018',      # Remote System Discovery\n",
        "        'T1082',      # System Information Discovery\n",
        "        'T1049',      # System Network Connections Discovery\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"✓ Attack scenarios defined:\")\n",
        "for scenario, techniques in ATTACK_SCENARIOS.items():\n",
        "    print(f\"  - {scenario}: {len(techniques)} techniques\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Stage 4: MITRE ATT&CK Data Loader\n",
        "\n",
        "Load ATT&CK STIX data from local file or download from MITRE's official repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MitreAttackLoader:\n",
        "    \"\"\"\n",
        "    Loader for MITRE ATT&CK STIX data.\n",
        "    Can load from local file or download from MITRE's official repository.\n",
        "    \"\"\"\n",
        "\n",
        "    ATTACK_STIX_URL = \"https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json\"\n",
        "\n",
        "    def __init__(self, data_path: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize the ATT&CK data loader.\n",
        "        \n",
        "        Args:\n",
        "            data_path: Path to local enterprise-attack.json file.\n",
        "                      If None, will download from MITRE.\n",
        "        \"\"\"\n",
        "        self.data_path = data_path\n",
        "        self.stix_data = None\n",
        "        self._techniques_cache: Dict[str, Dict] = {}\n",
        "        self._tactics_cache: Dict[str, Dict] = {}\n",
        "        self._data_sources_cache: Dict[str, Dict] = {}\n",
        "\n",
        "    def load(self) -> bool:\n",
        "        \"\"\"Load ATT&CK STIX data from file or download.\"\"\"\n",
        "        try:\n",
        "            if self.data_path and os.path.exists(self.data_path):\n",
        "                print(f\"Loading ATT&CK data from {self.data_path}...\")\n",
        "                with open(self.data_path, 'r', encoding='utf-8') as f:\n",
        "                    self.stix_data = json.load(f)\n",
        "            else:\n",
        "                print(\"Downloading ATT&CK data from MITRE repository...\")\n",
        "                self._download_attack_data()\n",
        "\n",
        "            self._build_caches()\n",
        "            print(f\"✓ Loaded {len(self._techniques_cache)} techniques\")\n",
        "            print(f\"✓ Loaded {len(self._tactics_cache)} tactics\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Failed to load ATT&CK data: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _download_attack_data(self):\n",
        "        \"\"\"Download ATT&CK STIX data from MITRE's repository.\"\"\"\n",
        "        try:\n",
        "            with urllib.request.urlopen(self.ATTACK_STIX_URL, timeout=60) as response:\n",
        "                data = response.read().decode('utf-8')\n",
        "                self.stix_data = json.loads(data)\n",
        "\n",
        "                # Save for future use\n",
        "                save_path = \"enterprise-attack.json\"\n",
        "                with open(save_path, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(self.stix_data, f)\n",
        "                print(f\"✓ Saved ATT&CK data to {save_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to download ATT&CK data: {e}\")\n",
        "\n",
        "    def _build_caches(self):\n",
        "        \"\"\"Build lookup caches for techniques, tactics, and data sources.\"\"\"\n",
        "        if not self.stix_data or 'objects' not in self.stix_data:\n",
        "            raise ValueError(\"No STIX data loaded\")\n",
        "\n",
        "        for obj in self.stix_data['objects']:\n",
        "            obj_type = obj.get('type', '')\n",
        "\n",
        "            if obj_type == 'attack-pattern':\n",
        "                external_refs = obj.get('external_references', [])\n",
        "                for ref in external_refs:\n",
        "                    if ref.get('source_name') == 'mitre-attack':\n",
        "                        technique_id = ref.get('external_id', '')\n",
        "                        if technique_id:\n",
        "                            self._techniques_cache[technique_id] = obj\n",
        "                            break\n",
        "\n",
        "            elif obj_type == 'x-mitre-tactic':\n",
        "                external_refs = obj.get('external_references', [])\n",
        "                for ref in external_refs:\n",
        "                    if ref.get('source_name') == 'mitre-attack':\n",
        "                        tactic_id = ref.get('external_id', '')\n",
        "                        if tactic_id:\n",
        "                            self._tactics_cache[tactic_id] = obj\n",
        "                            break\n",
        "\n",
        "            elif obj_type == 'x-mitre-data-source':\n",
        "                name = obj.get('name', '')\n",
        "                if name:\n",
        "                    self._data_sources_cache[name] = obj\n",
        "\n",
        "    def get_technique(self, technique_id: str) -> Optional[Dict]:\n",
        "        \"\"\"Get technique by ID (e.g., 'T1548.003').\"\"\"\n",
        "        return self._techniques_cache.get(technique_id)\n",
        "\n",
        "    def get_all_techniques(self) -> List[Dict]:\n",
        "        \"\"\"Get all techniques.\"\"\"\n",
        "        return list(self._techniques_cache.values())\n",
        "\n",
        "    def get_all_technique_ids(self) -> List[str]:\n",
        "        \"\"\"Get all technique IDs.\"\"\"\n",
        "        return list(self._techniques_cache.keys())\n",
        "\n",
        "    def search_techniques(self, keyword: str) -> List[Tuple[str, str]]:\n",
        "        \"\"\"Search techniques by keyword in name or description.\"\"\"\n",
        "        results = []\n",
        "        keyword_lower = keyword.lower()\n",
        "\n",
        "        for tech_id, tech in self._techniques_cache.items():\n",
        "            name = tech.get('name', '').lower()\n",
        "            description = tech.get('description', '').lower()\n",
        "\n",
        "            if keyword_lower in name or keyword_lower in description:\n",
        "                results.append((tech_id, tech.get('name', '')))\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "print(\"✓ MitreAttackLoader class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Stage 5: Technique Extractor\n",
        "\n",
        "Extract comprehensive details from ATT&CK techniques including tactics, data sources, and platforms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TechniqueExtractor:\n",
        "    \"\"\"Extract comprehensive details from ATT&CK techniques.\"\"\"\n",
        "\n",
        "    def __init__(self, attack_loader: MitreAttackLoader):\n",
        "        self.loader = attack_loader\n",
        "\n",
        "    def extract(self, technique_id: str) -> Optional[TechniqueDetails]:\n",
        "        \"\"\"\n",
        "        Extract comprehensive details for a specific ATT&CK technique.\n",
        "        \n",
        "        Args:\n",
        "            technique_id: ATT&CK technique ID (e.g., \"T1548.003\").\n",
        "        \n",
        "        Returns:\n",
        "            TechniqueDetails object or None if not found.\n",
        "        \"\"\"\n",
        "        technique = self.loader.get_technique(technique_id)\n",
        "\n",
        "        if not technique:\n",
        "            return None\n",
        "\n",
        "        # Extract basic info\n",
        "        name = technique.get('name', '')\n",
        "        description = technique.get('description', '')\n",
        "        platforms = technique.get('x_mitre_platforms', [])\n",
        "        detection = technique.get('x_mitre_detection', '')\n",
        "\n",
        "        # Extract tactics from kill chain phases\n",
        "        tactics = []\n",
        "        kill_chain = technique.get('kill_chain_phases', [])\n",
        "        for phase in kill_chain:\n",
        "            if phase.get('kill_chain_name') == 'mitre-attack':\n",
        "                tactic_name = phase.get('phase_name', '')\n",
        "                formatted_tactic = tactic_name.replace('-', ' ').title()\n",
        "                tactics.append(formatted_tactic)\n",
        "\n",
        "        # Extract data sources and components\n",
        "        data_sources = []\n",
        "        data_components = []\n",
        "        raw_data_sources = technique.get('x_mitre_data_sources', [])\n",
        "\n",
        "        for ds in raw_data_sources:\n",
        "            if isinstance(ds, str):\n",
        "                if ':' in ds:\n",
        "                    source, component = ds.split(':', 1)\n",
        "                    data_sources.append(source.strip())\n",
        "                    data_components.append(component.strip())\n",
        "                else:\n",
        "                    data_sources.append(ds.strip())\n",
        "\n",
        "        # Remove duplicates while preserving order\n",
        "        data_sources = list(dict.fromkeys(data_sources))\n",
        "        data_components = list(dict.fromkeys(data_components))\n",
        "\n",
        "        return TechniqueDetails(\n",
        "            id=technique_id,\n",
        "            name=name,\n",
        "            description=description,\n",
        "            tactics=tactics,\n",
        "            data_sources=data_sources,\n",
        "            data_components=data_components,\n",
        "            platforms=platforms,\n",
        "            detection=detection\n",
        "        )\n",
        "\n",
        "    def extract_batch(self, technique_ids: List[str]) -> Dict[str, TechniqueDetails]:\n",
        "        \"\"\"Extract details for multiple techniques.\"\"\"\n",
        "        results = {}\n",
        "        for tech_id in technique_ids:\n",
        "            details = self.extract(tech_id)\n",
        "            if details:\n",
        "                results[tech_id] = details\n",
        "        return results\n",
        "\n",
        "\n",
        "print(\"✓ TechniqueExtractor class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Stage 6: Entity Mapper\n",
        "\n",
        "Map ATT&CK data sources to FTE-HARM forensic entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EntityMapper:\n",
        "    \"\"\"Map ATT&CK data sources to FTE-HARM forensic entities.\"\"\"\n",
        "\n",
        "    def __init__(self, custom_mapping: Optional[Dict[str, List[str]]] = None):\n",
        "        self.mapping = custom_mapping or ATTACK_TO_ENTITY_MAPPING\n",
        "\n",
        "    def map_data_sources(self, data_sources: List[str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Map ATT&CK data sources to FTE-HARM entity types.\n",
        "        \n",
        "        Args:\n",
        "            data_sources: List of ATT&CK data sources.\n",
        "        \n",
        "        Returns:\n",
        "            List of relevant FTE-HARM entity types.\n",
        "        \"\"\"\n",
        "        entities = set()\n",
        "\n",
        "        for source in data_sources:\n",
        "            source_lower = source.lower()\n",
        "\n",
        "            for attack_source, entity_types in self.mapping.items():\n",
        "                if attack_source.lower() in source_lower or source_lower in attack_source.lower():\n",
        "                    entities.update(entity_types)\n",
        "\n",
        "        # DateTime is always relevant for temporal context\n",
        "        entities.add('DateTime')\n",
        "\n",
        "        # Validate entities are in our 22-entity set\n",
        "        valid_entities = [e for e in entities if e in FTE_HARM_ENTITIES]\n",
        "\n",
        "        return valid_entities\n",
        "\n",
        "    def map_data_components(self, data_components: List[str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Map ATT&CK data components to FTE-HARM entity types.\n",
        "        \"\"\"\n",
        "        entities = set()\n",
        "\n",
        "        component_keywords = {\n",
        "            'creation': ['DateTime', 'Action'],\n",
        "            'modification': ['DateTime', 'Action'],\n",
        "            'deletion': ['DateTime', 'Action'],\n",
        "            'access': ['Action', 'Username'],\n",
        "            'authentication': ['Username', 'Action', 'AuthenticationType'],\n",
        "            'connection': ['IPAddress', 'Port', 'Protocol'],\n",
        "            'process': ['Process', 'ProcessID'],\n",
        "            'command': ['Process', 'Action'],\n",
        "            'content': ['Object'],\n",
        "            'metadata': ['Object'],\n",
        "            'enumeration': ['Action']\n",
        "        }\n",
        "\n",
        "        for component in data_components:\n",
        "            component_lower = component.lower()\n",
        "            for keyword, entity_types in component_keywords.items():\n",
        "                if keyword in component_lower:\n",
        "                    entities.update(entity_types)\n",
        "\n",
        "        return [e for e in entities if e in FTE_HARM_ENTITIES]\n",
        "\n",
        "\n",
        "print(\"✓ EntityMapper class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Stage 7: Weight Generator\n",
        "\n",
        "Generate entity weights for FTE-HARM hypothesis configurations based on ATT&CK technique characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WeightGenerator:\n",
        "    \"\"\"Generate entity weights for FTE-HARM hypothesis configurations.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 critical_indicators: Optional[Dict[str, List[str]]] = None,\n",
        "                 datetime_weight: float = 0.10):\n",
        "        self.critical_indicators = critical_indicators or CRITICAL_ENTITY_INDICATORS\n",
        "        self.datetime_weight = datetime_weight\n",
        "\n",
        "    def generate(self,\n",
        "                 entities: List[str],\n",
        "                 technique_details: TechniqueDetails) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Generate entity weights based on ATT&CK technique characteristics.\n",
        "        \n",
        "        Weighting strategy:\n",
        "        - Critical entities (directly mentioned in technique): 0.30-0.40\n",
        "        - Strong entities (required for detection): 0.20-0.25\n",
        "        - Supporting entities (contextual): 0.10-0.15\n",
        "        - Temporal context (DateTime): Fixed weight\n",
        "        \"\"\"\n",
        "        weights = {}\n",
        "        technique_text = (technique_details.name + ' ' +\n",
        "                         technique_details.description + ' ' +\n",
        "                         technique_details.detection).lower()\n",
        "\n",
        "        # Categorize entities\n",
        "        critical = []\n",
        "        strong = []\n",
        "        supporting = []\n",
        "\n",
        "        for entity in entities:\n",
        "            if entity == 'DateTime':\n",
        "                continue\n",
        "\n",
        "            is_critical = False\n",
        "            if entity in self.critical_indicators:\n",
        "                for keyword in self.critical_indicators[entity]:\n",
        "                    if keyword in technique_text:\n",
        "                        is_critical = True\n",
        "                        break\n",
        "\n",
        "            if is_critical:\n",
        "                critical.append(entity)\n",
        "            elif entity in ['Process', 'Username', 'Action', 'DNSName', 'IPAddress', 'Service']:\n",
        "                strong.append(entity)\n",
        "            else:\n",
        "                supporting.append(entity)\n",
        "\n",
        "        # Calculate available weight\n",
        "        available_weight = 1.0 - self.datetime_weight\n",
        "\n",
        "        # Distribute weights\n",
        "        if len(critical) > 0:\n",
        "            critical_weight = min(0.35, available_weight * 0.5 / len(critical))\n",
        "            for entity in critical:\n",
        "                weights[entity] = critical_weight\n",
        "            available_weight -= critical_weight * len(critical)\n",
        "\n",
        "        if len(strong) > 0:\n",
        "            strong_weight = min(0.25, available_weight * 0.7 / len(strong))\n",
        "            for entity in strong:\n",
        "                weights[entity] = strong_weight\n",
        "            available_weight -= strong_weight * len(strong)\n",
        "\n",
        "        if len(supporting) > 0 and available_weight > 0:\n",
        "            supporting_weight = available_weight / len(supporting)\n",
        "            for entity in supporting:\n",
        "                weights[entity] = supporting_weight\n",
        "\n",
        "        # Add DateTime weight\n",
        "        weights['DateTime'] = self.datetime_weight\n",
        "\n",
        "        # Normalize to sum to 1.0\n",
        "        total = sum(weights.values())\n",
        "        if total > 0:\n",
        "            weights = {k: round(v / total, 3) for k, v in weights.items()}\n",
        "\n",
        "        # Final adjustment\n",
        "        adjustment = 1.0 - sum(weights.values())\n",
        "        if adjustment != 0 and weights:\n",
        "            max_entity = max(weights.keys(), key=lambda k: weights[k])\n",
        "            weights[max_entity] = round(weights[max_entity] + adjustment, 3)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def identify_critical_entity(self,\n",
        "                                  weights: Dict[str, float],\n",
        "                                  technique_details: TechniqueDetails) -> str:\n",
        "        \"\"\"Identify the critical entity for a hypothesis.\"\"\"\n",
        "        non_temporal_weights = {k: v for k, v in weights.items() if k != 'DateTime'}\n",
        "\n",
        "        if not non_temporal_weights:\n",
        "            return 'DateTime'\n",
        "\n",
        "        return max(non_temporal_weights.keys(), key=lambda k: non_temporal_weights[k])\n",
        "\n",
        "\n",
        "print(\"✓ WeightGenerator class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Stage 8: Hypothesis Generator\n",
        "\n",
        "Generate complete FTE-HARM hypothesis configurations from ATT&CK techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HypothesisGenerator:\n",
        "    \"\"\"Generate FTE-HARM hypothesis configurations from ATT&CK techniques.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 attack_loader: MitreAttackLoader,\n",
        "                 default_penalty_factor: float = 0.20):\n",
        "        self.loader = attack_loader\n",
        "        self.extractor = TechniqueExtractor(attack_loader)\n",
        "        self.entity_mapper = EntityMapper()\n",
        "        self.weight_generator = WeightGenerator()\n",
        "        self.default_penalty_factor = default_penalty_factor\n",
        "\n",
        "    def generate(self,\n",
        "                 technique_id: str,\n",
        "                 hypothesis_name: Optional[str] = None) -> Optional[HypothesisConfig]:\n",
        "        \"\"\"\n",
        "        Generate complete FTE-HARM hypothesis configuration from ATT&CK technique.\n",
        "        \"\"\"\n",
        "        # Extract technique details\n",
        "        details = self.extractor.extract(technique_id)\n",
        "        if not details:\n",
        "            return None\n",
        "\n",
        "        # Map to entities\n",
        "        entities_from_sources = self.entity_mapper.map_data_sources(details.data_sources)\n",
        "        entities_from_components = self.entity_mapper.map_data_components(details.data_components)\n",
        "\n",
        "        # Combine and deduplicate\n",
        "        all_entities = list(set(entities_from_sources + entities_from_components))\n",
        "\n",
        "        # Ensure minimum entities\n",
        "        if len(all_entities) < 2:\n",
        "            all_entities = ['DateTime', 'Action', 'Process']\n",
        "\n",
        "        # Generate weights\n",
        "        weights = self.weight_generator.generate(all_entities, details)\n",
        "\n",
        "        # Identify critical entity\n",
        "        critical_entity = self.weight_generator.identify_critical_entity(weights, details)\n",
        "\n",
        "        # Determine threshold\n",
        "        threshold = 0.50\n",
        "        for tactic in details.tactics:\n",
        "            if tactic in TACTIC_THRESHOLDS:\n",
        "                threshold = TACTIC_THRESHOLDS[tactic]\n",
        "                break\n",
        "\n",
        "        # Generate name\n",
        "        if not hypothesis_name:\n",
        "            safe_name = details.name.lower().replace(' ', '_').replace('/', '_').replace(':', '')\n",
        "            hypothesis_name = f\"{technique_id}_{safe_name[:30]}\"\n",
        "\n",
        "        # Truncate description\n",
        "        description = details.description\n",
        "        if len(description) > 250:\n",
        "            description = description[:247] + \"...\"\n",
        "\n",
        "        return HypothesisConfig(\n",
        "            name=hypothesis_name,\n",
        "            mitre_technique=technique_id,\n",
        "            mitre_tactic=details.tactics[0] if details.tactics else \"Unknown\",\n",
        "            description=description,\n",
        "            weights=weights,\n",
        "            critical_entity=critical_entity,\n",
        "            penalty_factor=self.default_penalty_factor,\n",
        "            threshold=threshold,\n",
        "            data_sources=details.data_sources,\n",
        "            platforms=details.platforms\n",
        "        )\n",
        "\n",
        "    def generate_scenario(self,\n",
        "                          scenario_name: str,\n",
        "                          technique_ids: Optional[List[str]] = None) -> Dict[str, HypothesisConfig]:\n",
        "        \"\"\"Generate all hypotheses for a specific attack scenario.\"\"\"\n",
        "        if technique_ids is None:\n",
        "            technique_ids = ATTACK_SCENARIOS.get(scenario_name, [])\n",
        "\n",
        "        if not technique_ids:\n",
        "            warnings.warn(f\"No techniques defined for scenario: {scenario_name}\")\n",
        "            return {}\n",
        "\n",
        "        hypotheses = {}\n",
        "\n",
        "        for idx, technique_id in enumerate(technique_ids, start=1):\n",
        "            hypothesis_name = f\"H{idx}_{scenario_name}\"\n",
        "\n",
        "            try:\n",
        "                config = self.generate(technique_id, hypothesis_name)\n",
        "                if config:\n",
        "                    hypotheses[hypothesis_name] = config\n",
        "                    print(f\"✓ Generated: {hypothesis_name} ({technique_id})\")\n",
        "                else:\n",
        "                    print(f\"✗ Not found: {technique_id}\")\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Failed: {technique_id} - {e}\")\n",
        "\n",
        "        return hypotheses\n",
        "\n",
        "    def generate_all_scenarios(self) -> Dict[str, Dict[str, HypothesisConfig]]:\n",
        "        \"\"\"Generate hypotheses for all predefined attack scenarios.\"\"\"\n",
        "        all_hypotheses = {}\n",
        "\n",
        "        for scenario_name in ATTACK_SCENARIOS.keys():\n",
        "            print(f\"\\n--- Generating {scenario_name} hypotheses ---\")\n",
        "            hypotheses = self.generate_scenario(scenario_name)\n",
        "            all_hypotheses[scenario_name] = hypotheses\n",
        "\n",
        "        return all_hypotheses\n",
        "\n",
        "\n",
        "print(\"✓ HypothesisGenerator class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Stage 9: P_Score Calculator\n",
        "\n",
        "Calculate P_Score using ATT&CK-generated hypothesis configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PScoreCalculator:\n",
        "    \"\"\"Calculate P_Score using ATT&CK-generated hypothesis configurations.\"\"\"\n",
        "\n",
        "    # Confidence thresholds (permissive for triage)\n",
        "    HIGH_THRESHOLD = 0.65\n",
        "    MEDIUM_THRESHOLD = 0.50\n",
        "    LOW_THRESHOLD = 0.35\n",
        "\n",
        "    def calculate(self,\n",
        "                  entities: Dict[str, List[Any]],\n",
        "                  hypothesis: HypothesisConfig) -> PScoreResult:\n",
        "        \"\"\"\n",
        "        Calculate P_Score using ATT&CK-generated hypothesis configuration.\n",
        "        \n",
        "        P_Score = (Σ(W_i × E_i)) × (1 - P_F) if critical entity missing\n",
        "        P_Score = Σ(W_i × E_i) if critical entity present\n",
        "        \"\"\"\n",
        "        weights = hypothesis.weights\n",
        "        critical_entity = hypothesis.critical_entity\n",
        "        penalty_factor = hypothesis.penalty_factor\n",
        "\n",
        "        # Calculate weighted sum\n",
        "        total_score = 0.0\n",
        "        entities_present = []\n",
        "        entities_missing = []\n",
        "        weighted_contributions = {}\n",
        "\n",
        "        for entity_type, weight in weights.items():\n",
        "            if entity_type in entities and entities[entity_type]:\n",
        "                total_score += weight\n",
        "                entities_present.append(entity_type)\n",
        "                weighted_contributions[entity_type] = weight\n",
        "            else:\n",
        "                entities_missing.append(entity_type)\n",
        "                weighted_contributions[entity_type] = 0.0\n",
        "\n",
        "        # Apply penalty if critical entity missing\n",
        "        critical_present = critical_entity in entities_present\n",
        "\n",
        "        if not critical_present:\n",
        "            p_score = total_score * (1 - penalty_factor)\n",
        "        else:\n",
        "            p_score = total_score\n",
        "\n",
        "        p_score = round(p_score, 3)\n",
        "\n",
        "        # Determine confidence level\n",
        "        if p_score >= self.HIGH_THRESHOLD:\n",
        "            confidence = ConfidenceLevel.HIGH\n",
        "            triage_decision = TriageDecision.INVESTIGATE_IMMEDIATE\n",
        "        elif p_score >= self.MEDIUM_THRESHOLD:\n",
        "            confidence = ConfidenceLevel.MEDIUM\n",
        "            triage_decision = TriageDecision.INVESTIGATE_PRIORITY\n",
        "        elif p_score >= self.LOW_THRESHOLD:\n",
        "            confidence = ConfidenceLevel.LOW\n",
        "            triage_decision = TriageDecision.INVESTIGATE_STANDARD\n",
        "        else:\n",
        "            confidence = ConfidenceLevel.INSUFFICIENT\n",
        "            triage_decision = TriageDecision.MONITOR\n",
        "\n",
        "        return PScoreResult(\n",
        "            p_score=p_score,\n",
        "            confidence=confidence,\n",
        "            triage_decision=triage_decision,\n",
        "            entities_present=entities_present,\n",
        "            entities_missing=entities_missing,\n",
        "            critical_entity_present=critical_present,\n",
        "            mitre_technique=hypothesis.mitre_technique,\n",
        "            mitre_tactic=hypothesis.mitre_tactic,\n",
        "            weighted_contributions=weighted_contributions\n",
        "        )\n",
        "\n",
        "    def calculate_multi_hypothesis(self,\n",
        "                                   entities: Dict[str, List[Any]],\n",
        "                                   hypotheses: Dict[str, HypothesisConfig]) -> Dict[str, PScoreResult]:\n",
        "        \"\"\"Calculate P_Score against multiple hypotheses.\"\"\"\n",
        "        results = {}\n",
        "        for name, hypothesis in hypotheses.items():\n",
        "            results[name] = self.calculate(entities, hypothesis)\n",
        "        return results\n",
        "\n",
        "    def get_best_match(self,\n",
        "                       entities: Dict[str, List[Any]],\n",
        "                       hypotheses: Dict[str, HypothesisConfig]) -> Tuple[str, PScoreResult]:\n",
        "        \"\"\"Find the best matching hypothesis for given entities.\"\"\"\n",
        "        results = self.calculate_multi_hypothesis(entities, hypotheses)\n",
        "\n",
        "        if not results:\n",
        "            return None, None\n",
        "\n",
        "        best_name = max(results.keys(), key=lambda k: results[k].p_score)\n",
        "        return best_name, results[best_name]\n",
        "\n",
        "\n",
        "print(\"✓ PScoreCalculator class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Stage 10: Hypothesis Table Generator\n",
        "\n",
        "Generate formatted hypothesis tables for thesis documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HypothesisTableGenerator:\n",
        "    \"\"\"Generate formatted hypothesis tables for documentation.\"\"\"\n",
        "\n",
        "    def to_markdown(self, hypotheses: Dict[str, HypothesisConfig]) -> str:\n",
        "        \"\"\"Generate formatted markdown table for thesis documentation.\"\"\"\n",
        "        lines = []\n",
        "        lines.append(\"| Hypothesis | MITRE ID | Tactic | Critical Entity | Threshold | Top Entity Weights |\")\n",
        "        lines.append(\"|------------|----------|--------|-----------------|-----------|-------------------|\")\n",
        "\n",
        "        for hyp_name, config in hypotheses.items():\n",
        "            sorted_weights = sorted(\n",
        "                config.weights.items(),\n",
        "                key=lambda x: x[1],\n",
        "                reverse=True\n",
        "            )[:3]\n",
        "            weights_str = \", \".join([f\"{e}({w:.2f})\" for e, w in sorted_weights])\n",
        "\n",
        "            row = [\n",
        "                hyp_name,\n",
        "                config.mitre_technique,\n",
        "                config.mitre_tactic[:20],\n",
        "                config.critical_entity,\n",
        "                f\"{config.threshold:.2f}\",\n",
        "                weights_str\n",
        "            ]\n",
        "\n",
        "            lines.append(\"| \" + \" | \".join(row) + \" |\")\n",
        "\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    def to_csv(self, hypotheses: Dict[str, HypothesisConfig]) -> str:\n",
        "        \"\"\"Generate CSV format for hypothesis configurations.\"\"\"\n",
        "        lines = []\n",
        "        header = \"Name,MITRE_ID,Tactic,Critical_Entity,Threshold,Penalty_Factor,Weights\"\n",
        "        lines.append(header)\n",
        "\n",
        "        for hyp_name, config in hypotheses.items():\n",
        "            weights_json = json.dumps(config.weights)\n",
        "            row = [\n",
        "                hyp_name,\n",
        "                config.mitre_technique,\n",
        "                config.mitre_tactic,\n",
        "                config.critical_entity,\n",
        "                str(config.threshold),\n",
        "                str(config.penalty_factor),\n",
        "                f'\"{weights_json}\"'\n",
        "            ]\n",
        "            lines.append(\",\".join(row))\n",
        "\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    def to_json(self, hypotheses: Dict[str, HypothesisConfig]) -> str:\n",
        "        \"\"\"Generate JSON format for hypothesis configurations.\"\"\"\n",
        "        output = {}\n",
        "        for hyp_name, config in hypotheses.items():\n",
        "            output[hyp_name] = {\n",
        "                'name': config.name,\n",
        "                'mitre_technique': config.mitre_technique,\n",
        "                'mitre_tactic': config.mitre_tactic,\n",
        "                'description': config.description,\n",
        "                'weights': config.weights,\n",
        "                'critical_entity': config.critical_entity,\n",
        "                'penalty_factor': config.penalty_factor,\n",
        "                'threshold': config.threshold,\n",
        "                'data_sources': config.data_sources,\n",
        "                'platforms': config.platforms\n",
        "            }\n",
        "\n",
        "        return json.dumps(output, indent=2)\n",
        "\n",
        "    def to_detailed_report(self, hypotheses: Dict[str, HypothesisConfig]) -> str:\n",
        "        \"\"\"Generate detailed report for each hypothesis.\"\"\"\n",
        "        lines = []\n",
        "\n",
        "        for hyp_name, config in hypotheses.items():\n",
        "            lines.append(\"=\" * 80)\n",
        "            lines.append(f\"HYPOTHESIS: {hyp_name}\")\n",
        "            lines.append(\"=\" * 80)\n",
        "            lines.append(f\"MITRE Technique: {config.mitre_technique}\")\n",
        "            lines.append(f\"MITRE Tactic: {config.mitre_tactic}\")\n",
        "            lines.append(f\"Critical Entity: {config.critical_entity}\")\n",
        "            lines.append(f\"Threshold: {config.threshold}\")\n",
        "            lines.append(f\"Penalty Factor: {config.penalty_factor}\")\n",
        "            lines.append(\"\")\n",
        "            lines.append(\"Entity Weights:\")\n",
        "            for entity, weight in sorted(config.weights.items(), key=lambda x: x[1], reverse=True):\n",
        "                bar = \"█\" * int(weight * 20)\n",
        "                lines.append(f\"  {entity:20s}: {weight:.3f} {bar}\")\n",
        "            lines.append(\"\")\n",
        "            lines.append(\"Data Sources (ATT&CK):\")\n",
        "            for ds in config.data_sources:\n",
        "                lines.append(f\"  - {ds}\")\n",
        "            lines.append(\"\")\n",
        "\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "print(\"✓ HypothesisTableGenerator class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Stage 11: Validation Pipeline\n",
        "\n",
        "End-to-end validation of ATT&CK-based hypothesis generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ValidationPipeline:\n",
        "    \"\"\"End-to-end validation of ATT&CK-based hypothesis generation.\"\"\"\n",
        "\n",
        "    def __init__(self, attack_loader: MitreAttackLoader):\n",
        "        self.loader = attack_loader\n",
        "        self.generator = HypothesisGenerator(attack_loader)\n",
        "        self.calculator = PScoreCalculator()\n",
        "\n",
        "    def validate_technique_extraction(self, technique_id: str) -> bool:\n",
        "        \"\"\"Validate that technique details can be extracted.\"\"\"\n",
        "        extractor = TechniqueExtractor(self.loader)\n",
        "        details = extractor.extract(technique_id)\n",
        "\n",
        "        if not details:\n",
        "            print(f\"✗ Failed to extract {technique_id}\")\n",
        "            return False\n",
        "\n",
        "        print(f\"✓ Extracted {technique_id}: {details.name}\")\n",
        "        print(f\"  Tactics: {', '.join(details.tactics)}\")\n",
        "        print(f\"  Data Sources: {len(details.data_sources)}\")\n",
        "        return True\n",
        "\n",
        "    def validate_hypothesis_generation(self, technique_id: str) -> bool:\n",
        "        \"\"\"Validate that hypothesis can be generated.\"\"\"\n",
        "        hypothesis = self.generator.generate(technique_id)\n",
        "\n",
        "        if not hypothesis:\n",
        "            print(f\"✗ Failed to generate hypothesis for {technique_id}\")\n",
        "            return False\n",
        "\n",
        "        if not hypothesis.validate():\n",
        "            print(f\"✗ Weights do not sum to 1.0 for {technique_id}\")\n",
        "            return False\n",
        "\n",
        "        print(f\"✓ Generated hypothesis: {hypothesis.name}\")\n",
        "        print(f\"  Critical Entity: {hypothesis.critical_entity}\")\n",
        "        print(f\"  Weight Sum: {sum(hypothesis.weights.values()):.3f}\")\n",
        "        return True\n",
        "\n",
        "    def validate_pscore_calculation(self,\n",
        "                                     technique_id: str,\n",
        "                                     test_entities: Dict[str, List[Any]]) -> bool:\n",
        "        \"\"\"Validate P_Score calculation.\"\"\"\n",
        "        hypothesis = self.generator.generate(technique_id)\n",
        "\n",
        "        if not hypothesis:\n",
        "            print(f\"✗ Failed to generate hypothesis for {technique_id}\")\n",
        "            return False\n",
        "\n",
        "        result = self.calculator.calculate(test_entities, hypothesis)\n",
        "\n",
        "        print(f\"✓ P_Score calculated: {result.p_score:.3f}\")\n",
        "        print(f\"  Confidence: {result.confidence.value}\")\n",
        "        print(f\"  Triage Decision: {result.triage_decision.value}\")\n",
        "        print(f\"  Entities Present: {', '.join(result.entities_present)}\")\n",
        "        return True\n",
        "\n",
        "    def run_full_validation(self, test_technique: str = \"T1548.003\") -> Dict[str, bool]:\n",
        "        \"\"\"Run full validation pipeline.\"\"\"\n",
        "        print(\"=\" * 80)\n",
        "        print(\"MITRE ATT&CK TO FTE-HARM PIPELINE VALIDATION\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # Test 1: Technique extraction\n",
        "        print(\"\\n[TEST 1] Technique Extraction\")\n",
        "        results['extraction'] = self.validate_technique_extraction(test_technique)\n",
        "\n",
        "        # Test 2: Hypothesis generation\n",
        "        print(\"\\n[TEST 2] Hypothesis Generation\")\n",
        "        results['generation'] = self.validate_hypothesis_generation(test_technique)\n",
        "\n",
        "        # Test 3: Entity mapping\n",
        "        print(\"\\n[TEST 3] Entity Mapping\")\n",
        "        mapper = EntityMapper()\n",
        "        extractor = TechniqueExtractor(self.loader)\n",
        "        details = extractor.extract(test_technique)\n",
        "        if details:\n",
        "            entities = mapper.map_data_sources(details.data_sources)\n",
        "            print(f\"✓ Mapped {len(details.data_sources)} data sources to {len(entities)} entities\")\n",
        "            print(f\"  Entities: {', '.join(entities)}\")\n",
        "            results['mapping'] = True\n",
        "        else:\n",
        "            results['mapping'] = False\n",
        "\n",
        "        # Test 4: P_Score calculation\n",
        "        print(\"\\n[TEST 4] P_Score Calculation\")\n",
        "        test_entities = {\n",
        "            'DateTime': ['Jan 24 10:30:45'],\n",
        "            'Process': ['su'],\n",
        "            'ProcessID': ['1234'],\n",
        "            'Username': ['admin', 'www-data']\n",
        "        }\n",
        "        results['pscore'] = self.validate_pscore_calculation(test_technique, test_entities)\n",
        "\n",
        "        # Test 5: Batch generation\n",
        "        print(\"\\n[TEST 5] Batch Hypothesis Generation\")\n",
        "        try:\n",
        "            hypotheses = self.generator.generate_scenario('privilege_escalation')\n",
        "            results['batch'] = len(hypotheses) > 0\n",
        "            print(f\"✓ Generated {len(hypotheses)} hypotheses for privilege_escalation\")\n",
        "        except Exception as e:\n",
        "            results['batch'] = False\n",
        "            print(f\"✗ Batch generation failed: {e}\")\n",
        "\n",
        "        # Summary\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"VALIDATION SUMMARY\")\n",
        "        print(\"=\" * 80)\n",
        "        passed = sum(results.values())\n",
        "        total = len(results)\n",
        "        print(f\"Passed: {passed}/{total}\")\n",
        "        for test, result in results.items():\n",
        "            status = \"✓ PASS\" if result else \"✗ FAIL\"\n",
        "            print(f\"  {test}: {status}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "print(\"✓ ValidationPipeline class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Stage 12: Execute Full Pipeline\n",
        "\n",
        "Load ATT&CK data and run the complete hypothesis generation pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MITRE ATT&CK data\n",
        "print(\"Loading MITRE ATT&CK data...\")\n",
        "print(\"(This may take a minute on first run as it downloads from MITRE)\")\n",
        "print()\n",
        "\n",
        "loader = MitreAttackLoader()\n",
        "success = loader.load()\n",
        "\n",
        "if success:\n",
        "    print(\"\\n✓ ATT&CK data loaded successfully!\")\n",
        "else:\n",
        "    print(\"\\n✗ Failed to load ATT&CK data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run validation pipeline\n",
        "if success:\n",
        "    pipeline = ValidationPipeline(loader)\n",
        "    validation_results = pipeline.run_full_validation(\"T1548.003\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Stage 13: Generate Hypothesis Tables\n",
        "\n",
        "Generate and display hypothesis configurations for attack scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate hypotheses for privilege escalation scenario\n",
        "if success:\n",
        "    generator = HypothesisGenerator(loader)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"GENERATING PRIVILEGE ESCALATION HYPOTHESES\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    priv_esc_hypotheses = generator.generate_scenario('privilege_escalation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display markdown table\n",
        "if success and priv_esc_hypotheses:\n",
        "    table_gen = HypothesisTableGenerator()\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"HYPOTHESIS TABLE (Markdown Format)\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "    print(table_gen.to_markdown(priv_esc_hypotheses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display detailed report\n",
        "if success and priv_esc_hypotheses:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"DETAILED HYPOTHESIS REPORT\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "    print(table_gen.to_detailed_report(priv_esc_hypotheses))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Stage 14: Interactive P_Score Calculation\n",
        "\n",
        "Calculate P_Score for sample log entities against generated hypotheses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Score a privilege escalation log\n",
        "if success and priv_esc_hypotheses:\n",
        "    calculator = PScoreCalculator()\n",
        "    \n",
        "    # Simulated extracted entities from a log line:\n",
        "    # \"Jan 24 10:30:45 server su[1234]: Successful su for admin by www-data\"\n",
        "    sample_entities = {\n",
        "        'DateTime': ['Jan 24 10:30:45'],\n",
        "        'Process': ['su'],\n",
        "        'ProcessID': ['1234'],\n",
        "        'Username': ['admin', 'www-data'],\n",
        "        'Action': ['Successful']\n",
        "    }\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"P_SCORE CALCULATION EXAMPLE\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "    print(\"Sample Log: Jan 24 10:30:45 server su[1234]: Successful su for admin by www-data\")\n",
        "    print()\n",
        "    print(\"Extracted Entities:\")\n",
        "    for entity_type, values in sample_entities.items():\n",
        "        print(f\"  [{entity_type}] {', '.join(values)}\")\n",
        "    print()\n",
        "    \n",
        "    # Find best matching hypothesis\n",
        "    best_name, best_result = calculator.get_best_match(sample_entities, priv_esc_hypotheses)\n",
        "    \n",
        "    if best_result:\n",
        "        print(f\"Best Match: {best_name}\")\n",
        "        print(f\"MITRE Technique: {best_result.mitre_technique}\")\n",
        "        print(f\"MITRE Tactic: {best_result.mitre_tactic}\")\n",
        "        print(f\"P_Score: {best_result.p_score:.3f}\")\n",
        "        print(f\"Confidence: {best_result.confidence.value}\")\n",
        "        print(f\"Triage Decision: {best_result.triage_decision.value}\")\n",
        "        print(f\"Critical Entity Present: {'✓' if best_result.critical_entity_present else '✗'}\")\n",
        "        print()\n",
        "        print(\"Weighted Contributions:\")\n",
        "        for entity, contribution in sorted(best_result.weighted_contributions.items(), \n",
        "                                           key=lambda x: x[1], reverse=True):\n",
        "            status = \"✓\" if contribution > 0 else \"✗\"\n",
        "            print(f\"  {entity:20s}: {contribution:.3f} {status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Stage 15: Generate All Scenarios\n",
        "\n",
        "Generate hypotheses for all predefined attack scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate hypotheses for all scenarios\n",
        "if success:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"GENERATING ALL ATTACK SCENARIO HYPOTHESES\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    all_hypotheses = generator.generate_all_scenarios()\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    total = 0\n",
        "    for scenario, hypotheses in all_hypotheses.items():\n",
        "        count = len(hypotheses)\n",
        "        total += count\n",
        "        print(f\"  {scenario}: {count} hypotheses\")\n",
        "    print(f\"\\nTotal: {total} hypotheses generated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Stage 16: Export Configurations\n",
        "\n",
        "Export hypothesis configurations to JSON for use in FTE-HARM system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export all hypotheses to JSON\n",
        "if success and all_hypotheses:\n",
        "    all_configs = {}\n",
        "    for scenario, hypotheses in all_hypotheses.items():\n",
        "        for hyp_name, config in hypotheses.items():\n",
        "            all_configs[hyp_name] = config\n",
        "    \n",
        "    json_output = table_gen.to_json(all_configs)\n",
        "    \n",
        "    # Save to file\n",
        "    with open('fte_harm_hypotheses.json', 'w') as f:\n",
        "        f.write(json_output)\n",
        "    \n",
        "    print(\"✓ Exported all hypotheses to fte_harm_hypotheses.json\")\n",
        "    print(f\"  Total configurations: {len(all_configs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Convenience Functions\n",
        "\n",
        "Quick utility functions for common operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def quick_generate_hypothesis(technique_id: str,\n",
        "                               data_path: Optional[str] = None) -> Optional[HypothesisConfig]:\n",
        "    \"\"\"\n",
        "    Quick utility to generate a single hypothesis.\n",
        "    \n",
        "    Args:\n",
        "        technique_id: ATT&CK technique ID.\n",
        "        data_path: Optional path to enterprise-attack.json.\n",
        "    \n",
        "    Returns:\n",
        "        HypothesisConfig or None.\n",
        "    \"\"\"\n",
        "    loader = MitreAttackLoader(data_path)\n",
        "    if not loader.load():\n",
        "        return None\n",
        "\n",
        "    generator = HypothesisGenerator(loader)\n",
        "    return generator.generate(technique_id)\n",
        "\n",
        "\n",
        "def quick_score_entities(entities: Dict[str, List[Any]],\n",
        "                         technique_id: str,\n",
        "                         data_path: Optional[str] = None) -> Optional[PScoreResult]:\n",
        "    \"\"\"\n",
        "    Quick utility to score entities against a technique.\n",
        "    \n",
        "    Args:\n",
        "        entities: Dict of extracted entities.\n",
        "        technique_id: ATT&CK technique ID.\n",
        "        data_path: Optional path to enterprise-attack.json.\n",
        "    \n",
        "    Returns:\n",
        "        PScoreResult or None.\n",
        "    \"\"\"\n",
        "    loader = MitreAttackLoader(data_path)\n",
        "    if not loader.load():\n",
        "        return None\n",
        "\n",
        "    generator = HypothesisGenerator(loader)\n",
        "    hypothesis = generator.generate(technique_id)\n",
        "\n",
        "    if not hypothesis:\n",
        "        return None\n",
        "\n",
        "    calculator = PScoreCalculator()\n",
        "    return calculator.calculate(entities, hypothesis)\n",
        "\n",
        "\n",
        "print(\"✓ Convenience functions defined\")\n",
        "print()\n",
        "print(\"Usage examples:\")\n",
        "print(\"  hypothesis = quick_generate_hypothesis('T1548.003')\")\n",
        "print(\"  result = quick_score_entities(entities, 'T1548.003')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "This notebook demonstrates the complete MITRE ATT&CK to FTE-HARM hypothesis generation pipeline:\n",
        "\n",
        "1. **Data Loading**: Load ATT&CK STIX data from MITRE repository\n",
        "2. **Technique Extraction**: Extract tactics, data sources, and platforms\n",
        "3. **Entity Mapping**: Map ATT&CK data sources to 22 FTE-HARM entities\n",
        "4. **Weight Generation**: Auto-generate entity weights based on technique characteristics\n",
        "5. **Hypothesis Configuration**: Create complete hypothesis configs with thresholds\n",
        "6. **P_Score Calculation**: Score entities against hypotheses\n",
        "7. **Table Generation**: Export to Markdown/CSV/JSON for documentation\n",
        "\n",
        "### Key Classes:\n",
        "- `MitreAttackLoader`: Load ATT&CK data\n",
        "- `TechniqueExtractor`: Extract technique details\n",
        "- `EntityMapper`: Map data sources to entities\n",
        "- `WeightGenerator`: Generate entity weights\n",
        "- `HypothesisGenerator`: Create hypothesis configs\n",
        "- `PScoreCalculator`: Calculate P_Scores\n",
        "- `HypothesisTableGenerator`: Export configurations\n",
        "- `ValidationPipeline`: End-to-end testing\n",
        "\n",
        "### Predefined Scenarios:\n",
        "- privilege_escalation\n",
        "- lateral_movement\n",
        "- exfiltration\n",
        "- dns_abuse\n",
        "- credential_access\n",
        "- command_and_control\n",
        "- persistence\n",
        "- defense_evasion\n",
        "- discovery"
      ]
    }
  ]
}
