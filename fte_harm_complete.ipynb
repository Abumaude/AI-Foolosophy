{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FTE-HARM Complete Validation Pipeline v3\n",
        "\n",
        "## With Dataset-Specific Hypotheses, Threshold Testing, and MITRE Corroboration\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook implements the complete FTE-HARM validation with:\n",
        "\n",
        "1. **Dataset-Specific Hypotheses** - Target AITv2 labels directly\n",
        "2. **MITRE ATT&CK Metadata** - For forensic corroboration\n",
        "3. **Threshold Testing** - Find optimal threshold prioritizing HIGH RECALL\n",
        "4. **Label Mapping** - Automatic matching between hypotheses and ground truth\n",
        "5. **Two P_Score Methods** - Option A (Binary) and Option B3 (Confidence-Weighted)\n",
        "\n",
        "**Validation Goal:** HIGH RECALL first (catch all evidence), acceptable precision second."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Imports and Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FTE-HARM COMPLETE VALIDATION v3: IMPORTS AND PATHS\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from collections import defaultdict, Counter\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# PATH CONFIGURATION\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "DATASET_BASE_PATH = '/content/drive/My Drive/thesis/dataset'\n",
        "OUTPUT_PATH = '/content/drive/My Drive/thesis/hypotheses_validation'\n",
        "SUMMARY_PATH = '/content/drive/My Drive/thesis/hypotheses_validation/summary'\n",
        "THRESHOLD_TEST_PATH = '/content/drive/My Drive/thesis/hypotheses_validation/threshold_test'\n",
        "MITRE_PATH = '/content/drive/My Drive/thesis/hypotheses_validation/mitre_att&ck'\n",
        "\n",
        "# Create directories\n",
        "for path in [OUTPUT_PATH, SUMMARY_PATH, THRESHOLD_TEST_PATH, MITRE_PATH]:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# Model paths\n",
        "MODELS = {\n",
        "    'distilbert': '/content/drive/My Drive/thesis/transformer/distilberta_base_uncased/results/checkpoint-5245',\n",
        "    'distilroberta': '/content/drive/My Drive/thesis/transformer/distilroberta_base/results/checkpoint-5275',\n",
        "    'roberta_large': '/content/drive/My Drive/thesis/transformer/roberta_large/results/checkpoint-2772',\n",
        "    'xlm_roberta_base': '/content/drive/My Drive/thesis/transformer/xlm_roberta_base/results/checkpoint-12216',\n",
        "    'xlm_roberta_large': '/content/drive/My Drive/thesis/transformer/xlm_roberta_large/results/checkpoint-12240',\n",
        "}\n",
        "\n",
        "SELECTED_MODEL = 'xlm_roberta_large'\n",
        "\n",
        "print(\"Configuration loaded\")\n",
        "print(f\"Threshold test output: {THRESHOLD_TEST_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Dataset-Specific Hypotheses (With MITRE Metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATASET-SPECIFIC HYPOTHESES FOR AITv2\n",
        "# With MITRE ATT&CK Metadata for Forensic Corroboration\n",
        "# =============================================================================\n",
        "\n",
        "DATASET_HYPOTHESES = {\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # PRIVILEGE ESCALATION\n",
        "    # -------------------------------------------------------------------------\n",
        "    \n",
        "    'H1_attacker_change_user': {\n",
        "        'name': 'H1_attacker_change_user',\n",
        "        'description': 'Attacker switches user identity via su/sudo',\n",
        "        'target_labels': ['attacker_change_user', 'escalate'],\n",
        "        'mitre_technique': 'T1548.003',\n",
        "        'mitre_tactic': 'Privilege Escalation',\n",
        "        'weights': {\n",
        "            'Username': 0.30, 'Process': 0.25, 'Action': 0.20,\n",
        "            'DateTime': 0.15, 'Status': 0.10\n",
        "        },\n",
        "        'critical_entity': 'Username',\n",
        "        'penalty_factor': 0.25,\n",
        "        'threshold': 0.50,\n",
        "        'log_sources': ['auth.log', 'audit.log', 'secure']\n",
        "    },\n",
        "    \n",
        "    'H2_escalate': {\n",
        "        'name': 'H2_escalate',\n",
        "        'description': 'Generic privilege escalation',\n",
        "        'target_labels': ['escalate', 'attacker_change_user'],\n",
        "        'mitre_technique': 'T1068',\n",
        "        'mitre_tactic': 'Privilege Escalation',\n",
        "        'weights': {\n",
        "            'Process': 0.30, 'Username': 0.25, 'Action': 0.20,\n",
        "            'Status': 0.15, 'DateTime': 0.10\n",
        "        },\n",
        "        'critical_entity': 'Process',\n",
        "        'penalty_factor': 0.25,\n",
        "        'threshold': 0.50,\n",
        "        'log_sources': ['auth.log', 'audit.log', 'syslog']\n",
        "    },\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # DISCOVERY / SCANNING\n",
        "    # -------------------------------------------------------------------------\n",
        "    \n",
        "    'H3_scan': {\n",
        "        'name': 'H3_scan',\n",
        "        'description': 'Network or host scanning activity',\n",
        "        'target_labels': ['scan'],\n",
        "        'mitre_technique': 'T1046',\n",
        "        'mitre_tactic': 'Discovery',\n",
        "        'weights': {\n",
        "            'IPAddress': 0.30, 'Port': 0.25, 'Process': 0.20,\n",
        "            'Action': 0.15, 'DateTime': 0.10\n",
        "        },\n",
        "        'critical_entity': 'IPAddress',\n",
        "        'penalty_factor': 0.20,\n",
        "        'threshold': 0.45,\n",
        "        'log_sources': ['suricata', 'firewall', 'apache_access']\n",
        "    },\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # CREDENTIAL ACCESS\n",
        "    # -------------------------------------------------------------------------\n",
        "    \n",
        "    'H4_crack': {\n",
        "        'name': 'H4_crack',\n",
        "        'description': 'Password cracking or brute-force attempts',\n",
        "        'target_labels': ['crack'],\n",
        "        'mitre_technique': 'T1110.001',\n",
        "        'mitre_tactic': 'Credential Access',\n",
        "        'weights': {\n",
        "            'Username': 0.30, 'Action': 0.25, 'Status': 0.20,\n",
        "            'IPAddress': 0.15, 'DateTime': 0.10\n",
        "        },\n",
        "        'critical_entity': 'Username',\n",
        "        'penalty_factor': 0.25,\n",
        "        'threshold': 0.50,\n",
        "        'log_sources': ['auth.log', 'secure', 'audit.log']\n",
        "    },\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # PERSISTENCE\n",
        "    # -------------------------------------------------------------------------\n",
        "    \n",
        "    'H5_webshell': {\n",
        "        'name': 'H5_webshell',\n",
        "        'description': 'Webshell upload or execution',\n",
        "        'target_labels': ['webshell'],\n",
        "        'mitre_technique': 'T1505.003',\n",
        "        'mitre_tactic': 'Persistence',\n",
        "        'weights': {\n",
        "            'FilePath': 0.30, 'Process': 0.25, 'Action': 0.20,\n",
        "            'IPAddress': 0.15, 'DateTime': 0.10\n",
        "        },\n",
        "        'critical_entity': 'FilePath',\n",
        "        'penalty_factor': 0.25,\n",
        "        'threshold': 0.50,\n",
        "        'log_sources': ['apache_access', 'apache_error', 'nginx']\n",
        "    },\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # EXFILTRATION\n",
        "    # -------------------------------------------------------------------------\n",
        "    \n",
        "    'H6_exfiltrate': {\n",
        "        'name': 'H6_exfiltrate',\n",
        "        'description': 'Data exfiltration over network',\n",
        "        'target_labels': ['exfiltrate', 'dns_exfil'],\n",
        "        'mitre_technique': 'T1048',\n",
        "        'mitre_tactic': 'Exfiltration',\n",
        "        'weights': {\n",
        "            'IPAddress': 0.30, 'DNSName': 0.25, 'Process': 0.20,\n",
        "            'Action': 0.15, 'DateTime': 0.10\n",
        "        },\n",
        "        'critical_entity': 'IPAddress',\n",
        "        'penalty_factor': 0.20,\n",
        "        'threshold': 0.55,\n",
        "        'log_sources': ['dns', 'firewall', 'suricata', 'netflow']\n",
        "    },\n",
        "    \n",
        "    'H7_dns_exfil': {\n",
        "        'name': 'H7_dns_exfil',\n",
        "        'description': 'DNS tunneling for exfiltration',\n",
        "        'target_labels': ['dns_exfil', 'exfiltrate'],\n",
        "        'mitre_technique': 'T1071.004',\n",
        "        'mitre_tactic': 'Exfiltration',\n",
        "        'weights': {\n",
        "            'DNSName': 0.35, 'IPAddress': 0.25, 'Action': 0.20,\n",
        "            'Process': 0.10, 'DateTime': 0.10\n",
        "        },\n",
        "        'critical_entity': 'DNSName',\n",
        "        'penalty_factor': 0.20,\n",
        "        'threshold': 0.55,\n",
        "        'log_sources': ['dns', 'named', 'bind']\n",
        "    },\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # LATERAL MOVEMENT\n",
        "    # -------------------------------------------------------------------------\n",
        "    \n",
        "    'H8_lateral': {\n",
        "        'name': 'H8_lateral',\n",
        "        'description': 'Lateral movement between systems',\n",
        "        'target_labels': ['lateral'],\n",
        "        'mitre_technique': 'T1021.004',\n",
        "        'mitre_tactic': 'Lateral Movement',\n",
        "        'weights': {\n",
        "            'IPAddress': 0.30, 'Username': 0.25, 'Process': 0.20,\n",
        "            'Action': 0.15, 'DateTime': 0.10\n",
        "        },\n",
        "        'critical_entity': 'IPAddress',\n",
        "        'penalty_factor': 0.25,\n",
        "        'threshold': 0.50,\n",
        "        'log_sources': ['auth.log', 'secure', 'sshd']\n",
        "    },\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # COMMAND & CONTROL\n",
        "    # -------------------------------------------------------------------------\n",
        "    \n",
        "    'H9_rce': {\n",
        "        'name': 'H9_rce',\n",
        "        'description': 'Remote command execution',\n",
        "        'target_labels': ['rce', 'c2'],\n",
        "        'mitre_technique': 'T1059',\n",
        "        'mitre_tactic': 'Execution',\n",
        "        'weights': {\n",
        "            'Process': 0.30, 'Action': 0.25, 'Username': 0.20,\n",
        "            'IPAddress': 0.15, 'DateTime': 0.10\n",
        "        },\n",
        "        'critical_entity': 'Process',\n",
        "        'penalty_factor': 0.25,\n",
        "        'threshold': 0.50,\n",
        "        'log_sources': ['audit.log', 'syslog', 'apache_access']\n",
        "    },\n",
        "    \n",
        "    'H10_c2': {\n",
        "        'name': 'H10_c2',\n",
        "        'description': 'Command and control communication',\n",
        "        'target_labels': ['c2', 'rce'],\n",
        "        'mitre_technique': 'T1071.001',\n",
        "        'mitre_tactic': 'Command And Control',\n",
        "        'weights': {\n",
        "            'IPAddress': 0.30, 'DNSName': 0.25, 'Process': 0.20,\n",
        "            'Action': 0.15, 'DateTime': 0.10\n",
        "        },\n",
        "        'critical_entity': 'IPAddress',\n",
        "        'penalty_factor': 0.20,\n",
        "        'threshold': 0.55,\n",
        "        'log_sources': ['firewall', 'suricata', 'dns', 'proxy']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Print summary\n",
        "print(f\"\\nLoaded {len(DATASET_HYPOTHESES)} dataset-specific hypotheses\")\n",
        "print(\"\\nHypotheses Summary:\")\n",
        "print(\"-\"*80)\n",
        "for name, hyp in DATASET_HYPOTHESES.items():\n",
        "    targets = ', '.join(hyp['target_labels'])\n",
        "    print(f\"  {name}: targets=[{targets}] | MITRE={hyp['mitre_technique']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Thresholds and Triage Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# THRESHOLDS AND TRIAGE PRIORITIES\n",
        "# =============================================================================\n",
        "\n",
        "# Default thresholds (will be optimized via threshold testing)\n",
        "THRESHOLDS = {\n",
        "    'HIGH': 0.65,\n",
        "    'MEDIUM': 0.50,\n",
        "    'LOW': 0.35\n",
        "}\n",
        "\n",
        "# Malicious classification threshold (to be optimized)\n",
        "# Start with LOW to maximize recall, then tune\n",
        "MALICIOUS_THRESHOLD = 0.35  # Will be updated after threshold testing\n",
        "\n",
        "TRIAGE_PRIORITIES = {\n",
        "    'HIGH': 'Priority 1: Investigate immediately',\n",
        "    'MEDIUM': 'Priority 2: Queue for investigation',\n",
        "    'LOW': 'Priority 3: Review when possible',\n",
        "    'INSUFFICIENT': 'Priority 4: Archive for later'\n",
        "}\n",
        "\n",
        "# Thresholds to test\n",
        "THRESHOLDS_TO_TEST = [0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70]\n",
        "\n",
        "print(f\"Initial malicious threshold: {MALICIOUS_THRESHOLD}\")\n",
        "print(f\"Will test {len(THRESHOLDS_TO_TEST)} threshold values\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATA LOADERS\n",
        "# =============================================================================\n",
        "\n",
        "def find_log_label_pair(folder_path):\n",
        "    \"\"\"Find log and label files with variable naming.\"\"\"\n",
        "    if not os.path.isdir(folder_path):\n",
        "        return None, None\n",
        "    \n",
        "    files = os.listdir(folder_path)\n",
        "    log_files = [f for f in files if f.endswith('.log')]\n",
        "    \n",
        "    if 'log.log' in log_files and 'label.log' in log_files:\n",
        "        return os.path.join(folder_path, 'log.log'), os.path.join(folder_path, 'label.log')\n",
        "    \n",
        "    for f in log_files:\n",
        "        if f.startswith('log_'):\n",
        "            label = f'label_{f[4:]}'\n",
        "            if label in log_files:\n",
        "                return os.path.join(folder_path, f), os.path.join(folder_path, label)\n",
        "    \n",
        "    return None, None\n",
        "\n",
        "\n",
        "def scan_all_datasets(base_path):\n",
        "    \"\"\"Scan for all valid dataset pairs.\"\"\"\n",
        "    datasets = []\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        log_path, label_path = find_log_label_pair(root)\n",
        "        if log_path and label_path:\n",
        "            datasets.append({\n",
        "                'name': os.path.relpath(root, base_path),\n",
        "                'folder': root,\n",
        "                'log_path': log_path,\n",
        "                'label_path': label_path\n",
        "            })\n",
        "    return datasets\n",
        "\n",
        "\n",
        "def load_ground_truth(label_path):\n",
        "    \"\"\"Load ground truth labels.\"\"\"\n",
        "    ground_truth = {}\n",
        "    if not os.path.exists(label_path):\n",
        "        return ground_truth\n",
        "    \n",
        "    with open(label_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                entry = json.loads(line)\n",
        "                line_num = entry.get('line')\n",
        "                if line_num is not None:\n",
        "                    ground_truth[line_num] = {\n",
        "                        'labels': entry.get('labels', []),\n",
        "                        'rules': entry.get('rules', {})\n",
        "                    }\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "    return ground_truth\n",
        "\n",
        "\n",
        "def load_raw_logs(log_path):\n",
        "    \"\"\"Load raw logs with 1-indexed line tracking.\"\"\"\n",
        "    logs = []\n",
        "    with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line_number, log_text in enumerate(f, 1):\n",
        "            log_text = log_text.strip()\n",
        "            if log_text:\n",
        "                logs.append((line_number, log_text))\n",
        "    return logs\n",
        "\n",
        "\n",
        "def load_dataset(dataset_info):\n",
        "    \"\"\"Load complete dataset.\"\"\"\n",
        "    logs = load_raw_logs(dataset_info['log_path'])\n",
        "    ground_truth = load_ground_truth(dataset_info['label_path'])\n",
        "    \n",
        "    # Discover all labels\n",
        "    all_labels = []\n",
        "    for data in ground_truth.values():\n",
        "        all_labels.extend(data['labels'])\n",
        "    \n",
        "    stats = {\n",
        "        'name': dataset_info['name'],\n",
        "        'total_lines': len(logs),\n",
        "        'malicious_lines': len(ground_truth),\n",
        "        'benign_lines': len(logs) - len(ground_truth),\n",
        "        'unique_labels': list(set(all_labels)),\n",
        "        'label_counts': dict(Counter(all_labels))\n",
        "    }\n",
        "    \n",
        "    return logs, ground_truth, stats\n",
        "\n",
        "\n",
        "# Scan datasets\n",
        "print(\"\\nScanning for datasets...\")\n",
        "all_datasets = scan_all_datasets(DATASET_BASE_PATH)\n",
        "print(f\"Found {len(all_datasets)} datasets\")\n",
        "for ds in all_datasets[:5]:\n",
        "    print(f\"  - {ds['name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Model and Entity Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MODEL AND ENTITY EXTRACTION\n",
        "# =============================================================================\n",
        "\n",
        "def get_model_pipeline(model_path):\n",
        "    \"\"\"Load NER model.\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "    return pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "\n",
        "\n",
        "def extract_entities(log_text, nlp_pipeline):\n",
        "    \"\"\"Extract entities with Physical Token Quantization.\"\"\"\n",
        "    raw_entities = nlp_pipeline(log_text)\n",
        "    physical_tokens = [(m.group(), m.start(), m.end()) \n",
        "                       for m in re.finditer(r'[^\\[\\]\\s]+', log_text)]\n",
        "    \n",
        "    if not physical_tokens:\n",
        "        return {}, []\n",
        "    \n",
        "    token_labels = []\n",
        "    for token_text, start, end in physical_tokens:\n",
        "        overlapping = [e for e in raw_entities if not (e['end'] <= start or e['start'] >= end)]\n",
        "        if overlapping:\n",
        "            priorities = {'IPAddress': 4, 'DNSName': 3, 'Port': 2, 'Object': 1}\n",
        "            best = max(overlapping, key=lambda e: (priorities.get(e['entity_group'], 0), e['score']))\n",
        "            token_labels.append({'text': token_text, 'label': best['entity_group'], \n",
        "                               'confidence': best['score']})\n",
        "        else:\n",
        "            token_labels.append({'text': token_text, 'label': 'O', 'confidence': 1.0})\n",
        "    \n",
        "    # Aggregate by entity type\n",
        "    entity_types = defaultdict(list)\n",
        "    for token in token_labels:\n",
        "        if token['label'] != 'O':\n",
        "            entity_types[token['label']].append({\n",
        "                'value': token['text'],\n",
        "                'confidence': token['confidence']\n",
        "            })\n",
        "    \n",
        "    return dict(entity_types), token_labels\n",
        "\n",
        "\n",
        "# Load model\n",
        "print(f\"\\nLoading model: {SELECTED_MODEL}...\")\n",
        "nlp = get_model_pipeline(MODELS[SELECTED_MODEL])\n",
        "print(\"Model loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: P_Score Calculation (Option A & B3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# P_SCORE CALCULATION\n",
        "# =============================================================================\n",
        "\n",
        "def calculate_pscore_option_a(entity_types, hypothesis):\n",
        "    \"\"\"P_Score using BINARY entity presence.\"\"\"\n",
        "    weights = hypothesis.get('weights', {})\n",
        "    critical_entity = hypothesis.get('critical_entity', 'Process')\n",
        "    penalty_factor = hypothesis.get('penalty_factor', 0.20)\n",
        "    \n",
        "    weighted_sum = 0.0\n",
        "    for entity_type, weight in weights.items():\n",
        "        if entity_type in entity_types and entity_types[entity_type]:\n",
        "            weighted_sum += weight\n",
        "    \n",
        "    critical_present = critical_entity in entity_types and len(entity_types.get(critical_entity, [])) > 0\n",
        "    p_score = weighted_sum if critical_present else weighted_sum * (1 - penalty_factor)\n",
        "    \n",
        "    return round(p_score, 4), critical_present\n",
        "\n",
        "\n",
        "def calculate_pscore_option_b3(entity_types, hypothesis):\n",
        "    \"\"\"P_Score using CONFIDENCE-WEIGHTED entity presence.\"\"\"\n",
        "    weights = hypothesis.get('weights', {})\n",
        "    critical_entity = hypothesis.get('critical_entity', 'Process')\n",
        "    penalty_factor = hypothesis.get('penalty_factor', 0.20)\n",
        "    \n",
        "    weighted_sum = 0.0\n",
        "    for entity_type, weight in weights.items():\n",
        "        if entity_type in entity_types and entity_types[entity_type]:\n",
        "            confidences = [e['confidence'] for e in entity_types[entity_type]]\n",
        "            avg_confidence = sum(confidences) / len(confidences)\n",
        "            weighted_sum += weight * avg_confidence\n",
        "    \n",
        "    critical_present = critical_entity in entity_types and len(entity_types.get(critical_entity, [])) > 0\n",
        "    p_score = weighted_sum if critical_present else weighted_sum * (1 - penalty_factor)\n",
        "    \n",
        "    return round(p_score, 4), critical_present\n",
        "\n",
        "\n",
        "def get_confidence_level(p_score):\n",
        "    \"\"\"Get confidence level from P_Score.\"\"\"\n",
        "    if p_score >= THRESHOLDS['HIGH']:\n",
        "        return 'HIGH'\n",
        "    elif p_score >= THRESHOLDS['MEDIUM']:\n",
        "        return 'MEDIUM'\n",
        "    elif p_score >= THRESHOLDS['LOW']:\n",
        "        return 'LOW'\n",
        "    return 'INSUFFICIENT'\n",
        "\n",
        "\n",
        "print(\"P_Score calculation functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Hypothesis Matching and Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HYPOTHESIS MATCHING AND MULTI-HYPOTHESIS SCORING\n",
        "# =============================================================================\n",
        "\n",
        "def check_hypothesis_label_match(hypothesis_name, ground_truth_labels):\n",
        "    \"\"\"Check if hypothesis targets match ground truth labels.\"\"\"\n",
        "    if hypothesis_name not in DATASET_HYPOTHESES:\n",
        "        return False\n",
        "    \n",
        "    target_labels = DATASET_HYPOTHESES[hypothesis_name].get('target_labels', [])\n",
        "    \n",
        "    for gt_label in ground_truth_labels:\n",
        "        gt_lower = gt_label.lower().strip()\n",
        "        for target in target_labels:\n",
        "            target_lower = target.lower().strip()\n",
        "            if gt_lower == target_lower or gt_lower in target_lower or target_lower in gt_lower:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def score_all_hypotheses(entity_types, method='option_a'):\n",
        "    \"\"\"Score against ALL hypotheses, return best match.\"\"\"\n",
        "    score_func = calculate_pscore_option_a if method == 'option_a' else calculate_pscore_option_b3\n",
        "    \n",
        "    scores = {}\n",
        "    for name, hyp in DATASET_HYPOTHESES.items():\n",
        "        p_score, critical_present = score_func(entity_types, hyp)\n",
        "        scores[name] = {\n",
        "            'p_score': p_score,\n",
        "            'critical_present': critical_present,\n",
        "            'confidence_level': get_confidence_level(p_score)\n",
        "        }\n",
        "    \n",
        "    # Rank by score\n",
        "    ranking = sorted(scores.items(), key=lambda x: x[1]['p_score'], reverse=True)\n",
        "    \n",
        "    if ranking:\n",
        "        top_name, top_data = ranking[0]\n",
        "        return {\n",
        "            'top_hypothesis': top_name,\n",
        "            'p_score': top_data['p_score'],\n",
        "            'confidence_level': top_data['confidence_level'],\n",
        "            'critical_present': top_data['critical_present'],\n",
        "            'all_scores': scores,\n",
        "            'ranking': [(n, s['p_score']) for n, s in ranking[:5]]\n",
        "        }\n",
        "    \n",
        "    return {\n",
        "        'top_hypothesis': None,\n",
        "        'p_score': 0.0,\n",
        "        'confidence_level': 'INSUFFICIENT',\n",
        "        'critical_present': False,\n",
        "        'all_scores': {},\n",
        "        'ranking': []\n",
        "    }\n",
        "\n",
        "\n",
        "def process_log_complete(line_number, log_text, nlp_pipeline, method='option_a', \n",
        "                         malicious_threshold=MALICIOUS_THRESHOLD):\n",
        "    \"\"\"Complete processing for a single log line.\"\"\"\n",
        "    entity_types, raw_tokens = extract_entities(log_text, nlp_pipeline)\n",
        "    scoring = score_all_hypotheses(entity_types, method)\n",
        "    \n",
        "    return {\n",
        "        'line_number': line_number,\n",
        "        'log_text': log_text[:100] + '...' if len(log_text) > 100 else log_text,\n",
        "        'entity_types': entity_types,\n",
        "        'top_hypothesis': scoring['top_hypothesis'],\n",
        "        'p_score': scoring['p_score'],\n",
        "        'confidence_level': scoring['confidence_level'],\n",
        "        'is_malicious': scoring['p_score'] >= malicious_threshold,\n",
        "        'triage_priority': TRIAGE_PRIORITIES.get(scoring['confidence_level'], 'Unknown'),\n",
        "        'method': method\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Hypothesis matching functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: Threshold Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# THRESHOLD TESTING\n",
        "# =============================================================================\n",
        "\n",
        "def evaluate_threshold(predictions, ground_truth, threshold):\n",
        "    \"\"\"Evaluate performance at a specific threshold.\"\"\"\n",
        "    tp = fp = tn = fn = 0\n",
        "    \n",
        "    for pred in predictions:\n",
        "        line_num = pred['line_number']\n",
        "        p_score = pred['p_score']\n",
        "        predicted_malicious = p_score >= threshold\n",
        "        actual_malicious = line_num in ground_truth\n",
        "        \n",
        "        if predicted_malicious and actual_malicious:\n",
        "            tp += 1\n",
        "        elif predicted_malicious and not actual_malicious:\n",
        "            fp += 1\n",
        "        elif not predicted_malicious and not actual_malicious:\n",
        "            tn += 1\n",
        "        else:\n",
        "            fn += 1\n",
        "    \n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "    \n",
        "    return {\n",
        "        'threshold': threshold,\n",
        "        'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn,\n",
        "        'precision': round(precision, 4),\n",
        "        'recall': round(recall, 4),\n",
        "        'f1_score': round(f1, 4),\n",
        "        'fnr': round(fn / (fn + tp), 4) if (fn + tp) > 0 else 0.0\n",
        "    }\n",
        "\n",
        "\n",
        "def find_optimal_threshold(predictions, ground_truth, min_recall=0.90):\n",
        "    \"\"\"Find optimal threshold prioritizing HIGH RECALL.\"\"\"\n",
        "    results = []\n",
        "    for threshold in THRESHOLDS_TO_TEST:\n",
        "        results.append(evaluate_threshold(predictions, ground_truth, threshold))\n",
        "    \n",
        "    # Find thresholds meeting recall requirement\n",
        "    meeting_recall = [r for r in results if r['recall'] >= min_recall]\n",
        "    \n",
        "    if meeting_recall:\n",
        "        optimal = max(meeting_recall, key=lambda x: x['precision'])\n",
        "        method = f\"Highest precision with recall >= {min_recall}\"\n",
        "    else:\n",
        "        optimal = max(results, key=lambda x: x['recall'])\n",
        "        method = \"Highest recall available\"\n",
        "    \n",
        "    best_f1 = max(results, key=lambda x: x['f1_score'])\n",
        "    \n",
        "    return {\n",
        "        'optimal_for_recall': optimal,\n",
        "        'selection_method': method,\n",
        "        'best_f1': best_f1,\n",
        "        'all_results': results\n",
        "    }\n",
        "\n",
        "\n",
        "def print_threshold_results(analysis):\n",
        "    \"\"\"Print threshold analysis.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"THRESHOLD ANALYSIS (Goal: HIGH RECALL)\")\n",
        "    print(\"=\"*90)\n",
        "    print(f\"{'Threshold':<12} {'Precision':<12} {'Recall':<12} {'F1':<12} {'FN (Missed)':<12}\")\n",
        "    print(\"-\"*90)\n",
        "    \n",
        "    for r in analysis['all_results']:\n",
        "        marker = \" <- OPTIMAL\" if r['threshold'] == analysis['optimal_for_recall']['threshold'] else \"\"\n",
        "        marker = \" <- BEST F1\" if r['threshold'] == analysis['best_f1']['threshold'] and not marker else marker\n",
        "        print(f\"{r['threshold']:<12} {r['precision']:<12} {r['recall']:<12} {r['f1_score']:<12} {r['fn']:<12}{marker}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    opt = analysis['optimal_for_recall']\n",
        "    print(f\"RECOMMENDED: Threshold={opt['threshold']} | Recall={opt['recall']} | Precision={opt['precision']}\")\n",
        "    print(f\"Selection: {analysis['selection_method']}\")\n",
        "    print(f\"False Negatives (MISSED EVIDENCE): {opt['fn']}\")\n",
        "\n",
        "\n",
        "print(\"Threshold testing functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9: Validation with Hypothesis Matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VALIDATION WITH HYPOTHESIS MATCHING\n",
        "# =============================================================================\n",
        "\n",
        "def validate_binary(predictions, ground_truth, threshold):\n",
        "    \"\"\"Binary validation at specific threshold.\"\"\"\n",
        "    return evaluate_threshold(predictions, ground_truth, threshold)\n",
        "\n",
        "\n",
        "def validate_two_stage(predictions, ground_truth, threshold):\n",
        "    \"\"\"Two-stage: detection + hypothesis matching.\"\"\"\n",
        "    binary = evaluate_threshold(predictions, ground_truth, threshold)\n",
        "    \n",
        "    hypothesis_correct = 0\n",
        "    hypothesis_total = 0\n",
        "    \n",
        "    for pred in predictions:\n",
        "        line_num = pred['line_number']\n",
        "        p_score = pred['p_score']\n",
        "        \n",
        "        if p_score >= threshold and line_num in ground_truth:\n",
        "            hypothesis_total += 1\n",
        "            gt_labels = ground_truth[line_num]['labels']\n",
        "            \n",
        "            if check_hypothesis_label_match(pred['top_hypothesis'], gt_labels):\n",
        "                hypothesis_correct += 1\n",
        "    \n",
        "    hypothesis_accuracy = hypothesis_correct / hypothesis_total if hypothesis_total > 0 else 0.0\n",
        "    \n",
        "    return {\n",
        "        'stage_a': binary,\n",
        "        'stage_b': {\n",
        "            'correct': hypothesis_correct,\n",
        "            'total': hypothesis_total,\n",
        "            'accuracy': round(hypothesis_accuracy, 4)\n",
        "        },\n",
        "        'combined_score': round(binary['f1_score'] * hypothesis_accuracy, 4)\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Validation functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 10: Complete Validation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPLETE VALIDATION PIPELINE\n",
        "# =============================================================================\n",
        "\n",
        "def run_complete_validation(dataset_info, nlp_pipeline, methods=['option_a', 'option_b3']):\n",
        "    \"\"\"Run complete FTE-HARM validation with threshold testing.\"\"\"\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"FTE-HARM COMPLETE VALIDATION\")\n",
        "    print(f\"Dataset: {dataset_info['name']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Load dataset\n",
        "    logs, ground_truth, stats = load_dataset(dataset_info)\n",
        "    print(f\"\\nLoaded: {stats['total_lines']} lines ({stats['malicious_lines']} malicious)\")\n",
        "    print(f\"Labels found: {stats['unique_labels']}\")\n",
        "    \n",
        "    results = {\n",
        "        'dataset': dataset_info['name'],\n",
        "        'stats': stats,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    for method in methods:\n",
        "        print(f\"\\n--- Processing with {method.upper()} ---\")\n",
        "        \n",
        "        # Process all logs\n",
        "        predictions = []\n",
        "        for i, (line_num, log_text) in enumerate(logs):\n",
        "            pred = process_log_complete(line_num, log_text, nlp_pipeline, method)\n",
        "            predictions.append(pred)\n",
        "            if (i + 1) % 500 == 0:\n",
        "                print(f\"  Processed: {i+1}/{len(logs)}\")\n",
        "        \n",
        "        print(f\"Processed {len(predictions)} logs\")\n",
        "        \n",
        "        # Threshold analysis\n",
        "        print(\"\\n  Running threshold analysis...\")\n",
        "        threshold_analysis = find_optimal_threshold(predictions, ground_truth)\n",
        "        print_threshold_results(threshold_analysis)\n",
        "        \n",
        "        # Get optimal threshold\n",
        "        optimal_threshold = threshold_analysis['optimal_for_recall']['threshold']\n",
        "        \n",
        "        # Run validations at optimal threshold\n",
        "        binary = validate_binary(predictions, ground_truth, optimal_threshold)\n",
        "        two_stage = validate_two_stage(predictions, ground_truth, optimal_threshold)\n",
        "        \n",
        "        results[method] = {\n",
        "            'predictions': predictions,\n",
        "            'threshold_analysis': threshold_analysis,\n",
        "            'optimal_threshold': optimal_threshold,\n",
        "            'binary_validation': binary,\n",
        "            'two_stage_validation': two_stage\n",
        "        }\n",
        "        \n",
        "        print(f\"\\n  Results at optimal threshold ({optimal_threshold}):\")\n",
        "        print(f\"    Precision: {binary['precision']}\")\n",
        "        print(f\"    Recall: {binary['recall']} <- PRIMARY METRIC\")\n",
        "        print(f\"    F1: {binary['f1_score']}\")\n",
        "        print(f\"    Hypothesis Accuracy: {two_stage['stage_b']['accuracy']}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"Complete validation pipeline defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 11: Save Results with MITRE Corroboration Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SAVE RESULTS WITH MITRE CORROBORATION TABLE\n",
        "# =============================================================================\n",
        "\n",
        "def save_complete_results(results, output_path=SUMMARY_PATH):\n",
        "    \"\"\"Save all results including MITRE corroboration table.\"\"\"\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    dataset_name = results['dataset'].replace('/', '_')\n",
        "    \n",
        "    # Create dataset folder\n",
        "    folder = os.path.join(output_path, dataset_name)\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    \n",
        "    for method in ['option_a', 'option_b3']:\n",
        "        if method not in results:\n",
        "            continue\n",
        "        \n",
        "        data = results[method]\n",
        "        \n",
        "        # Main results file\n",
        "        result_path = os.path.join(folder, f'{method}_results_{timestamp}.txt')\n",
        "        with open(result_path, 'w') as f:\n",
        "            f.write(\"=\"*80 + \"\\n\")\n",
        "            f.write(f\"FTE-HARM VALIDATION RESULTS\\n\")\n",
        "            f.write(\"=\"*80 + \"\\n\\n\")\n",
        "            f.write(f\"Dataset: {results['dataset']}\\n\")\n",
        "            f.write(f\"Method: {method}\\n\")\n",
        "            f.write(f\"Timestamp: {results['timestamp']}\\n\")\n",
        "            f.write(f\"Optimal Threshold: {data['optimal_threshold']}\\n\\n\")\n",
        "            \n",
        "            f.write(\"VALIDATION GOAL: HIGH RECALL (Forensic Triage Priority)\\n\\n\")\n",
        "            \n",
        "            binary = data['binary_validation']\n",
        "            f.write(\"BINARY VALIDATION:\\n\")\n",
        "            f.write(f\"  TP: {binary['tp']}, FP: {binary['fp']}, TN: {binary['tn']}, FN: {binary['fn']}\\n\")\n",
        "            f.write(f\"  Precision: {binary['precision']}\\n\")\n",
        "            f.write(f\"  Recall: {binary['recall']} <- PRIMARY METRIC\\n\")\n",
        "            f.write(f\"  F1: {binary['f1_score']}\\n\\n\")\n",
        "            \n",
        "            two_stage = data['two_stage_validation']\n",
        "            f.write(\"TWO-STAGE VALIDATION:\\n\")\n",
        "            f.write(f\"  Detection F1: {two_stage['stage_a']['f1_score']}\\n\")\n",
        "            f.write(f\"  Hypothesis Correct: {two_stage['stage_b']['correct']}/{two_stage['stage_b']['total']}\\n\")\n",
        "            f.write(f\"  Hypothesis Accuracy: {two_stage['stage_b']['accuracy']}\\n\")\n",
        "    \n",
        "    # MITRE Corroboration Table\n",
        "    mitre_path = os.path.join(folder, f'mitre_corroboration_table_{timestamp}.txt')\n",
        "    with open(mitre_path, 'w') as f:\n",
        "        f.write(\"=\"*100 + \"\\n\")\n",
        "        f.write(\"MITRE ATT&CK CORROBORATION TABLE\\n\")\n",
        "        f.write(\"Dataset-Specific Labels -> Standardised Threat Taxonomy\\n\")\n",
        "        f.write(\"=\"*100 + \"\\n\\n\")\n",
        "        \n",
        "        f.write(f\"{'AITv2 Label':<25} {'Hypothesis':<30} {'MITRE ID':<15} {'MITRE Tactic':<25}\\n\")\n",
        "        f.write(\"-\"*100 + \"\\n\")\n",
        "        \n",
        "        # Group by target label\n",
        "        label_to_hyp = defaultdict(list)\n",
        "        for name, hyp in DATASET_HYPOTHESES.items():\n",
        "            for label in hyp['target_labels']:\n",
        "                label_to_hyp[label].append(hyp)\n",
        "        \n",
        "        for label in sorted(label_to_hyp.keys()):\n",
        "            for hyp in label_to_hyp[label]:\n",
        "                f.write(f\"{label:<25} {hyp['name']:<30} {hyp['mitre_technique']:<15} {hyp['mitre_tactic']:<25}\\n\")\n",
        "        \n",
        "        f.write(\"\\n\" + \"=\"*100 + \"\\n\")\n",
        "        f.write(\"This table demonstrates corroboration between dataset-specific attack labels\\n\")\n",
        "        f.write(\"and the standardised MITRE ATT&CK threat taxonomy.\\n\")\n",
        "    \n",
        "    # Threshold test results\n",
        "    threshold_path = os.path.join(THRESHOLD_TEST_PATH, f'threshold_test_{dataset_name}_{timestamp}.txt')\n",
        "    with open(threshold_path, 'w') as f:\n",
        "        f.write(\"THRESHOLD TEST RESULTS\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        f.write(f\"Dataset: {results['dataset']}\\n\\n\")\n",
        "        \n",
        "        for method in ['option_a', 'option_b3']:\n",
        "            if method not in results:\n",
        "                continue\n",
        "            analysis = results[method]['threshold_analysis']\n",
        "            f.write(f\"\\n{method.upper()}:\\n\")\n",
        "            f.write(\"-\"*80 + \"\\n\")\n",
        "            f.write(f\"{'Threshold':<12} {'Precision':<12} {'Recall':<12} {'F1':<12} {'FN':<12}\\n\")\n",
        "            for r in analysis['all_results']:\n",
        "                f.write(f\"{r['threshold']:<12} {r['precision']:<12} {r['recall']:<12} {r['f1_score']:<12} {r['fn']:<12}\\n\")\n",
        "            \n",
        "            opt = analysis['optimal_for_recall']\n",
        "            f.write(f\"\\nOptimal: {opt['threshold']} (Recall={opt['recall']}, FN={opt['fn']})\\n\")\n",
        "    \n",
        "    print(f\"\\nResults saved to: {folder}\")\n",
        "    print(f\"Threshold test saved to: {threshold_path}\")\n",
        "    return folder\n",
        "\n",
        "\n",
        "# Run on first dataset\n",
        "if all_datasets:\n",
        "    selected = all_datasets[0]\n",
        "    print(f\"\\nSelected dataset: {selected['name']}\")\n",
        "    \n",
        "    results = run_complete_validation(selected, nlp)\n",
        "    save_complete_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 12: Run All Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# RUN ALL DATASETS\n",
        "# =============================================================================\n",
        "\n",
        "def run_all_datasets(datasets, nlp_pipeline):\n",
        "    \"\"\"Process all datasets.\"\"\"\n",
        "    all_results = []\n",
        "    \n",
        "    for i, ds in enumerate(datasets, 1):\n",
        "        print(f\"\\n[{i}/{len(datasets)}] {ds['name']}\")\n",
        "        try:\n",
        "            results = run_complete_validation(ds, nlp_pipeline)\n",
        "            save_complete_results(results)\n",
        "            all_results.append({'dataset': ds['name'], 'status': 'success', 'results': results})\n",
        "        except Exception as e:\n",
        "            print(f\"  Error: {e}\")\n",
        "            all_results.append({'dataset': ds['name'], 'status': 'error', 'error': str(e)})\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "# Uncomment to run all:\n",
        "# all_results = run_all_datasets(all_datasets, nlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Checklist\n",
        "\n",
        "- [ ] Model loaded\n",
        "- [ ] Datasets discovered\n",
        "- [ ] Threshold testing completed\n",
        "- [ ] Optimal threshold identified (prioritizing RECALL)\n",
        "- [ ] Hypothesis matching validated\n",
        "- [ ] MITRE corroboration table generated\n",
        "- [ ] Results saved to summary folder"
      ]
    }
  ]
}
