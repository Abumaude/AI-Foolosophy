{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FTE-HARM BASIC: Single Hypothesis Validation\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook implements a **simplified FTE-HARM validation** with:\n",
        "- **ONE hypothesis** (first label discovered from dataset)\n",
        "- **ONE P_Score method** (Option A: Binary Presence)\n",
        "- **ONE validation approach** (Binary: TP/FP/TN/FN)\n",
        "\n",
        "**Purpose:** Understand core FTE-HARM mechanics before scaling to full implementation.\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset Structure\n",
        "\n",
        "The datasets have varying naming conventions:\n",
        "\n",
        "| Path | Log File | Label File |\n",
        "|------|----------|------------|\n",
        "| grp1/rm/ | log_auth.log | label_auth.log |\n",
        "| grp1/santos_paired/santos/openvpn/ | openvpn.log | openvpn_labels.log |\n",
        "| grp1/santos_paired/santos_minimal/vpn_logs_openvpn/ | log.log | label.log |\n",
        "| grp1/santos/vpn_logs_openvpn/ | log.log | label.log |\n",
        "\n",
        "**Key Insight:** Log/Label file naming varies by dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FTE-HARM BASIC: IMPORTS AND SETUP\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# PATH CONFIGURATION\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "DATASET_BASE_PATH = '/content/drive/My Drive/thesis/dataset'\n",
        "OUTPUT_PATH = '/content/drive/My Drive/thesis/hypotheses_validation'\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# Transformer model paths\n",
        "MODELS = {\n",
        "    'distilbert': '/content/drive/My Drive/thesis/transformer/distilberta_base_uncased/results/checkpoint-5245',\n",
        "    'distilroberta': '/content/drive/My Drive/thesis/transformer/distilroberta_base/results/checkpoint-5275',\n",
        "    'roberta_large': '/content/drive/My Drive/thesis/transformer/roberta_large/results/checkpoint-2772',\n",
        "    'xlm_roberta_base': '/content/drive/My Drive/thesis/transformer/xlm_roberta_base/results/checkpoint-12216',\n",
        "    'xlm_roberta_large': '/content/drive/My Drive/thesis/transformer/xlm_roberta_large/results/checkpoint-12240',\n",
        "}\n",
        "\n",
        "SELECTED_MODEL = 'xlm_roberta_large'\n",
        "\n",
        "print(\"Libraries imported\")\n",
        "print(\"Google Drive mounted\")\n",
        "print(f\"Dataset base path: {DATASET_BASE_PATH}\")\n",
        "print(f\"Output path: {OUTPUT_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Dataset Discovery (Flexible File Naming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATASET DISCOVERY - HANDLES VARIABLE FILE NAMING\n",
        "# =============================================================================\n",
        "\n",
        "def find_log_label_pair(folder_path):\n",
        "    \"\"\"\n",
        "    Find log and label files in a folder, handling variable naming conventions.\n",
        "    \n",
        "    Naming patterns supported:\n",
        "    - log.log / label.log\n",
        "    - log_auth.log / label_auth.log\n",
        "    - openvpn.log / openvpn_labels.log\n",
        "    - <name>.log / <name>_labels.log\n",
        "    \n",
        "    Args:\n",
        "        folder_path (str): Path to dataset folder\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (log_file_path, label_file_path) or (None, None) if not found\n",
        "    \"\"\"\n",
        "    if not os.path.isdir(folder_path):\n",
        "        return None, None\n",
        "    \n",
        "    files = os.listdir(folder_path)\n",
        "    log_files = [f for f in files if f.endswith('.log')]\n",
        "    \n",
        "    log_file = None\n",
        "    label_file = None\n",
        "    \n",
        "    # Strategy 1: Look for log.log / label.log\n",
        "    if 'log.log' in log_files and 'label.log' in log_files:\n",
        "        log_file = 'log.log'\n",
        "        label_file = 'label.log'\n",
        "    \n",
        "    # Strategy 2: Look for log_*.log / label_*.log pattern\n",
        "    elif any(f.startswith('log_') for f in log_files):\n",
        "        for f in log_files:\n",
        "            if f.startswith('log_'):\n",
        "                suffix = f[4:]  # e.g., \"auth.log\" from \"log_auth.log\"\n",
        "                potential_label = f'label_{suffix}'\n",
        "                if potential_label in log_files:\n",
        "                    log_file = f\n",
        "                    label_file = potential_label\n",
        "                    break\n",
        "    \n",
        "    # Strategy 3: Look for <name>.log / <name>_labels.log pattern\n",
        "    else:\n",
        "        for f in log_files:\n",
        "            if not f.endswith('_labels.log'):\n",
        "                base_name = f[:-4]  # Remove .log\n",
        "                potential_label = f'{base_name}_labels.log'\n",
        "                if potential_label in log_files:\n",
        "                    log_file = f\n",
        "                    label_file = potential_label\n",
        "                    break\n",
        "    \n",
        "    if log_file and label_file:\n",
        "        return (\n",
        "            os.path.join(folder_path, log_file),\n",
        "            os.path.join(folder_path, label_file)\n",
        "        )\n",
        "    \n",
        "    return None, None\n",
        "\n",
        "\n",
        "def scan_all_datasets(base_path):\n",
        "    \"\"\"\n",
        "    Recursively scan for all valid dataset pairs.\n",
        "    \n",
        "    Args:\n",
        "        base_path (str): Base dataset path\n",
        "    \n",
        "    Returns:\n",
        "        list: List of dicts with dataset info\n",
        "    \"\"\"\n",
        "    datasets = []\n",
        "    \n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        log_path, label_path = find_log_label_pair(root)\n",
        "        \n",
        "        if log_path and label_path:\n",
        "            rel_path = os.path.relpath(root, base_path)\n",
        "            datasets.append({\n",
        "                'name': rel_path,\n",
        "                'folder': root,\n",
        "                'log_path': log_path,\n",
        "                'label_path': label_path,\n",
        "                'log_file': os.path.basename(log_path),\n",
        "                'label_file': os.path.basename(label_path)\n",
        "            })\n",
        "    \n",
        "    return datasets\n",
        "\n",
        "\n",
        "# Scan for datasets\n",
        "print(\"Scanning for datasets...\")\n",
        "all_datasets = scan_all_datasets(DATASET_BASE_PATH)\n",
        "\n",
        "print(f\"\\nFound {len(all_datasets)} dataset pairs:\\n\")\n",
        "for i, ds in enumerate(all_datasets, 1):\n",
        "    print(f\"  {i}. {ds['name']}\")\n",
        "    print(f\"     Log: {ds['log_file']} | Label: {ds['label_file']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Label Discovery (Find First Label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LABEL DISCOVERY - FIND FIRST LABEL FOR BASIC HYPOTHESIS\n",
        "# =============================================================================\n",
        "\n",
        "def discover_labels(label_path):\n",
        "    \"\"\"\n",
        "    Discover all unique labels in a label.log file.\n",
        "    \n",
        "    Format: JSON lines with {\"line\": N, \"labels\": [...], \"rules\": {...}}\n",
        "    \n",
        "    Args:\n",
        "        label_path (str): Path to label file\n",
        "    \n",
        "    Returns:\n",
        "        dict: {\n",
        "            'all_labels': set of unique labels,\n",
        "            'label_counts': {label: count},\n",
        "            'first_label': first label encountered,\n",
        "            'total_entries': number of labeled lines\n",
        "        }\n",
        "    \"\"\"\n",
        "    all_labels = set()\n",
        "    label_counts = defaultdict(int)\n",
        "    first_label = None\n",
        "    total_entries = 0\n",
        "    \n",
        "    if not os.path.exists(label_path):\n",
        "        print(f\"WARNING: Label file not found: {label_path}\")\n",
        "        return None\n",
        "    \n",
        "    with open(label_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            \n",
        "            try:\n",
        "                entry = json.loads(line)\n",
        "                labels = entry.get('labels', [])\n",
        "                \n",
        "                if labels:\n",
        "                    total_entries += 1\n",
        "                    \n",
        "                    for label in labels:\n",
        "                        all_labels.add(label)\n",
        "                        label_counts[label] += 1\n",
        "                        \n",
        "                        # Track first label encountered\n",
        "                        if first_label is None:\n",
        "                            first_label = label\n",
        "                            \n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "    \n",
        "    # Sort by count\n",
        "    sorted_labels = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    return {\n",
        "        'all_labels': all_labels,\n",
        "        'label_counts': dict(label_counts),\n",
        "        'sorted_labels': sorted_labels,\n",
        "        'first_label': first_label,\n",
        "        'most_common_label': sorted_labels[0][0] if sorted_labels else None,\n",
        "        'total_entries': total_entries\n",
        "    }\n",
        "\n",
        "\n",
        "def discover_labels_all_datasets(datasets):\n",
        "    \"\"\"\n",
        "    Discover labels across all datasets.\n",
        "    \n",
        "    Args:\n",
        "        datasets (list): List of dataset info dicts\n",
        "    \n",
        "    Returns:\n",
        "        dict: Combined label discovery results\n",
        "    \"\"\"\n",
        "    combined_labels = set()\n",
        "    combined_counts = defaultdict(int)\n",
        "    first_label = None\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"LABEL DISCOVERY\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for ds in datasets:\n",
        "        result = discover_labels(ds['label_path'])\n",
        "        \n",
        "        if result:\n",
        "            print(f\"\\n{ds['name']}:\")\n",
        "            print(f\"  Labeled lines: {result['total_entries']}\")\n",
        "            print(f\"  Unique labels: {len(result['all_labels'])}\")\n",
        "            print(f\"  Labels: {list(result['all_labels'])[:5]}{'...' if len(result['all_labels']) > 5 else ''}\")\n",
        "            \n",
        "            combined_labels.update(result['all_labels'])\n",
        "            for label, count in result['label_counts'].items():\n",
        "                combined_counts[label] += count\n",
        "            \n",
        "            if first_label is None and result['first_label']:\n",
        "                first_label = result['first_label']\n",
        "    \n",
        "    # Sort combined\n",
        "    sorted_combined = sorted(combined_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    print(f\"\\n{'-'*40}\")\n",
        "    print(f\"COMBINED RESULTS:\")\n",
        "    print(f\"  Total unique labels: {len(combined_labels)}\")\n",
        "    print(f\"  First label found: {first_label}\")\n",
        "    print(f\"  Most common label: {sorted_combined[0][0] if sorted_combined else None}\")\n",
        "    print(f\"\\n  All labels by frequency:\")\n",
        "    for label, count in sorted_combined:\n",
        "        print(f\"    {label}: {count}\")\n",
        "    \n",
        "    return {\n",
        "        'all_labels': combined_labels,\n",
        "        'label_counts': dict(combined_counts),\n",
        "        'sorted_labels': sorted_combined,\n",
        "        'first_label': first_label,\n",
        "        'most_common_label': sorted_combined[0][0] if sorted_combined else None\n",
        "    }\n",
        "\n",
        "\n",
        "# Run label discovery\n",
        "label_discovery = discover_labels_all_datasets(all_datasets)\n",
        "\n",
        "# Store first label for hypothesis creation\n",
        "FIRST_LABEL = label_discovery['first_label']\n",
        "print(f\"\\nFirst label discovered: '{FIRST_LABEL}'\")\n",
        "print(f\"  This will be used to create the basic hypothesis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Ground Truth Loader (No Tokenization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# GROUND TRUTH LOADER - NO TOKENIZATION\n",
        "# =============================================================================\n",
        "\n",
        "def load_ground_truth(label_path):\n",
        "    \"\"\"\n",
        "    Load ground truth labels from label file.\n",
        "    \n",
        "    IMPORTANT: Ground truth is NOT tokenized - only used for validation.\n",
        "    \n",
        "    Format: JSON lines where each line is:\n",
        "    {\"line\": N, \"labels\": [\"label1\", \"label2\"], \"rules\": {...}}\n",
        "    \n",
        "    Args:\n",
        "        label_path (str): Path to label file\n",
        "    \n",
        "    Returns:\n",
        "        dict: {line_number: {\"labels\": [...], \"rules\": {...}}}\n",
        "    \"\"\"\n",
        "    ground_truth = {}\n",
        "    \n",
        "    if not os.path.exists(label_path):\n",
        "        print(f\"WARNING: Label file not found: {label_path}\")\n",
        "        return ground_truth\n",
        "    \n",
        "    with open(label_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for json_line in f:\n",
        "            json_line = json_line.strip()\n",
        "            if not json_line:\n",
        "                continue\n",
        "            \n",
        "            try:\n",
        "                entry = json.loads(json_line)\n",
        "                line_num = entry.get('line')\n",
        "                \n",
        "                if line_num is not None:\n",
        "                    ground_truth[line_num] = {\n",
        "                        'labels': entry.get('labels', []),\n",
        "                        'rules': entry.get('rules', {})\n",
        "                    }\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "    \n",
        "    return ground_truth\n",
        "\n",
        "\n",
        "def get_label_for_line(line_number, ground_truth):\n",
        "    \"\"\"\n",
        "    Get ground truth for a specific log line.\n",
        "    \n",
        "    Args:\n",
        "        line_number (int): 1-indexed line number\n",
        "        ground_truth (dict): Loaded ground truth\n",
        "    \n",
        "    Returns:\n",
        "        dict: {\"is_malicious\": bool, \"labels\": [...], \"rules\": {...}}\n",
        "    \"\"\"\n",
        "    if line_number in ground_truth:\n",
        "        return {\n",
        "            \"is_malicious\": True,\n",
        "            \"labels\": ground_truth[line_number]['labels'],\n",
        "            \"rules\": ground_truth[line_number]['rules']\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"is_malicious\": False,\n",
        "            \"labels\": [],\n",
        "            \"rules\": {}\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"Ground truth loader defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Raw Log Loader (With Line Tracking)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# RAW LOG LOADER - WITH 1-INDEXED LINE TRACKING\n",
        "# =============================================================================\n",
        "\n",
        "def load_raw_logs(log_path):\n",
        "    \"\"\"\n",
        "    Load raw log lines with 1-indexed line number tracking.\n",
        "    \n",
        "    Args:\n",
        "        log_path (str): Path to log file\n",
        "    \n",
        "    Returns:\n",
        "        list: [(line_number, log_text), ...] - line numbers are 1-indexed\n",
        "    \"\"\"\n",
        "    logs = []\n",
        "    \n",
        "    if not os.path.exists(log_path):\n",
        "        print(f\"ERROR: Log file not found: {log_path}\")\n",
        "        return logs\n",
        "    \n",
        "    with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line_number, log_text in enumerate(f, 1):  # 1-indexed!\n",
        "            log_text = log_text.strip()\n",
        "            if log_text:\n",
        "                logs.append((line_number, log_text))\n",
        "    \n",
        "    return logs\n",
        "\n",
        "\n",
        "def load_dataset(dataset_info):\n",
        "    \"\"\"\n",
        "    Load complete dataset (logs + ground truth).\n",
        "    \n",
        "    Args:\n",
        "        dataset_info (dict): Dataset info from scan_all_datasets()\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (logs, ground_truth, stats)\n",
        "    \"\"\"\n",
        "    logs = load_raw_logs(dataset_info['log_path'])\n",
        "    ground_truth = load_ground_truth(dataset_info['label_path'])\n",
        "    \n",
        "    total_lines = len(logs)\n",
        "    malicious_lines = len(ground_truth)\n",
        "    \n",
        "    stats = {\n",
        "        'name': dataset_info['name'],\n",
        "        'total_lines': total_lines,\n",
        "        'malicious_lines': malicious_lines,\n",
        "        'benign_lines': total_lines - malicious_lines,\n",
        "        'malicious_pct': (malicious_lines / total_lines * 100) if total_lines > 0 else 0\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nLoaded: {dataset_info['name']}\")\n",
        "    print(f\"  Total lines: {total_lines}\")\n",
        "    print(f\"  Malicious: {malicious_lines} ({stats['malicious_pct']:.2f}%)\")\n",
        "    print(f\"  Benign: {stats['benign_lines']}\")\n",
        "    \n",
        "    return logs, ground_truth, stats\n",
        "\n",
        "\n",
        "print(\"Raw log loader defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Model Loading and Entity Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MODEL LOADING AND ENTITY EXTRACTION (PHYSICAL TOKEN QUANTIZATION)\n",
        "# =============================================================================\n",
        "\n",
        "ENTITY_LABELS = [\n",
        "    'O', 'B-Action', 'B-ApplicationSpecific', 'B-AuthenticationType',\n",
        "    'B-DNSName', 'I-DNSName', 'B-DateTime', 'I-DateTime', 'B-Error', 'I-Error',\n",
        "    'B-IPAddress', 'B-Object', 'B-Port', 'B-Process', 'B-Protocol',\n",
        "    'B-Service', 'B-SessionID', 'B-Severity', 'B-Status', 'I-Status',\n",
        "    'B-System', 'B-Username'\n",
        "]\n",
        "\n",
        "def get_model_pipeline(model_path):\n",
        "    \"\"\"Load NER model with aggregation_strategy='simple'.\"\"\"\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
        "    \n",
        "    print(f\"Loading model from: {model_path}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "    \n",
        "    nlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "    print(\"Model loaded\")\n",
        "    return nlp\n",
        "\n",
        "\n",
        "def process_text(text, nlp_pipeline):\n",
        "    \"\"\"\n",
        "    Physical Token Quantization for entity extraction.\n",
        "    \n",
        "    1. Identify physical tokens (text NOT separated by whitespace/brackets)\n",
        "    2. If model finds ANY entity in physical token -> whole token selected\n",
        "    3. Priority: IPAddress > DNSName\n",
        "    4. Merge adjacent tokens with same label\n",
        "    \"\"\"\n",
        "    raw_results = nlp_pipeline(text)\n",
        "    physical_tokens = [match for match in re.finditer(r'[^\\[\\]\\s]+', text)]\n",
        "    \n",
        "    atomic_entities = []\n",
        "    \n",
        "    for pt in physical_tokens:\n",
        "        t_start, t_end = pt.span()\n",
        "        matches = [r for r in raw_results if r['start'] < t_end and r['end'] > t_start]\n",
        "        \n",
        "        if not matches:\n",
        "            continue\n",
        "        \n",
        "        labels = set(m['entity_group'] for m in matches)\n",
        "        \n",
        "        if 'IPAddress' in labels and 'DNSName' in labels:\n",
        "            chosen_label = 'IPAddress'\n",
        "        else:\n",
        "            matches.sort(key=lambda x: x['start'])\n",
        "            chosen_label = matches[0]['entity_group']\n",
        "        \n",
        "        avg_score = sum(float(m['score']) for m in matches) / len(matches)\n",
        "        \n",
        "        atomic_entities.append({\n",
        "            \"label\": chosen_label,\n",
        "            \"text\": text[t_start:t_end],\n",
        "            \"start\": t_start,\n",
        "            \"end\": t_end,\n",
        "            \"confidence\": avg_score\n",
        "        })\n",
        "    \n",
        "    # Merge adjacent same-label entities\n",
        "    if not atomic_entities:\n",
        "        return []\n",
        "    \n",
        "    final_entities = [atomic_entities[0]]\n",
        "    \n",
        "    for curr in atomic_entities[1:]:\n",
        "        prev = final_entities[-1]\n",
        "        text_between = text[prev['end']:curr['start']]\n",
        "        is_pure_whitespace = text_between.strip() == '' and '[' not in text_between and ']' not in text_between\n",
        "        \n",
        "        if is_pure_whitespace and prev['label'] == curr['label']:\n",
        "            prev['end'] = curr['end']\n",
        "            prev['text'] = text[prev['start']:prev['end']]\n",
        "            prev['confidence'] = (prev['confidence'] + curr['confidence']) / 2\n",
        "        else:\n",
        "            final_entities.append(curr)\n",
        "    \n",
        "    for ent in final_entities:\n",
        "        ent['confidence'] = round(ent['confidence'], 4)\n",
        "    \n",
        "    return final_entities\n",
        "\n",
        "\n",
        "def extract_entities_for_line(line_number, log_text, nlp_pipeline):\n",
        "    \"\"\"Extract entities with line number tracking.\"\"\"\n",
        "    entities = process_text(log_text, nlp_pipeline)\n",
        "    \n",
        "    entity_types = defaultdict(list)\n",
        "    for ent in entities:\n",
        "        entity_types[ent['label']].append({\n",
        "            'value': ent['text'],\n",
        "            'confidence': ent['confidence']\n",
        "        })\n",
        "    \n",
        "    return {\n",
        "        'line_number': line_number,\n",
        "        'log_text': log_text,\n",
        "        'entities': entities,\n",
        "        'entity_types': dict(entity_types)\n",
        "    }\n",
        "\n",
        "\n",
        "# Load model\n",
        "nlp = get_model_pipeline(MODELS[SELECTED_MODEL])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Basic Hypothesis (Using First Label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BASIC HYPOTHESIS - USING FIRST DISCOVERED LABEL\n",
        "# =============================================================================\n",
        "\n",
        "# Create hypothesis based on first discovered label\n",
        "BASIC_HYPOTHESIS = {\n",
        "    'name': f'H1_{FIRST_LABEL}',\n",
        "    'description': f'Hypothesis for detecting {FIRST_LABEL} attacks',\n",
        "    'target_label': FIRST_LABEL,  # Ground truth label this maps to\n",
        "    'weights': {\n",
        "        # Default weights - adjust based on what entities are relevant\n",
        "        'Process': 0.25,\n",
        "        'Username': 0.20,\n",
        "        'Action': 0.20,\n",
        "        'IPAddress': 0.15,\n",
        "        'DateTime': 0.10,\n",
        "        'Status': 0.10\n",
        "    },\n",
        "    'critical_entity': 'Process',  # Most important entity\n",
        "    'penalty_factor': 0.20  # Penalty if critical entity missing\n",
        "}\n",
        "\n",
        "# Thresholds for confidence levels\n",
        "THRESHOLDS = {\n",
        "    'HIGH': 0.65,\n",
        "    'MEDIUM': 0.50,\n",
        "    'LOW': 0.35\n",
        "}\n",
        "\n",
        "# Triage priority mapping\n",
        "TRIAGE_PRIORITIES = {\n",
        "    'HIGH': 'Priority 1: Investigate immediately',\n",
        "    'MEDIUM': 'Priority 2: Queue for investigation',\n",
        "    'LOW': 'Priority 3: Investigate later',\n",
        "    'INSUFFICIENT': 'Priority 4: Archive for future relevance'\n",
        "}\n",
        "\n",
        "print(f\"Basic Hypothesis Created:\")\n",
        "print(f\"  Name: {BASIC_HYPOTHESIS['name']}\")\n",
        "print(f\"  Target Label: {BASIC_HYPOTHESIS['target_label']}\")\n",
        "print(f\"  Critical Entity: {BASIC_HYPOTHESIS['critical_entity']}\")\n",
        "print(f\"  Weights: {BASIC_HYPOTHESIS['weights']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: P_Score Calculation (Option A - Binary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# P_SCORE CALCULATION - OPTION A (BINARY PRESENCE)\n",
        "# =============================================================================\n",
        "\n",
        "def calculate_pscore(entity_types, hypothesis):\n",
        "    \"\"\"\n",
        "    Calculate P_Score using BINARY entity presence.\n",
        "    \n",
        "    Formula: P_Score = (Sum(W_i * E_i)) * (1 - P_F)\n",
        "    \n",
        "    Where:\n",
        "        W_i = weight for entity type i\n",
        "        E_i = 1 if entity present, 0 if absent\n",
        "        P_F = penalty factor (if critical entity missing)\n",
        "    \n",
        "    Args:\n",
        "        entity_types (dict): {entity_type: [{'value': str, 'confidence': float}]}\n",
        "        hypothesis (dict): Hypothesis configuration\n",
        "    \n",
        "    Returns:\n",
        "        dict: P_Score result\n",
        "    \"\"\"\n",
        "    weights = hypothesis['weights']\n",
        "    critical_entity = hypothesis['critical_entity']\n",
        "    penalty_factor = hypothesis['penalty_factor']\n",
        "    \n",
        "    # Calculate weighted sum\n",
        "    weighted_sum = 0.0\n",
        "    entity_breakdown = {}\n",
        "    \n",
        "    for entity_type, weight in weights.items():\n",
        "        is_present = entity_type in entity_types and len(entity_types[entity_type]) > 0\n",
        "        contribution = weight * (1 if is_present else 0)\n",
        "        weighted_sum += contribution\n",
        "        \n",
        "        entity_breakdown[entity_type] = {\n",
        "            'weight': weight,\n",
        "            'present': is_present,\n",
        "            'contribution': contribution\n",
        "        }\n",
        "    \n",
        "    # Check critical entity\n",
        "    critical_present = critical_entity in entity_types and len(entity_types[critical_entity]) > 0\n",
        "    \n",
        "    # Apply penalty\n",
        "    if critical_present:\n",
        "        p_score = weighted_sum\n",
        "    else:\n",
        "        p_score = weighted_sum * (1 - penalty_factor)\n",
        "    \n",
        "    # Determine confidence level\n",
        "    if p_score >= THRESHOLDS['HIGH']:\n",
        "        confidence_level = 'HIGH'\n",
        "    elif p_score >= THRESHOLDS['MEDIUM']:\n",
        "        confidence_level = 'MEDIUM'\n",
        "    elif p_score >= THRESHOLDS['LOW']:\n",
        "        confidence_level = 'LOW'\n",
        "    else:\n",
        "        confidence_level = 'INSUFFICIENT'\n",
        "    \n",
        "    return {\n",
        "        'p_score': round(p_score, 4),\n",
        "        'confidence_level': confidence_level,\n",
        "        'triage_priority': TRIAGE_PRIORITIES[confidence_level],\n",
        "        'is_malicious': p_score >= THRESHOLDS['LOW'],\n",
        "        'critical_present': critical_present,\n",
        "        'entity_breakdown': entity_breakdown\n",
        "    }\n",
        "\n",
        "\n",
        "def process_log_line(line_number, log_text, nlp_pipeline, hypothesis):\n",
        "    \"\"\"\n",
        "    Complete processing: extract entities -> calculate P_Score -> prediction.\n",
        "    \"\"\"\n",
        "    # Extract entities\n",
        "    extraction = extract_entities_for_line(line_number, log_text, nlp_pipeline)\n",
        "    \n",
        "    # Calculate P_Score\n",
        "    score_result = calculate_pscore(extraction['entity_types'], hypothesis)\n",
        "    \n",
        "    return {\n",
        "        'line_number': line_number,\n",
        "        'log_text': log_text,\n",
        "        'entities': extraction['entities'],\n",
        "        'entity_types': extraction['entity_types'],\n",
        "        'p_score': score_result['p_score'],\n",
        "        'confidence_level': score_result['confidence_level'],\n",
        "        'triage_priority': score_result['triage_priority'],\n",
        "        'is_malicious': score_result['is_malicious'],\n",
        "        'critical_present': score_result['critical_present']\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"P_Score calculation functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9: Binary Validation (Approach 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BINARY VALIDATION - TP/FP/TN/FN\n",
        "# =============================================================================\n",
        "\n",
        "def validate_binary(predictions, ground_truth, target_label=None):\n",
        "    \"\"\"\n",
        "    Binary validation: malicious vs benign.\n",
        "    \n",
        "    Args:\n",
        "        predictions (list): List of prediction dicts\n",
        "        ground_truth (dict): Ground truth {line_number: {\"labels\": [...]}}\n",
        "        target_label (str): Optional - only count as TP if this label present\n",
        "    \n",
        "    Returns:\n",
        "        dict: Validation metrics\n",
        "    \"\"\"\n",
        "    tp = fp = tn = fn = 0\n",
        "    \n",
        "    details = {\n",
        "        'true_positives': [],\n",
        "        'false_positives': [],\n",
        "        'true_negatives': [],\n",
        "        'false_negatives': []\n",
        "    }\n",
        "    \n",
        "    for pred in predictions:\n",
        "        line_num = pred['line_number']\n",
        "        predicted_malicious = pred['is_malicious']\n",
        "        \n",
        "        gt = get_label_for_line(line_num, ground_truth)\n",
        "        actually_malicious = gt['is_malicious']\n",
        "        \n",
        "        # If target_label specified, check if it's in the labels\n",
        "        if target_label and actually_malicious:\n",
        "            actually_malicious = target_label in gt['labels']\n",
        "        \n",
        "        if predicted_malicious and actually_malicious:\n",
        "            tp += 1\n",
        "            details['true_positives'].append({\n",
        "                'line': line_num,\n",
        "                'score': pred['p_score'],\n",
        "                'labels': gt['labels']\n",
        "            })\n",
        "        elif predicted_malicious and not actually_malicious:\n",
        "            fp += 1\n",
        "            details['false_positives'].append({\n",
        "                'line': line_num,\n",
        "                'score': pred['p_score']\n",
        "            })\n",
        "        elif not predicted_malicious and not actually_malicious:\n",
        "            tn += 1\n",
        "        else:\n",
        "            fn += 1\n",
        "            details['false_negatives'].append({\n",
        "                'line': line_num,\n",
        "                'labels': gt['labels']\n",
        "            })\n",
        "    \n",
        "    # Calculate metrics\n",
        "    total = tp + fp + tn + fn\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "    accuracy = (tp + tn) / total if total > 0 else 0.0\n",
        "    \n",
        "    return {\n",
        "        'confusion_matrix': {'TP': tp, 'FP': fp, 'TN': tn, 'FN': fn},\n",
        "        'metrics': {\n",
        "            'precision': round(precision, 4),\n",
        "            'recall': round(recall, 4),\n",
        "            'f1_score': round(f1, 4),\n",
        "            'accuracy': round(accuracy, 4)\n",
        "        },\n",
        "        'totals': {\n",
        "            'total': total,\n",
        "            'actual_malicious': tp + fn,\n",
        "            'actual_benign': tn + fp,\n",
        "            'predicted_malicious': tp + fp,\n",
        "            'predicted_benign': tn + fn\n",
        "        },\n",
        "        'details': details\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Binary validation function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 10: Run Basic Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# RUN BASIC FTE-HARM VALIDATION\n",
        "# =============================================================================\n",
        "\n",
        "def run_basic_validation(dataset_info, nlp_pipeline, hypothesis):\n",
        "    \"\"\"\n",
        "    Run basic FTE-HARM validation on a single dataset.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"BASIC FTE-HARM VALIDATION\")\n",
        "    print(f\"Dataset: {dataset_info['name']}\")\n",
        "    print(f\"Hypothesis: {hypothesis['name']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Load dataset\n",
        "    logs, ground_truth, stats = load_dataset(dataset_info)\n",
        "    \n",
        "    # Process all logs\n",
        "    print(f\"\\nProcessing {len(logs)} log lines...\")\n",
        "    predictions = []\n",
        "    \n",
        "    for i, (line_num, log_text) in enumerate(logs):\n",
        "        pred = process_log_line(line_num, log_text, nlp_pipeline, hypothesis)\n",
        "        predictions.append(pred)\n",
        "        \n",
        "        if (i + 1) % 500 == 0:\n",
        "            print(f\"  Processed: {i+1}/{len(logs)}\")\n",
        "    \n",
        "    print(f\"Processed {len(predictions)} logs\")\n",
        "    \n",
        "    # Validate\n",
        "    print(f\"\\nValidating against ground truth...\")\n",
        "    validation = validate_binary(predictions, ground_truth, hypothesis['target_label'])\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\n{'-'*40}\")\n",
        "    print(\"RESULTS\")\n",
        "    print(f\"{'-'*40}\")\n",
        "    \n",
        "    cm = validation['confusion_matrix']\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(f\"  True Positives:  {cm['TP']}\")\n",
        "    print(f\"  False Positives: {cm['FP']}\")\n",
        "    print(f\"  True Negatives:  {cm['TN']}\")\n",
        "    print(f\"  False Negatives: {cm['FN']}\")\n",
        "    \n",
        "    m = validation['metrics']\n",
        "    print(f\"\\nMetrics:\")\n",
        "    print(f\"  Precision: {m['precision']:.4f}\")\n",
        "    print(f\"  Recall:    {m['recall']:.4f}\")\n",
        "    print(f\"  F1-Score:  {m['f1_score']:.4f}\")\n",
        "    print(f\"  Accuracy:  {m['accuracy']:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'dataset': dataset_info['name'],\n",
        "        'hypothesis': hypothesis['name'],\n",
        "        'stats': stats,\n",
        "        'predictions': predictions,\n",
        "        'validation': validation\n",
        "    }\n",
        "\n",
        "\n",
        "# Select first dataset for testing\n",
        "if all_datasets:\n",
        "    selected_dataset = all_datasets[0]\n",
        "    print(f\"\\nSelected dataset: {selected_dataset['name']}\")\n",
        "    \n",
        "    # Run validation\n",
        "    results = run_basic_validation(selected_dataset, nlp, BASIC_HYPOTHESIS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 11: Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SAVE RESULTS TO GOOGLE DRIVE\n",
        "# =============================================================================\n",
        "\n",
        "def save_basic_results(results, output_path=OUTPUT_PATH):\n",
        "    \"\"\"Save basic validation results.\"\"\"\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    filename = f\"basic_validation_{results['dataset'].replace('/', '_')}_{timestamp}.txt\"\n",
        "    filepath = os.path.join(output_path, filename)\n",
        "    \n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        f.write(\"FTE-HARM BASIC VALIDATION RESULTS\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\\n\")\n",
        "        \n",
        "        f.write(f\"Timestamp: {datetime.now().isoformat()}\\n\")\n",
        "        f.write(f\"Dataset: {results['dataset']}\\n\")\n",
        "        f.write(f\"Hypothesis: {results['hypothesis']}\\n\")\n",
        "        f.write(f\"Model: {SELECTED_MODEL}\\n\\n\")\n",
        "        \n",
        "        f.write(\"DATASET STATISTICS:\\n\")\n",
        "        s = results['stats']\n",
        "        f.write(f\"  Total Lines: {s['total_lines']}\\n\")\n",
        "        f.write(f\"  Malicious: {s['malicious_lines']} ({s['malicious_pct']:.2f}%)\\n\")\n",
        "        f.write(f\"  Benign: {s['benign_lines']}\\n\\n\")\n",
        "        \n",
        "        f.write(\"CONFUSION MATRIX:\\n\")\n",
        "        cm = results['validation']['confusion_matrix']\n",
        "        f.write(f\"  True Positives:  {cm['TP']}\\n\")\n",
        "        f.write(f\"  False Positives: {cm['FP']}\\n\")\n",
        "        f.write(f\"  True Negatives:  {cm['TN']}\\n\")\n",
        "        f.write(f\"  False Negatives: {cm['FN']}\\n\\n\")\n",
        "        \n",
        "        f.write(\"METRICS:\\n\")\n",
        "        m = results['validation']['metrics']\n",
        "        f.write(f\"  Precision: {m['precision']:.4f}\\n\")\n",
        "        f.write(f\"  Recall:    {m['recall']:.4f}\\n\")\n",
        "        f.write(f\"  F1-Score:  {m['f1_score']:.4f}\\n\")\n",
        "        f.write(f\"  Accuracy:  {m['accuracy']:.4f}\\n\")\n",
        "    \n",
        "    print(f\"\\nResults saved to: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "\n",
        "# Save results\n",
        "if 'results' in dir():\n",
        "    save_basic_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Checklist\n",
        "\n",
        "**Before running:**\n",
        "- [ ] Google Drive mounted\n",
        "- [ ] Datasets found by scanner\n",
        "- [ ] Labels discovered (first label identified)\n",
        "- [ ] Model loaded successfully\n",
        "- [ ] Hypothesis created with target label\n",
        "\n",
        "**After running:**\n",
        "- [ ] Check confusion matrix (TP/FP/TN/FN)\n",
        "- [ ] Review F1-Score (target: > 0.45)\n",
        "- [ ] Check false negatives (missed attacks)\n",
        "- [ ] Review results file in Google Drive"
      ]
    }
  ]
}
