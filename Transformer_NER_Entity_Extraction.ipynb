{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer NER Model Loader for Forensic Log Entity Extraction\n",
        "\n",
        "This notebook demonstrates how to load fine-tuned transformer models and extract 22 entity types from forensic log data for the FTE-HARM framework.\n",
        "\n",
        "## Purpose\n",
        "- Load transformer models (DistilBERT, DistilRoBERTa, RoBERTa, XLM-RoBERTa) from Google Drive\n",
        "- Extract structured entities from raw log text\n",
        "- Bridge the gap between unstructured logs and FTE-HARM's hypothesis-aligned reasoning\n",
        "\n",
        "## Entity Types\n",
        "The models are trained to recognize 22 entity types using BIO tagging:\n",
        "- **DateTime**: Timestamps (Jan 24 10:30:45)\n",
        "- **IPAddress**: IP addresses (192.168.1.100)\n",
        "- **DNSName**: Domain names (example.com)\n",
        "- **Process**: Process names (sshd, dnsmasq)\n",
        "- **Username**: User identifiers (admin, root)\n",
        "- **Action**: Actions/verbs (login, failed, accept)\n",
        "- And 16 more..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 1: Mount Google Drive\n",
        "\n",
        "The trained models are stored in Google Drive. We need to mount it first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('Google Drive mounted successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 2: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers torch\n",
        "\n",
        "import torch\n",
        "print(f'PyTorch version: {torch.__version__}')\n",
        "print(f'CUDA available: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 3: Configuration\n",
        "\n",
        "Define model paths and entity labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model checkpoint paths on Google Drive\n",
        "TRANSFORMER_MODELS = {\n",
        "    # DistilBERT (Distilled BERT - Fast, efficient)\n",
        "    'distilbert': '/content/drive/My Drive/thesis/transformer/distilberta_base_uncased/results/checkpoint-5245',\n",
        "    \n",
        "    # DistilRoBERTa (RECOMMENDED - Best balance of speed and accuracy)\n",
        "    'distilroberta': '/content/drive/My Drive/thesis/transformer/distilroberta_base/results/checkpoint-5275',\n",
        "    \n",
        "    # RoBERTa Large (High accuracy, slower)\n",
        "    'roberta_large': '/content/drive/My Drive/thesis/transformer/roberta_large/results/checkpoint-2772',\n",
        "    \n",
        "    # XLM-RoBERTa Base (Multilingual capability)\n",
        "    'xlm_roberta_base': '/content/drive/My Drive/thesis/transformer/xlm_roberta_base/results/checkpoint-12216',\n",
        "    \n",
        "    # XLM-RoBERTa Large (Best accuracy, slowest)\n",
        "    'xlm_roberta_large': '/content/drive/My Drive/thesis/transformer/xlm_roberta_large/results/checkpoint-12240',\n",
        "}\n",
        "\n",
        "# 22 Entity labels based on BIO tagging scheme\n",
        "ENTITY_LABELS = [\n",
        "    'O',                        # 0  - Outside (not an entity)\n",
        "    'B-Action',                 # 1  - Action/verb (login, failed, accept)\n",
        "    'B-ApplicationSpecific',    # 2  - App-specific terms\n",
        "    'B-AuthenticationType',     # 3  - Auth methods (password, publickey)\n",
        "    'B-DNSName',                # 4  - Domain names (begin)\n",
        "    'I-DNSName',                # 5  - Domain names (continuation)\n",
        "    'B-DateTime',               # 6  - Timestamps (begin)\n",
        "    'I-DateTime',               # 7  - Timestamps (continuation)\n",
        "    'B-Error',                  # 8  - Error messages (begin)\n",
        "    'I-Error',                  # 9  - Error messages (continuation)\n",
        "    'B-IPAddress',              # 10 - IP addresses (begin only)\n",
        "    'B-Object',                 # 11 - File/object names\n",
        "    'B-Port',                   # 12 - Port numbers\n",
        "    'B-Process',                # 13 - Process names (sshd, su, dnsmasq)\n",
        "    'B-Protocol',               # 14 - Network protocols (TCP, UDP)\n",
        "    'B-Service',                # 15 - Service names\n",
        "    'B-SessionID',              # 16 - Session identifiers\n",
        "    'B-Severity',               # 17 - Log severity (error, warn, info)\n",
        "    'B-Status',                 # 18 - Status indicators (begin)\n",
        "    'I-Status',                 # 19 - Status indicators (continuation)\n",
        "    'B-System',                 # 20 - Hostnames/systems\n",
        "    'B-Username',               # 21 - User identifiers\n",
        "]\n",
        "\n",
        "# Create bidirectional mappings\n",
        "id2label = {i: label for i, label in enumerate(ENTITY_LABELS)}\n",
        "label2id = {label: i for i, label in enumerate(ENTITY_LABELS)}\n",
        "\n",
        "print(f'Entity labels defined: {len(ENTITY_LABELS)}')\n",
        "print(f'Available models: {list(TRANSFORMER_MODELS.keys())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 4: Load Model and Tokenizer\n",
        "\n",
        "Select and load a transformer model. **DistilRoBERTa** is recommended for best balance of speed and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import torch\n",
        "\n",
        "# Select model (change this to test different models)\n",
        "SELECTED_MODEL = 'distilroberta'  # Options: distilbert, distilroberta, roberta_large, xlm_roberta_base, xlm_roberta_large\n",
        "MODEL_PATH = TRANSFORMER_MODELS[SELECTED_MODEL]\n",
        "\n",
        "print(f'Loading {SELECTED_MODEL}...')\n",
        "print(f'Path: {MODEL_PATH}')\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f'\\nModel loaded: {type(model).__name__}')\n",
        "print(f'Number of labels: {model.config.num_labels}')\n",
        "print(f'Architecture: {model.config.model_type}')\n",
        "print(f'Device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 5: Define Hybrid Extraction Function\n",
        "\n",
        "This function combines model predictions with regex post-processing to handle entities that fragment at token boundaries.\n",
        "\n",
        "### Why Hybrid?\n",
        "- **Model handles**: DateTime, DNSName, Error, Status (have I- tags)\n",
        "- **Regex handles**: IPAddress, Process, Username, System (no I- tags, fragment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def extract_entities_bio(log_line):\n",
        "    \"\"\"\n",
        "    HYBRID extraction: Model predictions + regex post-processing\n",
        "    \n",
        "    Args:\n",
        "        log_line (str): Raw log entry text\n",
        "    \n",
        "    Returns:\n",
        "        list of tuples: [(entity_type, value), ...]\n",
        "    \"\"\"\n",
        "    \n",
        "    # ========== STEP 1: TOKENIZATION WITH OFFSET MAPPING ==========\n",
        "    inputs = tokenizer(\n",
        "        log_line,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_offsets_mapping=True\n",
        "    )\n",
        "    \n",
        "    offset_mapping = inputs.pop('offset_mapping')[0].tolist()\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    # ========== STEP 2: MODEL PREDICTION ==========\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "    \n",
        "    pred_labels = [id2label[p.item()] for p in predictions[0]]\n",
        "    \n",
        "    # ========== STEP 3: FIX MULTIPLE B- TAGS (CRITICAL) ==========\n",
        "    corrected_labels = []\n",
        "    prev_entity_type = None\n",
        "    \n",
        "    for label in pred_labels:\n",
        "        if label.startswith('B-'):\n",
        "            entity_type = label[2:]\n",
        "            if entity_type == prev_entity_type:\n",
        "                corrected_labels.append(f'I-{entity_type}')\n",
        "            else:\n",
        "                corrected_labels.append(label)\n",
        "                prev_entity_type = entity_type\n",
        "        elif label.startswith('I-'):\n",
        "            corrected_labels.append(label)\n",
        "            prev_entity_type = label[2:]\n",
        "        else:\n",
        "            corrected_labels.append(label)\n",
        "            prev_entity_type = None\n",
        "    \n",
        "    pred_labels = corrected_labels\n",
        "    \n",
        "    # ========== STEP 4: EXTRACT ENTITIES FROM MODEL ==========\n",
        "    model_entities = []\n",
        "    current_entity_type = None\n",
        "    entity_spans = []\n",
        "    \n",
        "    for idx, (label, (start, end)) in enumerate(zip(pred_labels, offset_mapping)):\n",
        "        if start == 0 and end == 0:  # Special token\n",
        "            continue\n",
        "        \n",
        "        if label.startswith('B-'):\n",
        "            if current_entity_type and entity_spans:\n",
        "                entity_start = entity_spans[0][0]\n",
        "                entity_end = entity_spans[-1][1]\n",
        "                entity_value = log_line[entity_start:entity_end].strip()\n",
        "                if entity_value:\n",
        "                    model_entities.append((current_entity_type, entity_value))\n",
        "            \n",
        "            current_entity_type = label[2:]\n",
        "            entity_spans = [(start, end)]\n",
        "        \n",
        "        elif label.startswith('I-') and current_entity_type:\n",
        "            entity_type = label[2:]\n",
        "            if entity_type == current_entity_type:\n",
        "                entity_spans.append((start, end))\n",
        "            else:\n",
        "                if entity_spans:\n",
        "                    entity_start = entity_spans[0][0]\n",
        "                    entity_end = entity_spans[-1][1]\n",
        "                    entity_value = log_line[entity_start:entity_end].strip()\n",
        "                    if entity_value:\n",
        "                        model_entities.append((current_entity_type, entity_value))\n",
        "                current_entity_type = entity_type\n",
        "                entity_spans = [(start, end)]\n",
        "        \n",
        "        elif label == 'O':\n",
        "            if current_entity_type and entity_spans:\n",
        "                entity_start = entity_spans[0][0]\n",
        "                entity_end = entity_spans[-1][1]\n",
        "                entity_value = log_line[entity_start:entity_end].strip()\n",
        "                if entity_value:\n",
        "                    model_entities.append((current_entity_type, entity_value))\n",
        "            current_entity_type = None\n",
        "            entity_spans = []\n",
        "    \n",
        "    if current_entity_type and entity_spans:\n",
        "        entity_start = entity_spans[0][0]\n",
        "        entity_end = entity_spans[-1][1]\n",
        "        entity_value = log_line[entity_start:entity_end].strip()\n",
        "        if entity_value:\n",
        "            model_entities.append((current_entity_type, entity_value))\n",
        "    \n",
        "    # ========== STEP 5: HYBRID POST-PROCESSING ==========\n",
        "    fragmented_types = {'IPAddress', 'Process', 'Username', 'System'}\n",
        "    entities = [e for e in model_entities if e[0] not in fragmented_types]\n",
        "    entities = [e for e in entities if e[0] != 'DNSName']  # DNS also fragments\n",
        "    \n",
        "    # ========== STEP 6: REGEX EXTRACTIONS ==========\n",
        "    \n",
        "    # IP Addresses\n",
        "    ip_pattern = r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b'\n",
        "    for match in re.finditer(ip_pattern, log_line):\n",
        "        entities.append(('IPAddress', match.group()))\n",
        "    \n",
        "    # DNS Names\n",
        "    dns_pattern = r'\\b([a-z0-9](?:[a-z0-9-]{0,61}[a-z0-9])?(?:\\.[a-z0-9](?:[a-z0-9-]{0,61}[a-z0-9])?)+)\\b'\n",
        "    for match in re.finditer(dns_pattern, log_line, re.IGNORECASE):\n",
        "        domain = match.group()\n",
        "        if not re.match(r'^\\d+\\.\\d+', domain):\n",
        "            entities.append(('DNSName', domain))\n",
        "    \n",
        "    # Process names with PIDs\n",
        "    process_pattern = r'\\b([a-zA-Z_][a-zA-Z0-9_-]*)\\[(\\d+)\\]'\n",
        "    for match in re.finditer(process_pattern, log_line):\n",
        "        entities.append(('Process', match.group(1)))\n",
        "        entities.append(('ProcessID', match.group(2)))\n",
        "    \n",
        "    # Usernames\n",
        "    username_patterns = [\n",
        "        r'\\bfor\\s+([a-z_][a-z0-9_-]*)\\b',\n",
        "        r'\\buser\\s+([a-z_][a-z0-9_-]*)\\b',\n",
        "        r'\\bby\\s+([a-z_][a-z0-9_-]*)\\b',\n",
        "    ]\n",
        "    excluded = {'root', 'unknown', 'invalid', 'port', 'from', 'to', 'on', 'at'}\n",
        "    \n",
        "    for pattern in username_patterns:\n",
        "        for match in re.finditer(pattern, log_line, re.IGNORECASE):\n",
        "            username = match.group(1).lower()\n",
        "            if username not in excluded and len(username) > 1:\n",
        "                entities.append(('Username', match.group(1)))\n",
        "    \n",
        "    # System/Hostname\n",
        "    parts = log_line.split()\n",
        "    if len(parts) > 3:\n",
        "        potential_host = parts[3].rstrip(':')\n",
        "        if re.match(r'^[a-zA-Z][a-zA-Z0-9-]*$', potential_host) and len(potential_host) > 2:\n",
        "            entities.append(('System', potential_host))\n",
        "    \n",
        "    # Deduplicate\n",
        "    seen = set()\n",
        "    unique = []\n",
        "    for e in entities:\n",
        "        if e not in seen:\n",
        "            seen.add(e)\n",
        "            unique.append(e)\n",
        "    \n",
        "    return unique\n",
        "\n",
        "print('Hybrid extraction function defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 6: Test Extraction\n",
        "\n",
        "Test the extraction on sample forensic logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1: DNS Query Log\n",
        "dns_log = \"Jan 24 10:30:45 dnsmasq[1234]: query[A] example.com from 192.168.1.100\"\n",
        "entities = extract_entities_bio(dns_log)\n",
        "\n",
        "print('=' * 60)\n",
        "print('TEST 1: DNS Query Log')\n",
        "print('=' * 60)\n",
        "print(f'Input: {dns_log}')\n",
        "print('\\nExtracted Entities:')\n",
        "for entity_type, value in entities:\n",
        "    print(f'  [{entity_type}] {value}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2: SSH Authentication Log\n",
        "ssh_log = \"Jan 24 04:37:40 intranet-server su[27950]: Successful su for jhall by www-data\"\n",
        "entities = extract_entities_bio(ssh_log)\n",
        "\n",
        "print('=' * 60)\n",
        "print('TEST 2: SSH Authentication Log')\n",
        "print('=' * 60)\n",
        "print(f'Input: {ssh_log}')\n",
        "print('\\nExtracted Entities:')\n",
        "for entity_type, value in entities:\n",
        "    print(f'  [{entity_type}] {value}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 3: Failed Login Attempt\n",
        "failed_log = \"Jan 24 10:15:32 server sshd[5678]: Failed password for invalid user admin from 10.0.0.50 port 22 ssh2\"\n",
        "entities = extract_entities_bio(failed_log)\n",
        "\n",
        "print('=' * 60)\n",
        "print('TEST 3: Failed Login Attempt')\n",
        "print('=' * 60)\n",
        "print(f'Input: {failed_log}')\n",
        "print('\\nExtracted Entities:')\n",
        "for entity_type, value in entities:\n",
        "    print(f'  [{entity_type}] {value}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 7: FTE-HARM Integration\n",
        "\n",
        "Convert extracted entities to formats compatible with FTE-HARM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def entities_to_dict(entities):\n",
        "    \"\"\"Convert entity list to dictionary format.\"\"\"\n",
        "    entity_dict = {}\n",
        "    for entity_type, value in entities:\n",
        "        if entity_type not in entity_dict:\n",
        "            entity_dict[entity_type] = []\n",
        "        entity_dict[entity_type].append(value)\n",
        "    return entity_dict\n",
        "\n",
        "def entities_to_tagged_string(entities):\n",
        "    \"\"\"Convert entities to tagged string format.\"\"\"\n",
        "    return ' '.join(f'[{t}: {v}]' for t, v in entities)\n",
        "\n",
        "def format_for_fte_harm(log_line, entities):\n",
        "    \"\"\"Format extracted entities for FTE-HARM input.\"\"\"\n",
        "    return {\n",
        "        'raw_log': log_line,\n",
        "        'entities': entities_to_dict(entities),\n",
        "        'entity_list': entities,\n",
        "        'tagged_text': entities_to_tagged_string(entities),\n",
        "    }\n",
        "\n",
        "# Example\n",
        "log = \"Jan 24 10:30:45 server sshd[1234]: Failed login for admin from 192.168.1.100\"\n",
        "entities = extract_entities_bio(log)\n",
        "fte_harm_input = format_for_fte_harm(log, entities)\n",
        "\n",
        "print('FTE-HARM Input Format:')\n",
        "print('-' * 40)\n",
        "for key, value in fte_harm_input.items():\n",
        "    print(f'{key}: {value}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 8: Batch Processing\n",
        "\n",
        "Process multiple log entries at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample batch of logs\n",
        "sample_logs = [\n",
        "    \"Jan 24 10:30:45 dnsmasq[1234]: query[A] malware-c2.evil.com from 192.168.1.50\",\n",
        "    \"Jan 24 10:31:02 server sshd[5678]: Failed password for root from 10.0.0.100 port 22\",\n",
        "    \"Jan 24 10:31:15 firewall kernel: DENY TCP 192.168.1.50:45678 -> 8.8.8.8:53\",\n",
        "    \"Jan 24 10:31:30 intranet su[9876]: Successful su for admin by www-data\",\n",
        "    \"Jan 24 10:32:00 dnsmasq[1234]: reply malware-c2.evil.com is 198.51.100.1\",\n",
        "]\n",
        "\n",
        "print('Batch Processing Results')\n",
        "print('=' * 70)\n",
        "\n",
        "for i, log in enumerate(sample_logs, 1):\n",
        "    entities = extract_entities_bio(log)\n",
        "    print(f'\\nLog {i}: {log[:60]}...')\n",
        "    print('Entities:')\n",
        "    for entity_type, value in entities:\n",
        "        print(f'  [{entity_type}] {value}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Validation Checklist\n",
        "\n",
        "Verify that all requirements are met."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('VALIDATION CHECKLIST')\n",
        "print('=' * 50)\n",
        "\n",
        "# Check model loaded\n",
        "model_loaded = model is not None and tokenizer is not None\n",
        "print(f'[{\"PASS\" if model_loaded else \"FAIL\"}] Model loads without errors')\n",
        "\n",
        "# Check entity labels\n",
        "labels_correct = len(ENTITY_LABELS) == 22\n",
        "print(f'[{\"PASS\" if labels_correct else \"FAIL\"}] 22 entity labels defined correctly')\n",
        "\n",
        "# Check mappings\n",
        "mappings_ok = len(id2label) == 22 and len(label2id) == 22\n",
        "print(f'[{\"PASS\" if mappings_ok else \"FAIL\"}] id2label and label2id mappings created')\n",
        "\n",
        "# Test extraction\n",
        "test_log = \"Jan 24 10:30:45 server sshd[1234]: Failed login from 192.168.1.100\"\n",
        "test_entities = extract_entities_bio(test_log)\n",
        "extraction_works = len(test_entities) > 0\n",
        "print(f'[{\"PASS\" if extraction_works else \"FAIL\"}] Hybrid extraction function executes')\n",
        "\n",
        "# Check IP not fragmented\n",
        "ip_entities = [e for e in test_entities if e[0] == 'IPAddress']\n",
        "ip_complete = any('192.168.1.100' in e[1] for e in ip_entities)\n",
        "print(f'[{\"PASS\" if ip_complete else \"FAIL\"}] No fragmented IP addresses')\n",
        "\n",
        "# Check process extracted\n",
        "process_entities = [e for e in test_entities if e[0] == 'Process']\n",
        "process_ok = any('sshd' in e[1] for e in process_entities)\n",
        "print(f'[{\"PASS\" if process_ok else \"FAIL\"}] Complete process names with PIDs')\n",
        "\n",
        "# Output format check\n",
        "format_ok = all(isinstance(e, tuple) and len(e) == 2 for e in test_entities)\n",
        "print(f'[{\"PASS\" if format_ok else \"FAIL\"}] Output format: [(entity_type, value), ...]')\n",
        "\n",
        "print('\\n' + '=' * 50)\n",
        "all_pass = all([model_loaded, labels_correct, mappings_ok, extraction_works, ip_complete, process_ok, format_ok])\n",
        "print(f'Overall: {\"ALL CHECKS PASSED\" if all_pass else \"SOME CHECKS FAILED\"}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Model Comparison (Optional)\n",
        "\n",
        "Compare different models on the same log data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to compare models (takes time to load each model)\n",
        "\"\"\"\n",
        "import time\n",
        "\n",
        "test_log = \"Jan 24 10:30:45 server sshd[1234]: Failed login for admin from 192.168.1.100\"\n",
        "\n",
        "for model_name in ['distilbert', 'distilroberta', 'roberta_large']:\n",
        "    print(f'\\n{\"=\"*50}')\n",
        "    print(f'Testing: {model_name}')\n",
        "    print(f'{\"=\"*50}')\n",
        "    \n",
        "    # Load model\n",
        "    path = TRANSFORMER_MODELS[model_name]\n",
        "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(path)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    # Time inference\n",
        "    start = time.time()\n",
        "    for _ in range(10):\n",
        "        entities = extract_entities_bio(test_log)\n",
        "    elapsed = (time.time() - start) / 10 * 1000\n",
        "    \n",
        "    print(f'Avg inference time: {elapsed:.2f}ms')\n",
        "    print('Entities:')\n",
        "    for entity_type, value in entities:\n",
        "        print(f'  [{entity_type}] {value}')\n",
        "\"\"\"\n",
        "print('Model comparison code available - uncomment to run')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "### What We Achieved\n",
        "- Loaded fine-tuned transformer models from Google Drive\n",
        "- Implemented hybrid entity extraction (model + regex)\n",
        "- Extracted 22 entity types from forensic log data\n",
        "- Created FTE-HARM compatible output formats\n",
        "\n",
        "### Key Findings\n",
        "- Pure model extraction fragments entities without I- tags (IPAddress, Process, etc.)\n",
        "- Hybrid approach solves fragmentation with regex post-processing\n",
        "- DistilRoBERTa provides best balance of speed and accuracy\n",
        "\n",
        "### Next Steps\n",
        "1. Integrate with Dataset Loader to process log files\n",
        "2. Feed extracted entities to FTE-HARM for hypothesis scoring\n",
        "3. Validate on real forensic datasets"
      ]
    }
  ]
}
